{
  "requested_url": "http://feeds.feedburner.com/IeeeSpectrumFullText",
  "fetched_at": "2016-05-13T00:45:33.084975",
  "status_code": 200,
  "response_text": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<?xml-stylesheet type=\"text/xsl\" media=\"screen\" href=\"/~d/styles/rss2full.xsl\"?><?xml-stylesheet type=\"text/css\" media=\"screen\" href=\"http://feeds.feedburner.com/~d/styles/itemcontent.css\"?><rss xmlns:content=\"http://purl.org/rss/1.0/modules/content/\" xmlns:slash=\"http://purl.org/rss/1.0/modules/slash/\" xmlns:apple-wallpapers=\"http://www.apple.com/ilife/wallpapers\" xmlns:itunes=\"http://www.itunes.com/dtds/podcast-1.0.dtd\" xmlns:g-custom=\"http://base.google.com/cns/1.0\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:creativeCommons=\"http://backend.userland.com/creativeCommonsRssModule\" xmlns:taxo=\"http://purl.org/rss/1.0/modules/taxonomy/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:g-core=\"http://base.google.com/ns/1.0\" xmlns:cc=\"http://web.resource.org/cc/\" xmlns:media=\"http://search.yahoo.com/mrss/\" xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:georss=\"http://www.georss.org/georss\" xmlns:geo=\"http://www.w3.org/2003/01/geo/wgs84_pos#\" xmlns:feedburner=\"http://rssnamespace.org/feedburner/ext/1.0\" version=\"2.0\">\r\n  <channel>\r\n    <title>IEEE Spectrum Recent Content full text</title>\r\n    <link>http://spectrum.ieee.org</link>\r\n    <description>IEEE Spectrum Recent Content headlines</description>\r\n    <atom10:link xmlns:atom10=\"http://www.w3.org/2005/Atom\" rel=\"self\" type=\"application/rss+xml\" href=\"http://feeds.feedburner.com/IeeeSpectrumFullText\" /><feedburner:info uri=\"ieeespectrumfulltext\" /><atom10:link xmlns:atom10=\"http://www.w3.org/2005/Atom\" rel=\"hub\" href=\"http://pubsubhubbub.appspot.com/\" /><item>\r\n      <title>5G Researchers Set New World Record For Spectrum Efficiency</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/jd6llYUoNow/5g-researchers-achieve-new-spectrum-efficiency-record</link>\r\n      <description>They showed a 22-fold increase over existing 4G networks</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"5G2170-1463058680775.jpg\" src=\"http://spectrum.ieee.org/img/5G2170-1463058680775.jpg\"/>\n<figcaption class=\"hi-cap\">Photo: University of Bristol</figcaption>\n<figcaption>Researchers set up this 128-antenna array in March at the University of Bristol to carry out the first of several attempts to achieve greater spectrum efficiency.</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>A team of 5G researchers has set a new world record for spectrum efficiency. Their achievement with massive MIMO (multiple-input, multiple-output)\u00a0arrays, which are cellular base stations comprised of dozens of antennas, is further evidence that this technology is a promising option for wireless engineers working to construct networks to deliver ultra-fast data speeds to more smartphones and tablets than ever before.</p>\n<p/>\n<p>In an experiment on Wednesday, the group<span>\n<span/>\n</span>achieved a rate of 145.6 (bits/s)/Hz for 22 users, each modulated with 256-QAM, on a <span>\n<span>shared 20 MHz radio channel at 3.51 GHz<em>\n<span>\n<span/>\n</span>\n</em>with</span>\n</span> an 128-antenna massive MIMO array. That represents a 22-fold increase in spectrum efficiency over today\u2019s existing 4G networks.</p>\n<p/>\n<p>Eight researchers and postdocs from the University of Bristol and Sweden\u2019s Lund University completed the demonstration on the upper level in the atrium of a university building on Bristol\u2019s campus in England. The group was led by <a shape=\"rect\" href=\"http://www.bristol.ac.uk/engineering/people/mark-a-beach/index.html\">Mark Beach</a>, a radio systems engineer at Bristol University.</p>\n<p>\"It's been quite challenging to get it to come together,\u201d he says. \u201cThere's a lot of things that have to happen to get the equipment to work. Mainly\u2014wires.\"</p>\n<p/>\n<p>With the new results, the team beat its earlier record of 79.4 (bits/s)/Hz for 12 users from March, and the rate of 71 (bits/s)/Hz that Facebook achieved for 24 users in April with its 96-antenna ARIES array. While the Bristol team has designed an array that would work best as part of ultra-dense small cell networks in cities, the company is working on massive MIMO for a different purpose\u2014as a way to beam wireless internet to rural areas from within cities.</p>\n<p/>\n<p>In the future, users are expected to exchange much more information over the almost instant data transfer offered by 5G. But with consumers already facing a dwindling supply of spectrum, wireless engineers need to find ways to use the available spectrum to exchange all this new data more efficiently without causing delays for everyone.</p>\n<p/>\n<p>To achieve this, many groups are focused on massive MIMO, which allows for the simultaneous transfer of many incoming and outgoing messages at once. While traditional cellular base stations might rely on four antennas, a massive MIMO array features dozens that rely on signal processing to find the best and fastest way to route messages to their intended destinations.</p>\n<p/>\n<p>Industry has said it will require a 1000-fold increase in spectrum efficiency in order for 5G to function as envisioned. Not all of that increase must come from massive MIMO, though\u2014many other 5G technologies are in the works including millimeter wave and beamforming.</p>\n<p>\n<a shape=\"rect\" href=\"http://www.commsys.isy.liu.se/en/staff/egl\">Erik Larsson</a>, who leads the Division for Communications Systems at Linkoping University in Sweden, points out that these new results are experimental and not real world. They were achieved in a highly controlled environment with no interference from other cellular signals. The users were stationary while in reality, people and their phones are often in motion.\u00a0</p>\n<p>Still, Larsson <span data-mce-style=\"font-size: 18px;\">\n<span data-mce-style=\"font-size: 12px;\">says the results are very significant for the field. \u201cI'm very impressed by these experimental results and I think they demonstrate with clarity the extraordinary potential massive MIMO has as a key scalable technology for 5G,\u201d he says. </span>\n</span>\n</p>\n<p/>\n<p>In its demonstration, the team used a flexible prototyping platform from <a shape=\"rect\" href=\"http://www.ni.com/en-us.html\">National Instruments</a> built with LabVIEW system design software and PXI hardware. The equipment was provided through a <a shape=\"rect\" href=\"http://www.bristolisopen.com/\">unique partnership</a> between the University of Bristol and its city council as part of a broader initiative to develop Bristol as a testbed for wireless technologies.</p>\n<p/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/jd6llYUoNow\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Thu, 12 May 2016 21:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/tech-talk/telecom/wireless/5g-researchers-achieve-new-spectrum-efficiency-record</guid>\r\n      <dc:creator>Amy Nordrum</dc:creator>\r\n      <dc:date>2016-05-12T21:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MjM2Ng\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MjM2Ng\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/tech-talk/telecom/wireless/5g-researchers-achieve-new-spectrum-efficiency-record</feedburner:origLink></item>\r\n    <item>\r\n      <title>Advanced Research Projects from DARPA's Pentagon Demo Day</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/qgKXH4qx3ug/advanced-research-projects-from-darpas-pentagon-demo-day</link>\r\n      <description>DARPA shows off all of its cool new toys</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Yesterday, the\u00a0Defense Advanced Research Projects Agency (DARPA) held a Demo Day for the Department of Defense in the courtyard at the center of the Pentagon to give the defense community \u201can up-close look at the Agency's diverse portfolio of innovative technologies and military systems at various stages of development and readiness.\u201d In other words, prototypes of ultra-futuristic, high-risk high-reward\u00a0hardware and software.</p>\n<p>The Pentagon Courtyard was filled with displays centered around ten different theme areas: air, biology, counterterrorism, cyber, ground warfare, maritime, microsystems, seeds of surprise, space, and spectrum. That last one is probably why we were invited, and we came back with this\u00a0gallery full of pictures of all of the coolest new stuff.</p>\n<table style=\"width:620px\" cellpadding=\"5\" border=\"0\" cellspacing=\"1\">\n<tbody>\n<tr>\n<td colspan=\"1\" rowspan=\"1\">\n<h4>\n<a shape=\"rect\" href=\"#1\">\n<span>Modular Prosthetic Limb</span>\n</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#2\">\n<span>Tern UAV</span>\n</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#3\">\n<span>Upward Falling Payloads</span>\n</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#4\">\n<span>VTOL X-Plane</span>\n</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#5\">\n<span>Mobile Hotspots</span>\n</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#6\">\n<span>Transparent Armor</span>\n</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#7\">\n<span>Soft Exosuit</span>\n</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#8\">\n<span>Self-Destructing Electronics</span>\n</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#9\">\n<span>Photonic Computer</span>\n</a>\n</h4>\n</td>\n<td colspan=\"1\" rowspan=\"1\">\n<h4>\n<a shape=\"rect\" href=\"#10\">Microstructured Materials</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#11\">MicroFactory</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#12\">Fast Autonomous Drone</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#13\">IntraChip Cooling</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#14\">World's Fastest Chip</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#15\">LIDAR On a Chip</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#16\">Satlets</a>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"#17\">Robotic Satellite Servicing</a>\n</h4>\n</td>\n</tr>\n</tbody>\n</table>\n<a shape=\"rect\" href=\"#17\"/>\n<p/>\n<hr/>\n<h2>\n<a id=\"1\" name=\"1\" shape=\"rect\">Modular Prosthetic Limb</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYxMQ\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>Johnny Matheny demonstrates Johns Hopkins' Modular Prosthetic Limb, which he controls with a pair of myoelectric armbands. The arm features human-like strength and dexterity, high-resolution tactile and position sensing, and an anthropomorphic form factor.</em>\n</p>\n<p/>\n<p>\n<strong>DARPA Program: <a shape=\"rect\" href=\"http://www.darpa.mil/program/revolutionizing-prosthetics\">Revolutionizing Prosthetics</a>\n</strong>\n</p>\n<hr/>\n<h2>\n<a name=\"2\" shape=\"rect\">Tern UAV</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYxMg\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>A mockup of Tern (Tactically Exploited Reconnaissance Node), a long endurance UAV that can take off and land vertically. Designed for both surveillance and strike capability, a full scale demonstrator is scheduled to fly in 2018.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/tactically-exploited-reconnaissance-node\">Tactically Exploited Reconnaissance Node</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"3\" name=\"3\" shape=\"rect\">Upward Falling Payloads</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYxMw\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>This 'Riser Node' is placed at the bottom of the ocean, from where it can be remotely triggered on demand. The pod then 'falls' upward to the surface of the ocean, where it releases its payload, which could include an unmanned surface vessel or a UAV.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/upward-falling-payloads\">Upward Falling Payloads</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"4\" name=\"4\" shape=\"rect\">VTOL X-Plane</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYxNA\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>A mockup of Aurora Flight Science's Lightning Strike, which uses rotating pods of hybrid-electric propulsion ducted fans to take off vertically and then fly horizontally. A 20% scale prototype has flown already, and DARPA has put nearly $90 million towards the construction of two full-scale versions which are scheduled to fly by September of 2018.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/vertical-takeoff-and-landing-experimental-plane\">Vertical Takeoff and Landing Experimental Plane</a>\u00a0</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"5\" name=\"5\" shape=\"rect\">Mobile Hotspots</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYxNQ\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>Two steerable antennas\u00a0can transfer one gigabyte of data per second through a flying mobile hotspot. A pair of these pods are mounted on a Shadow UAV to provide mobile connectivity at the equivalent of 4G speeds to soldiers in remote areas, and teams of drones can cooperate to create flexible, resilient networks.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/mobile-hotspots\">Mobile Hotspots</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"6\" name=\"6\" shape=\"rect\">Transparent Armor</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYxNg\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>Transparent armor made up of layered ceramics, glass, and polymer that's only about six centimeters thick is able to withstand multiple hits from 7.62mm rounds. The crystalline structure of the ceramic helps to prevent the armor from shattering after the first shot, making it ideal for use as windows in tactical vehicles.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/soldier-protection-systems\">Soldier Protection Systems</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"7\" name=\"7\" shape=\"rect\">Soft Exosuit</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYxNw\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>Patrick Murphy from Harvard's Wyss Institude demonstrates a soft exosuit that can increase the effective strength and endurance of the wearer by up to 25%. Motors pull on cables attached to the user's thighs, taking over some of the work that muscles would otherwise be doing while walking. The suit is currently being tested by soldiers at the Army Research Lab.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/warrior-web\">Warrior Web</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"8\" name=\"8\" shape=\"rect\">Self-Destructing Electronics</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYxOA\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>On the right is an electronic chip embedded in stressed glass, and on the left is all that remains of a similar chip after receiving a signal to self-destruct. \"Vanishing\" electronics like these can help keep sophisticated technology under control by giving components specific lifetimes, after which they physically destroy themselves.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/vanishing-programmable-resources\">Vanishing Programmable Resources</a>\n</strong>\n</p>\n<a shape=\"rect\" href=\"http://www.darpa.mil/program/vanishing-programmable-resources\"/>\n<p/>\n<strong/>\n<hr/>\n<h2>\n<a id=\"9\" name=\"9\" shape=\"rect\">Photonic Computer</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYxOQ\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>This is a photonic computer, which uses photons (lasers) for internal communications instead of electrons. Lasers can transmit more data at higher speeds while using significantly less energy, and this chip consumes 20x less power than a non-photonic version with similar performance. Several spin offs from UC Berkeley are already working on commercializing this technology.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/photonically-optimized-embedded-microprocessors\">Photonically Optimized Embedded Microprocessors</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"10\" name=\"10\" shape=\"rect\">Microstructured Materials</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYyMA\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>Next to the bar of solid copper (upper right) is copper that has been woven into a lattice structure. Using a variety of different techniques, copper and other metals can be printed or woven into tightly controlled micro or nanostructures with unique properties, including bespoke density, porosity, flexibility, weight, strength, and more. The difference between using bulk materials and microstructured materials is like the difference between building the Pyramids and the Eiffel Tower, and the potential applications are enormous.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/materials-with-controlled-microstructural-architecture\">Materials with Controlled Microstructural Architecture (MCMA)</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"11\" name=\"11\" shape=\"rect\">MicroFactory</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYyMQ\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>This lightweight carbon fiber truss was created by a swarm of micro-robots from SRI International. It contains embedded electronics as well. The robots are currently working on building skins, which when combined with trusses and electronics, can be used to create structures or potentially even vehicles. \u00a0</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/open-manufacturing\">Open Manufacturing</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"12\" name=\"12\" shape=\"rect\">Fast Autonomous Drone</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYyMg\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>For true autonomy, drones will have to sense obstacles and avoid them using only on-board sensing and computing. This drone is equipped with a pair of stereo cameras that allow it to avoid obstacles visually at a speed of 1 m/s. By 2018, DARPA hopes to have drones like these autonomously navigating indoors, outdoors, and even through windows at speeds of up to 20 m/s.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/fast-lightweight-autonomy\">Fast Lightweight Autonomy</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"13\" name=\"13\" shape=\"rect\">IntraChip Cooling</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYyMw\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>Instead of just cooling the top of a computer processor, actively circulating liquid coolant through the processor itself has the potential to dramatically increase performance. Seen here is a chip with microfluidic channels embedded inside the substrate, which could be used for high-performance computing and solid-state lasers.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/intrachip-interchip-enhanced-cooling\">Intrachip/Interchip Enhanced Cooling</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"14\" name=\"14\" shape=\"rect\">World's Fastest Chip</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYyNA\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>You're looking at the \"World's Fastest Chip,\" powering a radio link operating at 850 GHz. This is the highest frequency radio link ever demonstrated, according to DARPA. With the RF spectrum getting more and more crowded, ultra high frequency radios like these mean more devices transferring more data more quickly.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/thz-electronics\">THz Electronics</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"15\" name=\"15\" shape=\"rect\">LIDAR On a Chip</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYyNQ\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>The tiny sliver in the upper right of this image is an operational solid-state LIDAR, which uses an infrared laser to determine the direction and distance to objects. This LIDAR can detect objects out to a distance of 1m, but by the end of the year, should reach 10m, with the potential to reach a range of 100m or more. Most robots (including autonomous cars) rely on large, complex, and expensive LIDAR systems, and low-cost solid-state LIDAR could enable all kinds of new and affordable robotic applications.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/dahi-electronic-photonic-heterogenous-integration\">Electronic-Photonic Heterogeneous Integration</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"16\" name=\"16\" shape=\"rect\">Satlets</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYyNg\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>When recycling components of dead satellites on-orbit, you still need a basic structure to place those components on that can provide power and control. These modular satlets can be assembled into any number of configurations, offering flexibility, redundancy, and cost savings. The modular bus in this picture is scheduled to carry a telescope into space aboard a SpaceX Falcon 9 this August.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/program/phoenix\">Phoenix</a>\n</strong>\n</p>\n<p/>\n<hr/>\n<h2>\n<a id=\"17\" name=\"17\" shape=\"rect\">Robotic Satellite Servicing</a>\n</h2>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MjYyNw\"/>\n<figcaption class=\"hi-cap\">Photo: Evan Ackerman / IEEE Spectrum</figcaption>\n</figure>\n<p>\n<em>A mockup of a satellite designed to service other satellites in geosynchronous orbits, which are currently beyond the reach of existing solutions. Satellites could be inspected, repaired, have new payloads installed, or even moved to different orbits, saving on launch costs and reducing orbital debris.</em>\n</p>\n<p>\n<strong>DARPA Program:\u00a0<a shape=\"rect\" href=\"http://www.darpa.mil/news-events/2016-03-25\">Robotic Servicing of Geosynchronous Satellites</a>\n</strong>\n</p>\n<p/>\n<p/>\n<p/>\n<div/>\n<div/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/qgKXH4qx3ug\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Thu, 12 May 2016 20:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/tech-talk/computing/hardware/advanced-research-projects-from-darpas-pentagon-demo-day</guid>\r\n      <dc:creator>Evan Ackerman</dc:creator>\r\n      <dc:date>2016-05-12T20:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1Mjk4Ng\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1Mjk4Ng\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/tech-talk/computing/hardware/advanced-research-projects-from-darpas-pentagon-demo-day</feedburner:origLink></item>\r\n    <item>\r\n      <title>Spintronic Devices From Topological Insulators Inch Closer</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/vLmRQe57tiI/spintronic-devices-from-topological-insulators-inch-closer</link>\r\n      <description>Mating a topological insulator with a stable magnetic material produces surprising results</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"nature17635Katmiss-1463071504458.jpg\" src=\"http://spectrum.ieee.org/img/nature17635Katmiss-1463071504458.jpg\"/>\n<figcaption class=\"hi-cap\">Image: MIT/Nature</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>\n<a shape=\"rect\" href=\"http://spectrum.ieee.org/searchContent?q=topological+insulators&amp;type=&amp;sortby=relevance\">Topological insulators</a>\u00a0(TIs) are materials that are insulators on the inside but conductors on the outside. They\u00a0have\u00a0been both a great hope and a head scratcher for scientists and engineers in fields such as \u201c<a shape=\"rect\" href=\"http://spectrum.ieee.org/searchContent?q=spintronics\">spintronics</a>\u201d and <a shape=\"rect\" href=\"http://spectrum.ieee.org/searchContent?q=quantum+computing&amp;type=&amp;sortby=newest\">quantum computing</a> who have been trying to make practical devices with the materials. The inability to\u00a0combine TIs\u00a0with a material that has a controllable magnetic property has proven to be a major roadblock.</p>\n<p/>\n<p>Now in joint research, led by a team at the Massachusetts Institute of Technology, researchers have <a shape=\"rect\" href=\"http://news.mit.edu/2016/unexpected-magnetic-effect-thin-film-materials-0509\">combined several molecular layers of a topological insulator material called bismuth selenide (Bi2Se3) with an ultrathin layer of a magnetic material, europium sulfide (EuS).</a>\n</p>\n<p/>\n<p>This successful combination of the two materials was a feat in itself. But what the hybrid material enabled was something that went beyond the researchers expectations.</p>\n<p/>\n<p>Typically, europium sulfide has pretty stable magnetic properties, but it can only maintain those stable magnetic states at extremely low temperatures, 17 degrees above absolute zero (17 Kelvin). To the researchers surprise, when the europium sulfide was mated with the TI material it maintained those magnetic states all the way up to room temperature. When you are looking for practical devices, \u201croom temperature\u201d operation sounds like music to your ears.</p>\n<p/>\n<p>These results described in the journal <a shape=\"rect\" href=\"http://www.nature.com/nature/journal/vaop/ncurrent/full/nature17635.html\">\n<em>Nature</em>\n</a>seem promising. But there is a bit of a caveat: the slightest variation in how these two materials are combined results in wildly different properties for the resulting hybrid material. In fact, eliminating contamination or imperfections at the interface of the two materials is the key to the new material\u2019s properties. As the researchers point out, the new properties of the hybrid material stem from the interface between the two materials.</p>\n<p/>\n<p>The researchers have dubbed the magnetic interaction at the interface of the two materials \u201cproximity-induced magnetism,\u201d\u00a0which they believe could lead to the new breed of spintronic devices.</p>\n<p/>\n<p>This research\u00a0may also\u00a0offer a way to produce a particle that to date has only been theoretical, called <a shape=\"rect\" href=\"http://spectrum.ieee.org/searchContent?q=Majorana+fermions&amp;type=&amp;sortby=newest\">Majorana fermions</a>. In theory, these so-called quasiparticles can encode data so that it cannot be disrupted by thermal fluctuations, which has handicapped current quantum computing systems.</p>\n<p/>\n<p>Philip Kim, a professor of physics at Harvard University, who was not involved in this work, said in a press release: \u201cTopological insulators and magnetic insulators are two completely dissimilar materials. Yet they produce very unusual emergent effects at their atomically clean interface. The enhanced interfacial magnetism shown in this work can be very relevant to building up novel spintronics devices that can process information with low energy consumption.\u201d</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/vLmRQe57tiI\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Thu, 12 May 2016 19:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/nanoclast/semiconductors/materials/spintronic-devices-from-topological-insulators-inch-closer</guid>\r\n      <dc:creator>Dexter Johnson</dc:creator>\r\n      <dc:date>2016-05-12T19:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MjU4NQ\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MjU4NQ\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/nanoclast/semiconductors/materials/spintronic-devices-from-topological-insulators-inch-closer</feedburner:origLink></item>\r\n    <item>\r\n      <title>Six Creative Ways to Solve Biomedicine's Big Data Problem</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/_UtVJger54s/competition-seeks-to-solve-biomedicines-big-data-problem</link>\r\n      <description>International teams compete to develop tools to harness biomedical big data</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"DonaldIainSmithGettyImages597273583-1462897093253.jpg\" src=\"http://spectrum.ieee.org/img/DonaldIainSmithGettyImages597273583-1462897093253.jpg\"/>\n<figcaption class=\"hi-cap\">Photo-illustration: Donald Iain Smith/Getty Images</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Biomedical research generates an obscene amount of data. Many of the sensors, robots and other technologies <em>IEEE Spectrum </em>regularly profiles spew out terabytes to petabytes of data\u2014and that\u2019s only a sliver of the volume of health information stored in databases around the world.</p>\n<p/>\n<p>Now, three funding agencies are trying to spur the development of tools and platforms to improve researchers\u2019 ability to find, access and use that data. Yesterday at the 7<sup>th</sup>\n<a shape=\"rect\" href=\"http://healthdatapalooza.org/\">Health Datapalooza</a> conference in Washington, D.C., the National Institutes of Health, the U.K.-based Wellcome Trust, and the Howard Hughes Medical Institute announced six finalists for the first-ever <a shape=\"rect\" href=\"https://www.openscienceprize.org/\">Open Science Prize</a>, a global science competition for\u00a0prototype tools and platforms to tame biomedical\u2019s big data behemoth.</p>\n<p/>\n<p>Part of the problem with developing these kind of tools is that no one is sure who should\u00a0be responsible for them. \u201cData is generated globally, but it\u2019s essentially managed and funded nationally,\u201d says Philip Bourne, associate director for data science at the NIH. In an effort to transcend international borders and fund data science in a new way, Bourne and colleagues at Wellcome and HHMI hatched a plan for the prize.</p>\n<p/>\n<p>Launched last October, 96 teams spanning 45 countries entered the competition. Each team was required to have one member based in the U.S. and at least one other in another country.</p>\n<p/>\n<p>Yesterday, an\u00a0<a shape=\"rect\" href=\"https://www.openscienceprize.org/res/p/EA/\">expert panel</a> announced the six finalists who would receive $80,000 each to spend over the next nine months to develop their prototype. \u201cWe tried to pick things that had real promise, and where a small amount of money and a bit of publicity would really help,\u201d says Bourne.</p>\n<p/>\n<p>Here we take a peek at the six finalists, but be sure to stay tuned later this year to help pick a winner: In December, each team will demonstrate their prototype at a showcase, and the public will be invited to vote for their favorites. The winner will receive a grand prize of $230,000 to turn their idea into reality.</p>\n<p/>\n<p>Without further ado, let\u2019s meet the <a shape=\"rect\" href=\"https://www.openscienceprize.org/res/p/finalists/\">finalists</a>:</p>\n<p/>\n<ul>\n<li>\n<strong>Brainbox \u2013 </strong>The amount of brain imaging data available on the Internet is, well, mind-boggling. And compared to other types of data, neuroimaging data requires a substantial amount of human effort, such as curating and editing images. <a shape=\"rect\" href=\"https://www.openscienceprize.org/p/s/1838127/\">BrainBox</a> is an online laboratory designed to give researchers easy access to\u00a0brain imaging data (notably without downloading it) and to enable distributed collaboration so everyone can share in the effort.</li>\n</ul>\n<p/>\n<ul>\n<li>\n<strong>NeuroArch \u2013 </strong>Despite valiant efforts to <a shape=\"rect\" href=\"http://spectrum.ieee.org/biomedical/imaging/the-us-brain-initiative-boldly-begins\">map the entire human brain</a>, a more near-term goal is to map a smaller brain, such as that of a fruit fly<span>\u2014</span>which shares more than 70 percent of the genes involved in human brain disorders. The <a shape=\"rect\" href=\"https://www.openscienceprize.org/p/s/1998747/\">Fruit Fly Brain Observatory</a> project would develop an open graph database platform called NeuroArch to store and process information about the fly brain, including the location, shape, and connectivity of every neuron. With all that data in one place, it might be possible to generate a simulated fly brain and see what happens when it is altered via genetics or drugs.</li>\n</ul>\n<p/>\n<ul>\n<li>\n<strong>MyGene2 \u2013 </strong>Rare diseases aren\u2019t as rare as you think. More than 6,000 known rare diseases affect an estimated <a shape=\"rect\" href=\"https://report.nih.gov/nihfactsheets/ViewFactSheet.aspx?csid=80\">25 million</a> people in the U.S. today. Yet more than half of families who undergo genetic testing fail to get a diagnosis for a suspected rare disease. A website named <a shape=\"rect\" href=\"https://www.mygene2.org/MyGene2/\">MyGene2</a> provides a place for families and clinicians to share health and genetic information on rare diseases as a way to promote the diagnosis and discovery of new rare conditions and the genes that cause them.</li>\n</ul>\n<p/>\n<ul>\n<li>\n<strong>Nextstrain </strong>\u2013 To intervene and stop the outbreak of an epidemic, scientists need to get their hands on genomic data from viral pathogens as soon as possible. The <a shape=\"rect\" href=\"http://nextstrain.org/\">Nextstrain</a> project pools genetic data from research groups around the world to\u00a0visualize\u00a0the spread of a virus in near real-time. For example, check out their graphic of\u00a0the current evolution\u00a0of the\u00a0<a shape=\"rect\" href=\"http://nextstrain.org/zika/\">Zika virus</a>.</li>\n</ul>\n<p/>\n<ul>\n<li>\n<strong>OpenAQ</strong> \u2013 According to the World Health Organization, air pollution exposure is responsible for <a shape=\"rect\" href=\"http://www.who.int/mediacentre/news/releases/2014/air-pollution/en/\">one in eight</a>\u00a0global deaths, yet air quality data has traditionally been stored on obscure websites that are difficult to access and have inconsistent formats. The <a shape=\"rect\" href=\"https://openaq.org/#\">OpenAQ platform</a> prototype aggregates and standardizes publicly available, real-time air quality data. It has already collected and shared 9.7 million air quality measurements from 500+ locations in 13 countries.</li>\n</ul>\n<p/>\n<ul>\n<li>\n<strong>OpenTrialsFDA \u2013 </strong>When the U.S. Food and Drug Administration approves a drug, the agency publically publishes a package of information about that drug, often including previously unpublished clinical trials. Though this information is quite valuable, it is notoriously difficult to access, aggregate and search. <a shape=\"rect\" href=\"https://www.openscienceprize.org/p/s/1844843/\">OpenFDA</a> is an effort to build a user-friendly web interface to enable anyone to access the data, plus APIs to allow third-party platforms to tap into and search the data.\u00a0</li>\n</ul>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/_UtVJger54s\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Thu, 12 May 2016 17:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/the-human-os/biomedical/diagnostics/competition-seeks-to-solve-biomedicines-big-data-problem</guid>\r\n      <dc:creator>Megan Scudellari</dc:creator>\r\n      <dc:date>2016-05-12T17:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MTUwNg\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MTUwNg\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/the-human-os/biomedical/diagnostics/competition-seeks-to-solve-biomedicines-big-data-problem</feedburner:origLink></item>\r\n    <item>\r\n      <title>Microspines Make It Easy for Drones to Perch on Walls and Ceilings</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/XEUTEg7woSE/microspines-make-it-easy-for-drones-to-perch-on-walls-and-ceilings</link>\r\n      <description>Perching drones can extend their useful mission life from minutes to hours or days</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"quad_perch_hed-1462941798114.jpg\" src=\"http://spectrum.ieee.org/img/quad_perch_hed-1462941798114.jpg\"/>\n<figcaption class=\"hi-cap\">Photo: Stanford</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>\n<em>\n<a style=\"background-color: transparent;\" shape=\"rect\" href=\"http://bdml.stanford.edu/Profiles/MorganPope\">Morgan Pope</a>\u00a0is a PhD student investigating robots that live at the boundary of airborne and surface locomotion at Stanford\u2019s Biomimetics and Dexterous Manipulation Lab. He wrote about <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/drones/stanfords-flying-perching-scamp-can-climb-up-walls\">SCAMP, a flying and perching robot</a>, for Automaton earlier this year.</em>\n</p>\n<p>\n<span id=\"docs-internal-guid-6b8afe5e-9e11-6edd-95f8-75a574636d83\">\n<span>A disaster site. A rainforest. A battlefield. These places have something in common: we have a need to understand what\u2019s going on where established infrastructure can\u2019t give us good data. Advances in computation, fabrication, and materials over the last half-century have resulted in small, cheap, and lightweight sensors that can provide us with these data; now the task is to find ways to deploy such sensors rapidly and effectively. \u00a0</span>\n</span>\n</p>\n<p>\n<span id=\"docs-internal-guid-6b8afe5e-9e11-6edd-95f8-75a574636d83\">\n<span>One way to do this is with small, agile aerial vehicles like quadrotors. Quadrotors are becoming affordable, ubiquitous platforms that can fly quickly over rugged terrain to collect critical data. There\u2019s a catch, though: most small (less than 1 meter in diameter) quadrotors can only stay in the air for tens of minutes at a time, and this limited endurance makes some missions unachievable. However, if the goal is to collect data from a fixed vantage point, there is an alternative to hovering in place that might extend mission life from minutes to days or even\u00a0 longer: perching.</span>\n</span>\n</p>\n<p>\n<span>\n<span>Perching allows a quadrotor to shut down its power-hungry motors and let its sensors get to work acquiring data over an extended period of time, tracking parameters like the stability of a building after an earthquake, the nocturnal activity of a jaguar, or enemy troop movements. While perched, the quadrotor can also happily continue to operate in weather conditions that would make flying impossible. At <a shape=\"rect\" href=\"http://bdml.stanford.edu/\">Stanford\u2019s Biomimetics and Dexterous Manipulation Laboratory</a>, we have been working on perching with the goal of making landing on a wall as easy as landing on the ground. By adding a few grams of structure and mechanism to an off-the-shelf commercial quadrotor, we are now able to perch on both vertical and inverted surfaces without using any special firmware or flying techniques. While it\u2019s still not as foolproof as landing on a level surface, we are closer than ever to making perching accessible outside of a research environment.</span>\n</span>\n</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/7cKlbmfY6cQ?rel=0\"/>\n</p>\n<p>\n<span id=\"docs-internal-guid-6b8afe5e-9e11-6edd-95f8-75a574636d83\">\n<span>This solution originated when <a shape=\"rect\" href=\"http://bdml.stanford.edu/Profiles/HaoJiang\">Hao Jiang</a>, a graduate student in our lab, took a closer look at another recent result from our group, <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/drones/stanfords-flying-perching-scamp-can-climb-up-walls\">a perching/climbing robot I built called SCAMP</a>\n</span>\n<span>, which uses a climbing mechanism mounted on top of the robot. \u00a0Mounting the mechanism in this way meant that the thrust from the robot\u2019s rotors actively push the robot onto the wall during perching, assisting with the maneuver. By first impacting the wall with a rigid carbon fiber \u201ctail,\u201d\u00a0SCAMP creates\u00a0a stable pivot point to transition from level flight to pitching up parallel with the surface. </span>\n</span>\n</p>\n<p>\n<span id=\"docs-internal-guid-6b8afe5e-9e11-6edd-95f8-75a574636d83\">\n<span>SCAMP also used an on-board computational routine to detect impact and actively pitch the quadrotor into the surface. However, as Hao and I investigated this behavior, we realized that this active maneuver wasn\u2019t strictly necessary. If Hao could outfit the machine with a gripping system that was centrally located between all four rotors and hit the wall with a reasonable incoming velocity, the physics of the system should consistently lead to good contact with the surface.</span>\n</span>\n</p>\n<p>\n<span id=\"docs-internal-guid-6b8afe5e-9e11-6edd-95f8-75a574636d83\">\n<span>To test out this new perching technique, Hao built a gripping system that, while unable to climb like SCAMP, has the advantage of being centrally located and capable of attaching to an inverted surface like a ceiling (a useful feature when you want to get your robot out of the rain). He then distilled the mechanical design of SCAMP into a simple tail structure that helps the robot engage with the wall. After some trial and error, Hao found that he could perch successfully by simply flying the quadrotor straight into the wall: as long as the quadrotor is moving at a reasonable pace and is pitched forward (usually the case when flying forward) and squared up with the target surface, the rotors reliably bring the mechanism into contact to engage the wall using an opposed grip.</span>\n</span>\n</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTk1Mg\"/>\n</div>\n<p>\n<span id=\"docs-internal-guid-6b8afe5e-9e11-6edd-95f8-75a574636d83\">\n<span>This opposed grip strategy works by dragging two sets of microspines\u2014hardened steel spikes on a special suspension\u2014 along a surface in opposite directions. The spines catch against tiny bumps and pits on the surface and hang on using friction. Pulling the opposed sets of spines against each other produces a tight grip that lets the quadrotor land not only on vertical walls, but also on angled or overhanging surfaces. As Hao explains it, \u201cthe opposed-grip strategy for microspines is just like a human hand grasping a bottle of water, except that while humans require some macroscopic curvature to get our fingers around both sides of an object, the microspines can go deep into the micro-features of a rough surface and latch on those tiny bumps and pits.\u201d When the frequency of small bumps and pits is high, as with stucco or cinderblock, the grip is more reliable than on surfaces like polished concrete. Soon after perching on the wall, Hao started flying the quadrotor up into ceilings, and found that the inverted surfaces were often even easier to perch on because the quadrotor was already aligned to the surface in its normal flight configuration.</span>\n</span>\n</p>\n<aside class=\"inlay pullquote rt med-lrg\">\u201cThe opposed-grip strategy for microspines is just like a human hand grasping a bottle of water, except that while humans require some macroscopic curvature to get our fingers around both sides of an object, the microspines can go deep into the micro features of a rough surface and latch on those tiny bumps and pits.\u201d</aside>\n<p>\n<span id=\"docs-internal-guid-6b8afe5e-9e11-6edd-95f8-75a574636d83\">\n<span>Once perched securely on a wall, Hao had to figure out how to enable the quadrotor to release and fly off. On an inverted surface, it was easy: a servo releases the spines and gravity does the rest. On a vertical surface, it took a little more thought to make sure the quadrotor would rotate away from the wall properly, since the thrust from the rotors tends to keep the robot tucked tightly against the surface. As Hao explains, the solution was to add some spines on the tail: \u201cDuring take-off, as the mechanism is released and the quadrotor starts to fall, the microspines on the tail catch on bumps and pits [on the surface] again. The quadrotor then pivots on the tail spines backward away from the surface, and can fly away.\u201d</span>\n</span>\n</p>\n<p>\n<span id=\"docs-internal-guid-6b8afe5e-9e11-6edd-95f8-75a574636d83\">\n<span>Speaking about challenges and future ways to address them, Hao says that, \u201ceven if the perching strategy is robust, the quadrotor can still fail to perch due to improper choices of target surfaces. While we have achieved robust perching failure detection and recovery for indoor environments, we will investigate failure recoveries for outdoor applications, possibly with wind disturbances and surface uncertainties.\u201d We are also interested in pursuing new gripping strategies and possibly combining microspines with dry adhesives to stick to a larger range of surfaces. We\u2019re excited by the recent advances in perching and are hoping to refine our approach even further, so that people can start putting sensors where they need them the most.</span>\n</span>\n</p>\n<p>\n<span>\n<span>[ <a shape=\"rect\" href=\"http://bdml.stanford.edu/\">Stanford BDML</a> ]</span>\n</span>\n</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/XEUTEg7woSE\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Thu, 12 May 2016 16:24:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/automaton/robotics/drones/microspines-make-it-easy-for-drones-to-perch-on-walls-and-ceilings</guid>\r\n      <dc:creator>Morgan Pope</dc:creator>\r\n      <dc:date>2016-05-12T16:24:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MjU4Ng\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MjU4Ng\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/automaton/robotics/drones/microspines-make-it-easy-for-drones-to-perch-on-walls-and-ceilings</feedburner:origLink></item>\r\n    <item>\r\n      <title>This Wearable Is a Cure for Motion Sickness and Is Changing My Life</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/hK8H0bo8K28/this-wearable-is-a-cure-for-motion-sickness-and-is-changing-my-life</link>\r\n      <description>The first consumer generation of the ReliefBand is too big, ugly, and looks and feels like a cheap toy. And I will never again leave the house without it.</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"ReliefBand2-1462976370571.jpg\" src=\"http://spectrum.ieee.org/img/ReliefBand2-1462976370571.jpg\"/>\n<figcaption class=\"hi-cap\">Photo: Tekla Perry</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>I suffer from motion sickness. Bad motion sickness.\u00a0 Non-medicated, I can\u2019t be a passenger in a car for more than 10 minutes of city driving\u20145 minutes on a windy road\u2014without feeling sick. Virtual reality? Forget it, even with a double-dose of sleep-inducing medication, I can only spend about eight seconds in VR without the warning signs of an impending wave of nausea. (That\u2019s a big problem for a journalist trying to cover consumer electronics these days.) I\u2019ve tried chomping on ginger and ginger-filled capsules, pushing on acupressure points, and all sorts of prescription and non-prescription pharmaceuticals, settling on meclizine as my go-to; trading off 24 hours of annoying but not intolerable sleepiness and a slightly fuzzy brain for a calm stomach.</p>\n<p>So when I spotted the ReliefBand booth at CES this year, touting a wearable that uses electric pulses to block the neurological signals that kick off motion sickness, I was eager to try the device\u2014but was more than skeptical. I was sure that it wasn\u2019t going to work, because if this really were possible, wouldn\u2019t someone have thought of it years ago?</p>\n<p>It turns out that someone did\u2014about 20 years ago, and it\u2019s been marketed for at least a decade as a prescription device to hospitals. And the various companies that owned the technology along the way made occasional attempts to break into the consumer market.</p>\n<p>But more on that later.</p>\n<p>What you probably really want to know is if the <a shape=\"rect\" href=\"http://shop.reliefband.com/\">$90 gadget</a> works. And oh yeah, it works. Amazingly, quickly, absolutely. And it is changing my life.</p>\n<p>I first got an evaluation unit from ReliefBand in February. That evening, my husband and I had tickets a show in San Francisco; in rush hour, that\u2019s anywhere from an hour to an hour and a half away, much of it in heavy traffic. I hate navigating the traffic and the city driving, so my usual plan would have been take a meclizine earlier in the day and hope it doesn\u2019t knock me out during the show. I decided to instead try the ReliefBand, warning my husband that if it doesn\u2019t work it could be a rough evening.</p>\n<p>The device isn\u2019t intuitive to use; even though the brochure says \u201cslip it on your wrist\u201d you have to follow instructions, though they are straightforward: measure two finger widths below the crease of your wrist and feel for a spot between the two tendons, rub in some of the conductive gel included with the product, then strap it on. Push the on button; push it again to raise the level of electricity until you feel it down to your middle fingertip: there are five levels, I felt a noticeable, but not uncomfortable, tingle at level 2.</p>\n<p class=\"jwcode\">\n<script class=\"jwembed\" src=\"//content.jwplatform.com/players/MXj5kUGk-7pFgM9ap.js\"/>\n</p>\n<p>It was a hairier drive to the city than usual that evening\u2014due to road closures for Super Bowl events in San Francisco, we had to turn the navigation over to Waze, which sent us winding through narrow alleys and making frequent turns to get to our destination. To my shock, I felt fine. Absolutely fine. And my husband was feeling pretty good as well. \u201cThat\u2019s the first time we\u2019ve ever gone anywhere that you haven\u2019t at least sighed, if not complained, when I came to a fast stop,\u201d he told me. I turned the ReliefBand off as soon as we parked, and felt that momentary pre-nausea feeling. I turned it back on, and it went away. I decided to give myself five minutes to stabilize as we walked to the theater, after that, I was fine.</p>\n<p>A few days later, I was driving my son over to Santa Cruz on Highway 17\u2014a twisty mountain road, the motion effects worse on a sunny day as the light strobes through trees. I was fine, because I was driving. He isn\u2019t super sensitive to motion, though mountain roads sometimes affect him, and checking his phone for texts doesn\u2019t help. About half way there he put his phone down and stared straight ahead. \u201cMom, I think I\u2019m getting sick.\u201d</p>\n<p>\u201cI have that gadget in my purse if you want to try it.\u201d I talked him through putting it on. And five minutes later he was madly texting (I\u2019m not so sure that\u2019s a win, but at least he wasn\u2019t sick.)</p>\n<p>Since then, I\u2019ve been using the ReliefBand in just about every situation in which I would normally pre-medicate with meclizine. In April, we took a cruise. Let\u2019s just say I took far more advantage of the nightlife this time, not having to fight the meclizine sleepies. I hardly turned it on after the first day\u2014cruise ships are pretty stable until they aren\u2019t\u2014at which point, I\u2019ve learned on past trips, it\u2019s too late to medicate. The only time I medicated (and got sick anyway) was for a snorkel trip, not exactly a situation in which I could wear an external electrical device. However, though I did get sick, I do believe the gadget helped me recover faster\u2014typically, I would have been sick all afternoon, and needed at least a two-hour nap to reset my stomach. I did lie down and close my eyes for a while, but using the wearable I was happily eating lunch in a little over an hour.</p>\n<p>These days, I also use a ReliefBand in situations that I wouldn\u2019t have medicated for, but that sometimes prove stressful\u2014like all cab and Uber rides, even short ones (you never know if the driver or road conditions are going to be motion-sickness friendly).</p>\n<p>Finally, in May I gave ReliefBand the ultimate test: virtual reality. As a <em>New York Times</em> subscriber, I received Google\u2019s cardboard VR glasses, a low-cost version with a relatively sluggish response to real-world motion guaranteed, for me, to trigger nausea in seconds. I won\u2019t say ReliefBand solved the problem, but I was able to tolerate about half a minute before the symptoms got worrisome; that leads me to think that I might be able to spend a few minutes with ReliefBand and a better VR system.</p>\n<p>While I\u2019m clearly in love with the gadget\u2019s function, its form leaves a lot to be desired. It\u2019s too big and the hard plastic shell makes it seem like a cheap\u00a0 toy\u2014it reminds me a lot of a wearable \u201ccompass\u201d that came in a $10 explorer\u2019s kit for four year olds (along with useless binoculars and a canteen). Still, wearables are so common these days that even chatty Uber drivers didn\u2019t question what I was fiddling with on my wrist, assuming it\u2019s some kind of sport watch<span style=\"display:none\">\u00a0 </span>. And the small flexible battery door is a disaster, nearly impossible to close once opened.</p>\n<p>CEO Nick Spring admits to all of this. In addition to my criticisms, he pointed out that the strap is hard to manage one-handed; that\u2019s because, he says, it was designed for nurses to put on patients, not for people to use themselves.</p>\n<p>Here\u2019s that backstory I promised. Nearly 20 years ago, Spring said, a group of researchers at a California company, Woodside Biomedical, working with researchers at Penn State University, came up with a device called Accutens that, they determined, by electrically stimulating the T6 acupuncture point on the wrist, worked as well as current medications to prevent motion sickness.\u00a0 In the early 2000s, Abbott Laboratories purchased Woodside and began developing the technology into a product. Abbott later spun Woodside out again. Along the way, the device received FDA clearance for use in chemotherapy and post-operative sickness, and made some inroads into hospitals as a prescription-only device; a later prescription-only version, the PrimaBella, got some traction as a treatment for morning sickness. A nonprescription version was marketed at various times to pilots and boaters. The current version is an update of that model.</p>\n<p>Spring came on the scene in late 2014, joining the company and leading a management buyout in July of 2015, at which point the company changed its name to ReliefBand Technologies and by December had scooped up US $5 million in venture capital.</p>\n<p>\u201cWith the evolution of Fitbit and other wearables,\u201d Spring said, \u201cwearable technology has come of age. And, it seemed, the next evolution of the market, from wearables that measure to wearables that actually provide therapy, was about to come. ReliefBand offered that kind of solution, though the design was pretty basic.\u201d Spring also said attention that\u2019s lately come to neuromodulation technology has made people more comfortable with the idea of using electricity to affect the nervous system.</p>\n<p>Now, he says, the company is in the midst of a major redesign, updating the electronics to reduce the size, changing the material, switching to a rechargeable battery, making it water resistant\u2014essentially, turning it, he hopes, into a hip-looking wearable. Future versions, he says, will communicate to mobile phones, relying on smarts in the phone to determine when your world is rocking and turn the wearable on automatically. What he\u2019s not changing, he says, is the algorithm that triggers the electric pulses. \u201cIf you stimulate the nerve constantly, it stops reacting,\u201d Spring says. ReliefBand says it has figured out how to time the pulses so they are frequent enough to block nausea but not so frequent as to have the nerve tune them out.</p>\n<p>Spring expects to unveil the new version at CES 2017. I plan on being one of the first in line to buy it, and if it\u2019s cute enough, it just may join the Fitbit on my wrist daily. In the meantime, I carry the current version with me, ready to strap on, always.</p>\n<p>\n<em>Correction made 12 May 2016.</em>\n</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/hK8H0bo8K28\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Wed, 11 May 2016 17:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/gadgets/this-wearable-is-a-cure-for-motion-sickness-and-is-changing-my-life</guid>\r\n      <dc:creator>Tekla S. Perry</dc:creator>\r\n      <dc:date>2016-05-11T17:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MjA2Mg\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MjA2Mg\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/view-from-the-valley/consumer-electronics/gadgets/this-wearable-is-a-cure-for-motion-sickness-and-is-changing-my-life</feedburner:origLink></item>\r\n    <item>\r\n      <title>Volvo's Self-Driving Program Will Have Redundancy For Everything</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/MTMPUvDIiXE/volvos-selfdriving-program-will-have-redundancy-for-everything</link>\r\n      <description>The cars will have two independent motors to be sure that at least one of them turns the steering wheel</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"VolvoAutonomousdriving-1462974184827.jpg\" src=\"http://spectrum.ieee.org/img/VolvoAutonomousdriving-1462974184827.jpg\"/>\n<figcaption class=\"hi-cap\">Photo: Volvo</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Next year <a shape=\"rect\" href=\"http://www.volvo.com\">Volvo</a> will do something no other company has tried: it will put 100 fully self-driving cars in the hands of customers. The tests, which will begin small and ramp up slowly, are to be held in Gothenberg, Sweden and <a shape=\"rect\" href=\"https://www.media.volvocars.com/global/en-gb/media/pressreleases/189969/volvo-cars-to-launch-uks-largest-and-most-ambitious-autonomous-driving-trial\">in London.</a>\n</p>\n<p>It\u2019s a lot harder than it may seem. True, the cars will be in self-driving mode only in the testing area and\u00a0only\u00a0when driving conditions permit. But they\u2019ll be production cars, not experimental prototypes, and\u00a0the drivers will be laymen, not engineers, with full ability\u00a0to sit back and read a book, not <a shape=\"rect\" href=\"http://spectrum.ieee.org/cars-that-think/transportation/self-driving/tesla-robocar-to-driver-accept-the-liability-buster\">continually putting a hand on the steering wheel</a> to prove they\u2019re capable of intervening at a moment\u2019s notice. There\u2019s no human being to serve as backup.</p>\n<p>So far,\u00a0driver-assistance\u00a0features have enjoyed the inherent redundancy that\u00a0comes from having a\u00a0human being behind the wheel. \u00a0\u201cIf antilock brakes fail in a car today, the driver steps on the brake,\u201d says Erik Coelingh, a leader of the project. \u201cIn a self-driving car, if ABS fails you need a backup ABS. \u00a0You need two systems for everything.\u201d</p>\n<p>Volvo, a\u00a0brand\u00a0with the word\u00a0\u201csafety\u201d inscribed in it, is insisting on full backup for every element before it allows one of its cars full command. Take the company\u2019s\u00a0latest\u00a0active safety feature, an emergency steering system in\u00a0the 2017\u00a0S90 <a shape=\"rect\" href=\"http://www.cnet.com/roadshow/news/volvo-prices-s90-rolls-out-online-purchasing-service/\">that\u00a0senses if the car\u2019s\u00a0about to leave the road </a>and\u00a0takes control. And, if\u00a0the motor that\u2019s supposed to twist the wheel should fail, there\u2019s a backup.\u00a0</p>\n<p>\u201cThere are two separate windings\u2014essentially two independent motors in an integrated package,\u201d Coelingh says. \u201cEach winding has its own ECU [electronic control unit], its own power source, its own battery.\u201d</p>\n<p>What everybody in the business agrees on is the need for redundant sensing systems. \u201cYou have to combine sensors with different physical principles and combine them to compensate for each one\u2019s disadvantages,\u201d he says. \u201cRadar is very much becoming a commodity; camera vision is developing really fast, in part thanks to the development of deep neural networks; and lower-cost LIDAR is coming.\u201d</p>\n<p>Volvo\u2019s test cars\u00a0in Gothenberg and London\u00a0will each have a small LIDAR unit from an unspecified vendor. The unit will\u00a0be integrated inconspicuously into the front of the car from where it will scan the road with four beams, unlike the rotating beacons seen on the roofs of Google cars. It will cost far less than the $10,000-plus price of those beacons, also.</p>\n<p>High-definition maps of the Gothenberg roads should offer 10-centimeter (4-inch) resolution. Right now the maps are drawn in-house, but when the time comes to expand beyond Gothenberg Volvo will rely on other methods. \u201cIn the long run we\u2019ll need crowdsourcing to get up-to-date information on road conditions,\u201d Coelingh says. \u201cWe <a shape=\"rect\" href=\"http://www.computing.co.uk/ctg/news/2455036/volvo-analytics-chief-i-think-teradata-is-giving-value-for-money-but-there-s-a-journey-ahead\">ran a big pilot</a> this winter that used hundreds of cars to measure road friction, sending every reading of slippery conditions to the cloud, so that other cars could share it.\u201d</p>\n<p>Such real-world validation is key. \u201cYou have to drive quite a lot to make sure that automatic steering doesn\u2019t kick in when you\u2019re not about the leave the road,\u201d he says. \u201cIf we get less than one accident in 500,000 kilometers of driving, that\u2019s sufficient.\u201d\u00a0</p>\n<p>Every step will involve voluminous testing, first under relatively propitious conditions and then progressing through more challenging ones. Volvo hopes the Gothenberg tests will lead to a rollout of a commercial product sometime around\u00a02020,\u00a0though even that futuristic vehicle\u00a0won\u2019t work on all roads or under\u00a0all weather conditions. Not at first, anyway.</p>\n<p>And when Volvo does\u00a0sell a self-driving car, Coelingh says, it will accept full legal responsibility for any accidents that may come. \u201cAnd that is reasonable,\u201d he says, \u201cbecause we told the customer he could do something else while the car drives itself.\u201d</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/MTMPUvDIiXE\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Wed, 11 May 2016 16:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/cars-that-think/transportation/self-driving/volvos-selfdriving-program-will-have-redundancy-for-everything</guid>\r\n      <dc:creator>Philip E. Ross</dc:creator>\r\n      <dc:date>2016-05-11T16:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MjAzMA\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MjAzMA\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/cars-that-think/transportation/self-driving/volvos-selfdriving-program-will-have-redundancy-for-everything</feedburner:origLink></item>\r\n    <item>\r\n      <title>Gauntlets of Levitation, Living Desktops, and More: Video Highlights from The 2016 Human-Computer Interaction Conference</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/740UVLD1x9w/gauntlets-of-levitation-living-desktops-and-more-video-highlights-from-the-2016-humancomputer-interaction-conference</link>\r\n      <description>Some of the craziest computer interaction prototypes we've ever seen are on display this week in San Jose</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"HelloWorld-1462977354639.jpg\" src=\"http://spectrum.ieee.org/img/HelloWorld-1462977354639.jpg\"/>\n<figcaption class=\"hi-cap\">Image: Autodesk Research</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>The <a shape=\"rect\" href=\"https://chi2016.acm.org/wp/\">2016 Computer Human Interaction Conference</a> (CHI, which is pronounced \u201ckai\u201d like the Greek letter) is taking place this week in San Jose, Calif. The conference is all about the ways in which the future of interaction technology is advancing, and how it will shape the ways in which we experience our environment (and each other). Really, this is just a complicated way of saying that the conference provides a great excuse for researchers to explore new and crazy ways of using computers, and some of the stuff that they\u2019ve come up with will blow your mind. Think\u00a0power tools that tell you what to do with themselves, or a couch that\u2019s also a huge touch controller, or projection-augmented 3-D printing on your skin, or gloves that let you levitate objects: all of these are functional prototypes that researchers described at this year\u2019s conference.</p>\n<p>You can watch all 281 video previews <a shape=\"rect\" href=\"https://www.youtube.com/playlist?list=PLqhXYFYmZ-Vcui07v-6-TXiETq3Zk7YTt\">here</a>, or you can have a look at this baker\u2019s dozen of videos that we\u2019ve hand selected for overall future-ness, technical bewonderment, transformational\u00a0potential, and generalized weirditude.</p>\n<h2>GauntLev: A Wearable to Manipulate Free-floating Objects</h2>\n<p>Acoustic levitation can be used to trap small objects in ultrasonic fields, and by building ultrasonic emitter arrays into wearables, you can make yourself a \u201cGauntlet of Levitation\u201d as well as a \u201cSonic Screwdriver:\u201d</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/vLCrm6gos4w?rel=0\"/>\n</p>\n<p>Read more from\u00a0<a shape=\"rect\" href=\"http://big.cs.bris.ac.uk/projects/gauntlev-1\">the Bristol Interaction Group at the University of Bristol</a>.</p>\n<hr/>\n<h2>Haptic Retargeting: Dynamic Repurposing of Passive Haptics for Enhanced Virtual Reality</h2>\n<p>The best way to achieve a satisfying experience manipulating an object in virtual reality is to be manipulating a similar \u201cproxy\u201d object in reality and at the same time. If you want to manipulate many virtual objects, you\u2019d need many proxy objects, unless you can use haptic retargeting to manipulate users into thinking they\u2019re interacting with many objects when there\u2019s actually just one:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/SiH3IHEdmR0?rel=0\"/>\n</p>\n<p>Read the paper <a shape=\"rect\" href=\"http://markhancock.ca/pmwiki/uploads/Main/HapticRetargetingCHI2016_final.pdf\">here</a>.</p>\n<hr/>\n<h2>SATURNO: a Shadow-Pushing Lamp for Better Focusing and Reading</h2>\n<p>With a single light source, shadows cast by your body can get in the way of whatever you\u2019re doing. The SATURNO lamp can track your motions, and dynamically adjust its light output to reduce or eliminate shadows. It can do some other cool stuff, too:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/ipRIhJJtdkw?rel=0\"/>\n</p>\n<p>The extended abstract is available <a shape=\"rect\" href=\"http://delivery.acm.org/10.1145/2890000/2889436/ea9-jeong.pdf\">here</a>.</p>\n<hr/>\n<h2>TactileVR: Integrating Physical Toys into Learn and Play Virtual Reality Experiences</h2>\n<p>Kids are already absorbed in virtual worlds, and TactileVR takes the experience to its logical conclusion by allowing physical objects co-exist with virtually augmented versions of themselves. It looks like a lot of fun to play with, even if it\u2019s slightly creepy to watch:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/7hRl-LdGYPs?rel=0\"/>\n</p>\n<p>The extended abstract is available <a shape=\"rect\" href=\"http://delivery.acm.org/10.1145/2890000/2889438/ea1-amores.pdf\">here</a>.</p>\n<hr/>\n<h2>Drill Sergeant: Supporting Physical Construction Projects through an Ecosystem of Augmented Tools</h2>\n<p>I love these prototypes of power tools that use real-time interactive feedback to walk you through building things. Instead of reading the instructions, measuring twice, and cutting once, now you can just skip straight to the cutting step because your tools are doing the rest for you:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/y8Qvgf1dBqA?rel=0\"/>\n</p>\n<p>Read the paper <a shape=\"rect\" href=\"http://delivery.acm.org/10.1145/2900000/2892429/ea1607-schoop.pdf\">here</a>.</p>\n<hr/>\n<h2>bioSync: Synchronize Kinesthetic Experience among People</h2>\n<p>Using paired sensors and electrodes, bioSync allows you to \u201cjack into\u201d another person, such that when sensors detect them moving their muscles, electrodes force you to move yours, or vice versa. It\u2019s intended to replicate the experience of having a neuromuscular disease, and it certainly would never be used for anything else:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" webkitallowfullscreen=\"\" width=\"620\" src=\"https://player.vimeo.com/video/155394019?title=0&amp;byline=0&amp;portrait=0\" mozallowfullscreen=\"\"/>\n</p>\n<p>Read the paper <a shape=\"rect\" href=\"http://delivery.acm.org/10.1145/2900000/2890244/ea3742-nishida.pdf\">here</a>.</p>\n<hr/>\n<h2>Embodied Interactions for Novel Immersive Presentational Experiences</h2>\n<p>PowerPoint presentation a little dry? By putting yourself inside of your own presentation as an avatar, you can interact with the content of your slides in real-time with gestures as your audience giggles at you:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/3IqrLy7OL6s?rel=0\"/>\n</p>\n<p>Read the paper <a shape=\"rect\" href=\"http://delivery.acm.org/10.1145/2900000/2892501/ea1713-matulic.pdf\">here</a>.</p>\n<hr/>\n<h2>FlexTiles: A Flexible, Stretchable, Pressure-Sensitive Input Sensor</h2>\n<p>Having a couch that is also a soft and comfortable touch input device based on a scalable textile sensor seems like a fantastic idea:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/_srs0_krNSg?rel=0\"/>\n</p>\n<p>Read more from <a shape=\"rect\" href=\"http://mi-lab.org/projects/flextiles/\">the Media Interaction Lab at the University of Austria</a>.</p>\n<hr/>\n<h2>HotFlex: Post-print Customization of 3-D Prints Using Embedded State Change</h2>\n<p>3-D printing lets you physicalize designs very quickly, but if you don\u2019t get things just right, it still takes iterations to end up with something that you\u2019re happy with. By embedding heating elements into 3-D printed structures, the user can soften, remold, and reharden 3-D printed objects whenever they like.</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/i9V0KF74Ugk?rel=0\"/>\n</p>\n<p>Read more from <a shape=\"rect\" href=\"https://hci.cs.uni-saarland.de/research/hotflex/\">the Human Interaction Lab at Saarland University</a>.</p>\n<hr/>\n<h2>One-Dimensional Handwriting: Inputting Letters and Words on Smart Glasses</h2>\n<p>What\u2019s the minimum amount of input that it takes to write? One dimension (swiping back and forth) is apparently enough, and for input on something like Google Glass, that\u2019s all you get:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/l9FIXMPrjF8?rel=0\"/>\n</p>\n<p>Read the paper <a shape=\"rect\" href=\"http://delivery.acm.org/10.1145/2860000/2858542/p71-yu.pdf\">here</a>.</p>\n<hr/>\n<h2>Haptic Edge Display for Mobile Tactile Interaction</h2>\n<p>The most common way to hold your phone is by the edges, which presents an interaction opportunity through the use of an array of piezo actuators:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/xh5G-PDEsIQ?rel=0\"/>\n</p>\n<p>Read more from <a shape=\"rect\" href=\"http://www.sjjang.com/#!haptic-edge-display/a17g1\">Sungjune Jang at MIT</a>.</p>\n<hr/>\n<h2>ExoSkin: On-Body Fabrication</h2>\n<p>In case the only thing stopping you from 3-D printing on your own body was a lack of confidence in your ability to freehand draw:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/zFWH09dmJHc?rel=0\"/>\n</p>\n<p>Read more from <a shape=\"rect\" href=\"https://www.autodeskresearch.com/publications/exoskin-body-fabrication\">Autodesk</a>.</p>\n<hr/>\n<h2>LivingDesktop: Augmenting Desktop Workstation with Actuated Devices</h2>\n<p>What if your keyboard, mouse, and monitor could autonomously move themselves around your desktop? How cool would that be? Exactly this cool:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/hC3hDlUIZ6s?rel=0\"/>\n</p>\n<p>Read the paper\u00a0<a shape=\"rect\" href=\"http://delivery.acm.org/10.1145/2860000/2858208/p5298-bailly.pdf\">here</a>.</p>\n<hr/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/740UVLD1x9w\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Wed, 11 May 2016 15:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/tech-talk/consumer-electronics/gadgets/gauntlets-of-levitation-living-desktops-and-more-video-highlights-from-the-2016-humancomputer-interaction-conference</guid>\r\n      <dc:creator>Evan Ackerman</dc:creator>\r\n      <dc:date>2016-05-11T15:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MjA4Ng\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MjA4Ng\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/tech-talk/consumer-electronics/gadgets/gauntlets-of-levitation-living-desktops-and-more-video-highlights-from-the-2016-humancomputer-interaction-conference</feedburner:origLink></item>\r\n    <item>\r\n      <title>Looking Ahead 4 Tomorrow's Materials</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/aPDzr9GQDbI/looking-ahead-4-tomorrows-materials</link>\r\n      <description>The race is on \u2013 for stronger lighter and more sustainable materials</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<p>\n<strong>\u201c</strong>\n<strong>Materials have always been about enthusiasm. Materials are a dream that we have realized. Everything is possible, it\u2019s just that we have to make it happen,\u201d says </strong>\n<strong>Mark Miodownik, the author behind the popular book <em>Stuff Matters</em>, in the </strong>\n<strong>fourth film of the Looking Ahead series<em>.</em>\n</strong>\n</p>\n<p/>\n<p>\n<strong>The age of new materials</strong>\n</p>\n<p>Throughout history, materials and advances in material technology have influenced humankind. Now we just might be on the verge of the next shift in this type of technology, enabling products and functions we never believed possible.</p>\n<p/>\n<p>Demands from industry are requiring that materials be lighter, tougher, thinner, denser and more flexible or rigid, as well as to be heat- and wear-resistant. At the same time, researchers are pushing the boundaries of what we imagine is possible, seeking to improve and enhance existing materials and at the same time come up with completely new materials that, while years away from day-to-day use, take us down entirely new technological pathways.</p>\n<p/>\n<p/>\n<p/>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"/image/Mjc1MjE4Nw\" src=\"http://spectrum.ieee.org/image/Mjc1MjE4Nw\"/>\n<div class=\"ai\"/>\n</figure>\n<p>\n<strong>The sky is the limit</strong>\n</p>\n<p>Based on the research we\u2019re seeing today, the field of applied material science is set to move in new, almost science-fiction-like directions. Looming resource scarcity is demanding innovations and out-of-the-box thinking.</p>\n<p/>\n<p>On the materials front, composites with such desirable attributes as low weight, high strength and high durability look likely to take a larger market share, and more of these materials will likely be based on renewable resources, as the need for this becomes greater.</p>\n<p/>\n<p>The most promising jewel in this arena is graphene. Only a single atom thick (1 million times thinner than a human hair), but 200 times stronger than steel by weight, extremely flexible, super light and almost transparent with great heat and electricity conductivity. It\u2019s the stuff legends are made of.</p>\n<p/>\n<p>In fact, researchers at Nankai University in Tianjin, China, recently found that a graphene sponge can turn light into energy, thus taking humankind one step closer to a fuel-free spacecraft, one that runs by the light of the sun.</p>\n<p/>\n<p>\n<strong>Heading for the graphene revolution</strong>\n</p>\n<p>Other potential areas of application for graphene range from water purification and energy storage to household goods, computers and other electronics. Meanwhile, although graphene-related patents are increasing by the thousands, widespread industrial adoption of graphene is limited by the expense of producing it \u2013 but that may be about to change. Researchers at the University of Glasgow have found a way to produce large sheets of graphene at a cost some 100 times cheaper than the previous production method.</p>\n<p/>\n<p>Synthetic skin, capable of providing sensory feedback to people with limb prostheses, is one of the many possibilities that could grow out of this development. \u201cGraphene could help provide an ultraflexible, conductive surface that could provide people with prosthetics capable of providing sensation in a way that is impossible for even the most advanced prosthetics today,\u201d says Dr. Ravinder Dahiya, who led the research team at the University of Glasgow.</p>\n<p/>\n<p/>\n<p/>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"/image/Mjc1MjE4OA\" src=\"http://spectrum.ieee.org/image/Mjc1MjE4OA\"/>\n<div class=\"ai\"/>\n</figure>\n<p>\n<strong>If it\u2019s broken, let it fix itself</strong>\n</p>\n<p>Nanocomposite research is opening up the possibility of materials that fix themselves, much the way the human body heals itself. Researchers at the Beckman Institute\u2019s Autonomous Materials Systems Group at the University of Illinois in the United States are working on fiber-composite materials with self-healing properties that involve the integration of healing agents that are released to mix and polymerize when a defect is detected.</p>\n<p/>\n<p>\u201cMaterials that heal themselves are coming,\u201d says material scientist Mark Miodownik in the new Looking Ahead film, <a shape=\"rect\" href=\"https://www.youtube.com/watch?v=SkM_1Pdj8xM\">\n<em>Tomorrow\u2019s materials</em>\n</a>. For now, what\u2019s technically possible isn\u2019t close to being reasonable economically, but the possibility of fixing anything on the fly, from airplane wings to bike frames to car parts crucial to the safety of vehicles and passengers, is on the horizon. And it will have massive impact on product development, life cycle and sustainability. Researchers are even working on materials that will allow a roadway to repair itself instead of waiting for an overworked, understaffed maintenance crew.</p>\n<p/>\n<p/>\n<p/>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"/image/Mjc1MjE4OQ\" src=\"http://spectrum.ieee.org/image/Mjc1MjE4OQ\"/>\n<div class=\"ai\"/>\n</figure>\n<p>\n<strong>Sustainability as a key driver</strong>\n</p>\n<p>Material science and the development of new materials, as well as improvement of existing ones, look likely to play a crucial role in such areas as resource scarcity and sustainability. New materials \u2013 for example, light-absorbing building materials \u2013 could help counter global warming.</p>\n<p/>\n<p>We seem to be on the verge of a new age, one that is characterized not only by digitalization and the Internet of Things but also, importantly, by new materials \u2013 materials that can make our future easier, safer and more sustainable. The sky really is the limit.<br clear=\"none\"/>\n<br clear=\"none\"/>\nFor more information, please click <a shape=\"rect\" href=\"http://www.sandvik.coromant.com/en-gb/aboutus/lookingahead/pages/age-of-new-materials.aspx?utm_source=ieee-spectrum&amp;utm_medium=sponsored-content&amp;utm_content=landing-page&amp;utm_campaign=la4\">here</a>.</p>\n<hr/>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"/image/Mjc1MjE5MA\" src=\"http://spectrum.ieee.org/image/Mjc1MjE5MA\"/>\n<div class=\"ai\"/>\n</figure>\n<p>\n<strong>The Graphene Challenge \u2013 global innovation competition launched by Sandvik Coromant</strong>\n</p>\n<p>On a mission to discover who can realize the full potential of graphene, Sandvik Coromant has initiated a global competition for individuals to submit ideas of how the application of graphene could be utilized in a way that would revolutionize the modern household.</p>\n<p>Sandvik Coromant will consider all entries from high end industrial design to everyday useable items. Participants are asked to submit ideas based around household objects that highlight how they could be improved with the use of graphene as a next-generation, innovative and sustainable material.</p>\n<p>The competition concludes on May 26, 2016.<br clear=\"none\"/>\n<br clear=\"none\"/>\nFor more information, visit the <a shape=\"rect\" href=\"http://www.sandvik.coromant.com/en-gb/aboutus/lookingahead/Pages/the-graphene-challenge.aspx?utm_source=ieee-spectrum&amp;utm_medium=sponsored-content&amp;utm_campaign=la4\">Sandvik Coromant website</a>.</p>\n<p/>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/aPDzr9GQDbI\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Wed, 11 May 2016 14:30:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/video/semiconductors/materials/looking-ahead-4-tomorrows-materials</guid>\r\n      <dc:date>2016-05-11T14:30:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MTcwNA\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MTcwNA\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/video/semiconductors/materials/looking-ahead-4-tomorrows-materials</feedburner:origLink></item>\r\n    <item>\r\n      <title>Facebook Revises Bot Platform to Place Messenger Users Firmly in Control</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/DEAYVgMHdc0/facebook-aims-to-place-users-firmly-in-control-with-revisions-to-bot-platform</link>\r\n      <description>No one wants to deal with a pesky bot that won\u2019t shut up</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"Facebookmessengerbots2-1462915291892.jpg\" src=\"http://spectrum.ieee.org/img/Facebookmessengerbots2-1462915291892.jpg\"/>\n<figcaption class=\"hi-cap\">Image: Facebook</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Facebook\u2019s big announcement at the annual F8 developer conference in April was its unveiling of a bot platform that developers could use to build digital assistants that operate within Messenger. The move was meant to expand the functionality of the messaging service, now used by nearly a billion people worldwide, so that it can also deliver customized news and facilitate e-commerce.</p>\n<p>Immediate reactions were mixed, and the announcement spurred a lot of discussion about whether users would embrace this newest experiment from one of the world\u2019s largest tech companies. Commenters also mused about\u00a0 how Messenger bots might evolve to play a role in our daily lives. That could depend as much on Facebook\u2019s ability to seamlessly integrate new notifications and chats into the user expereince as on\u00a0the ability of developers to devise clever functions for<sup/>all those digital bots.</p>\n<p/>\n<p>To keep the bot platform under wraps, Facebook did not conduct any external tests before its release. Now, soon after launch, user and developer feedback is quickly reshaping its future. On Tuesday, at <a shape=\"rect\" href=\"http://techcrunch.com/event-info/disrupt-ny-2016/\">2016 TechCrunch Disrupt</a> in Brooklyn, N.Y., <a shape=\"rect\" href=\"https://www.linkedin.com/in/stanchudnovsky\">Stan Chudnovsky</a>, head of product for Messenger, said his team is already making some early revisions to its new bot world.</p>\n<p/>\n<p>In one update, Facebook addressed concerns over bot-generated spam. The company knows that if busy bots send too many notifications to users or don\u2019t deliver useful content, their behavior could erode the messaging service\u2019s current status as a highly valued link between friends.</p>\n<p/>\n<p>To prevent this, Chudnovsky\u00a0says Facebook swapped the original \u201cBlock\u201d button that appears in the upper right hand corner of each new bot chat to a \u201cManage\u201d option that permits users to choose the type of messages they wish to receive. \u201cGiving people more control seems to be what people want to have,\u201d he said.</p>\n<p/>\n<p>Facebook is also looking at ways to differentiate between messages that are immediately important to users (perhaps those sent by a friend or which include breaking news) and those that can be read later (the pesky bot variety). In practice, this could mean that the company designates a different ringtone or vibration pattern when users receive an urgent message, or that it simply filters certain messages from the instant stream and issues them in a group alert a few times throughout the day.</p>\n<p/>\n<p>Overall, Chudnovsky is pleased with the launch and said <a shape=\"rect\" href=\"http://techcrunch.com/2016/05/10/facebook-chatbot-analytics/\">more than 10,000</a> developers have begun building bots. He points out that more than 2,500 merchants on <a shape=\"rect\" href=\"https://www.facebook.com/shopify/\">Shopify</a>, the company\u2019s virtual marketplace, offer bot-based customer service. \u201cYou have a bunch of early signs that the platform is starting to work,\u201d he says.</p>\n<p/>\n<p>Of course, it\u2019s not yet clear to users or the company which bots will become most integral to users\u2019 lives. Chudnovsky compares that uncertainty to the early days of Apple\u2019s App Store, when many of today\u2019s most successful apps weren\u2019t yet obvious or even imagined.</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/DEAYVgMHdc0\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Tue, 10 May 2016 21:30:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/tech-talk/telecom/internet/facebook-aims-to-place-users-firmly-in-control-with-revisions-to-bot-platform</guid>\r\n      <dc:creator>Amy Nordrum</dc:creator>\r\n      <dc:date>2016-05-10T21:30:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MTg5NA\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MTg5NA\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/tech-talk/telecom/internet/facebook-aims-to-place-users-firmly-in-control-with-revisions-to-bot-platform</feedburner:origLink></item>\r\n    <item>\r\n      <title>Stanford AI Grads Launch Low(ish)-Cost Underwater Robot</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/PxSbtKkOf50/stanford-ai-grads-launch-lowish-cost-underwater-robot</link>\r\n      <description>This underwater drone is looking for work at fish farms\u2014and as a research robot at universities</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"SeadroneORobotixTekla-1462470765613.jpg\" src=\"http://spectrum.ieee.org/img/SeadroneORobotixTekla-1462470765613.jpg\"/>\n<figcaption class=\"hi-cap\">Photo: Tekla Perry</figcaption>\n<figcaption>Eduardo Moreno (right) and Shuyun Chung started O-Robotix to build low-cost underwater robots and other automated gear for fish farms.</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>\n<a shape=\"rect\" href=\"http://seadronepro.com/#landing\">SeaDrone</a>, the underwater robot coming out of a new company founded by two Stanford AI lab veterans, is aiming to make fish farming a lot easier\u2014particularly for smaller aquaculture operations\u2014by making underwater inspection cheaper and easier.</p>\n<p>The ocean ROV\u2019s story is not an unusual one for Silicon Valley:\u00a0two Stanford students meet over a lab bench, get an idea that something they\u2019d been tinkering around with for their themselves could be turned into a product and the basis of a company. It\u2019s a story Silicon Valley loves.</p>\n<p>Eduardo Moreno met Shuyun Chung in the Stanford AI lab in 2013. Moreno, in the thick of his studies for a master\u2019s degree in mechanical engineering, was working on underwater robot hardware in collaboration with King Abdullah University of Science and Technology in Saudi Arabia. Chung, a postdoctoral scholar, was working on the software for <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/humanoids/supraped-robots-will-use-smart-trekking-poles\">SupraPed robots</a>, which are\u00a0designed to walk over rough terrain using trekking poles. The two were assigned seats next to each other in the labs, even though their projects were vastly different.</p>\n<p>One day back in 2014, another researcher in the lab brought in\u00a0a DJI quadcopter that he had just purchased,\u00a0and showed off the drone\u2019s capabilities.</p>\n<p>\u201cI was amazed that this $600 robot had much better technology than underwater robots that cost $10,000 or $15,000,\u201d Moreno said. \u201cThat\u2019s when I realized how far behind underwater robots were.\u201d</p>\n<p>He sketched out a design for a small, low-cost, underwater observational robot that used many of the same parts\u2014like\u00a0<a shape=\"rect\" href=\"http://electronics.howstuffworks.com/brushless-motor.htm\">brushless DC motors</a>, cameras, inertial sensors, and batteries\u2014commonly used in consumer aerial drones. Moreno showed his sketch to workplace-neighbor Chung;\u00a0he was immediately intrigued, and asked how he could help turn the sketch into a useful product.</p>\n<p>Moreno was thrilled with the offer. For one, he says, \u201cOnce you get someone else to sign on to an idea, then you know you\u2019re not crazy.\u201d For another, he knew that to design really good robotics hardware you need to be creating the software at the same time, because it informs the design. Chung had the software chops that Moreno didn\u2019t. And, finally, Moreno says, he knew Chung was good at helping people, because he\u2019d already been helping Moreno with his homework assignments for his graduate classes.</p>\n<p>Before they began working on the detailed specs for the robot, Moreno took some Stanford entrepreneurship classes to try to figure out how to make an underwater drone into a commercial business. Figuring out the market, says Moreno, \u201ctook longer than expected. It\u2019s the hardest thing for someone with an engineering background to do, I think, putting on the marketing hat.\u201d</p>\n<p>They were thinking of marketing the gadget to city governments, for use in inspecting bridges and other infrastructure and in police work (looking for weapons or even bodies in lakes and rivers), until a Stanford professor told him that was a tough path, given government entities have a long and complicated purchasing process that is a red flag for potential investors. They explored the idea of inspection in general, but couldn\u2019t find an appealing niche that could serve as an entry point.</p>\n<p>Finally, they came to aquaculture. \u201cWe\u2019d looked at the markets for aerial drones, and agriculture is a big one. So we searched online to see if there was anything like that for aquaculture, and, it turns out, there is some underwater observational technology. But it\u2019s a really new market,\u201d Moreno said. In aquaculture, nets, lines, and anchors must be inspected regularly; most fish farmers today send divers out to perform this task.</p>\n<p>Market identified, they started focusing seriously on designing their underwater drone, and had their first prototype built in six months. They tossed it into a swimming pool at a local apartment complex.</p>\n<p>It worked better than they had hoped. They even tested it up against a commercial underwater observational robot, borrowed from a local dive shop, and their home-built drone was more stable and easier to control, even for novices. (The commercial systems are typically used to inspect ocean-based oil drilling platforms and pipelines, dams, pilings, and boats.)</p>\n<p>It was also one-quarter its commercial cousin\u2019s size, measuring just 0.3 by 0.25 meters. Enabling the compact size, Moreno said, is the choice of brushless motors. They also redesigned the underwater thruster into something that costs tens of dollars to build instead of\u00a0thousands. They followed that up by\u00a0\u00a0and using off-the-shelf\u00a0pressure sensors, humidity sensors, inertial navigation units, and camera modules\u2014all of which have come down dramatically in price in the past few years\u00a0thanks to demand from the mobile phone market. They put it all together, and waterproofed the entire system.</p>\n<figure class=\"lt lrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MDQwNw\"/>\n<figcaption class=\"hi-cap\">Photo: O-Robotix</figcaption>\n</figure>\n<p>The toughest thing for Chung was coming up with an easy-to-use control system that could handle different numbers of thrusters. That was a necessity because they decided early on that\u00a0they\u2019d have to offer versions at different price points. Generally, says Moreno, the system is image-based\u00a0and uses what the robot \u201csees\u201d in the camera as a navigation guide.</p>\n<p>Moreno\u2019s biggest challenge was creating the propellers. \u201cI had never done any propeller design before, or created molds for injection molding\u2014any of that. And I built something with 50 separate plastic parts that are mass producible.\u201d Designing an expensive robot, as he had done in the past, he says, \u201cis significantly easier.\u201d</p>\n<p>Chung and Moreno both worked on the electronics design. The drone sends an HD video stream to a tablet or smartphone; the mobile device also acts as the controller.</p>\n<p>At that point, they realized that they had a gadget that did pretty much everything commercial underwater observational robots do, but in a much smaller and vastly cheaper package. The only similar gadget out there was the OpenROV, an open source effort that offers kits for hobbyists. That system, Moreno said, is inexpensive to build and can move quickly, but smaller movements lack precision and it struggles to capture stable video.</p>\n<p>Now, O-Robotix is testing beta versions of its underwater robot, called the SeaDrone, at an offshore fish farm near the coast of Baja, Mexico. The company will have production versions of its hardware ready to ship by the end of the summer, ranging in price from $2700 to $3300 depending on the number of thrusters. More thrusters give\u00a0the drone operator finer control of the robot. \u201cWe\u2019re starting with inspection drones, but, long term, our goal is to automate multiple parts of the process of aquaculture, including feeding, maintenance, and cleaning,\u201d Moreno says.</p>\n<p>Though\u00a0<a shape=\"rect\" href=\"http://www.dailymail.co.uk/sciencetech/article-2575030/The-automated-fish-rigs-farm-Salmon-North-Sea.html\">large, comprehensive, automated systems</a> are on the market, Moreno says O-Robotix is aimed at small, independent fish farmers who don\u2019t have a million dollars to invest in a massive fish-factory, and instead want to gradually add low-cost technology.</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/DR1CY_lOZ3o\"/>\n</p>\n<p>While aquaculture will be the main market for SeaDrone, Moreno and Chung think there\u2019s another, smaller market out there\u2014one that is particularly meaningful to them: the education and research market. \u201cWorking on robotics research,\u201d Moreno says, \u201cwe would spend a significant amount of time either building hardware, or hacking something we bought to make it work for our purposes, instead of focusing on the research we really wanted to do.\u201d So, he says, they are selling a reconfigurable developer\u2019s kit designed for researchers. So far, researchers at MIT, Stanford, and several universities in China have purchased early versions, Moreno says.</p>\n<p>Moreno and Chung have been getting mentoring and other help this spring from the <a shape=\"rect\" href=\"http://startx.com/accelerator\">Start-X</a> accelerator. The company was to officially unveil its technology at the TechCrunch Startup Battlefield today.</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/PxSbtKkOf50\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Tue, 10 May 2016 20:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/view-from-the-valley/robotics/drones/stanford-ai-grads-launch-lowish-cost-underwater-robot</guid>\r\n      <dc:creator>Tekla S. Perry</dc:creator>\r\n      <dc:date>2016-05-10T20:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MDQ4Nw\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MDQ4Nw\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/view-from-the-valley/robotics/drones/stanford-ai-grads-launch-lowish-cost-underwater-robot</feedburner:origLink></item>\r\n    <item>\r\n      <title>2-D Semiconductor Glows 20,000 Times as Brightly as Ever Before</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/-kk7LFEZVww/2d-semiconductor-glows-20000-times-more-brightly</link>\r\n      <description>Plasmonic nanostructures push tungsten diselenide to massive increase in photoluminescence</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"ncommsdexterphotoluminescence-1462897663052.jpg\" src=\"http://spectrum.ieee.org/img/ncommsdexterphotoluminescence-1462897663052.jpg\"/>\n<figcaption class=\"hi-cap\">Image: Nature Communications</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Researchers at the National University of Singapore (NUS) have <a shape=\"rect\" href=\"http://news.nus.edu.sg/press-releases/10360-improve-photoluminescence-efficiency-semiconductors#\">developed a way to give a massive boost to the photoluminescent efficiency of tungsten diselenide</a>.\u00a0In so doing, they may have paved the way for this two-dimensional semiconductor\u2014which belongs to a class of 2-D crystals known as\u00a0<a shape=\"rect\" href=\"http://spectrum.ieee.org/searchContent?q=transition+metal+dichalcogenides+&amp;type=&amp;sortby=relevance\">transition metal dichalcogenides</a>\u2014to have a greater impact on optoelectronics and photonics, including applications such as\u00a0photovoltaics, quantum dots, and LEDs.</p>\n<p/>\n<p/>\n<a shape=\"rect\" href=\"http://spectrum.ieee.org/searchContent?q=tungsten+diselenide&amp;type=&amp;sortby=relevance\"/>\n<p/>\n<p>While all of these applications take advantage of <a shape=\"rect\" href=\"http://spectrum.ieee.org/searchContent?q=tungsten+diselenide&amp;type=&amp;sortby=relevance\">tungsten diselenide</a>\u2019s ability to convert light to electricity and vice versa, the material\u2019s\u00a0thinness is a limiting factor in its ability to\u00a0absorb photons and its photoluminescence.</p>\n<p/>\n<p>In research described in the journal <a shape=\"rect\" href=\"http://www.nature.com/ncomms/2016/160506/ncomms11283/full/ncomms11283.html\">\n<em>Nature Communications</em>\n</a>, the NUS researchers turned to <a shape=\"rect\" href=\"http://spectrum.ieee.org/searchContent?q=plasmonic+nanostructures&amp;type=&amp;sortby=relevance\">plasmonic nanostructures</a>, which exploit oscillations in the density of electrons that are generated when photons hit a metal surface, to improve the brightness of tungsten diselenide\u2019s photoluminescence by\u00a020,000 times.</p>\n<p/>\n<p>\u201cThis is the first work to demonstrate the use of gold plasmonic nanostructures to improve the photoluminescence of tungsten diselenide,\u201d said Wang Zhuo, one of the NUS researchers and first author of the paper, in a press release. \u00a0\u201cWe have managed to achieve an unprecedented enhancement of the light absorption and emission efficiency of this nanomaterial.\u201d</p>\n<p/>\n<p>Prior attempts at improving the photoluminescence of\u00a0tungsten diselenid\u2014<span>which involved simultaneously improving its absorption, emission and directionality\u2014</span>managed to make it shine only\u00a01,000 times as bright\u00a0as it does in\u00a0its natural state.</p>\n<p/>\n<p>The NUS researchers achieved the\u00a020,000-fold brightness increase by suspending flakes of tungsten diselenide over sub-20-nanometer-wide trenches in a gold substrate. The researchers say\u00a0this design yields such a huge increase due to enhanced absorption of the photons emitted by the\u00a0pump laser. The absorption boost, they concluded, is\u00a0because of plasmons that are confined in the trenches.</p>\n<p/>\n<p>\u201cThe key to this work is the design of the gold plasmonic nanoarray templates,\u201d said Andrew Wee, a professor at NUS, in the press release. \u201cIn our system, the resonances can be tuned to be matched with the pump laser wavelength by varying the pitch of the structures. This is critical for plasmon coupling with light to achieve optimal field confinement.\u201d</p>\n<p>In continuing research, the NUS team will examine how effective the gold plasmons are in enhancing electroluminescence of transition metal dichalcogenides. They intend to extend this investigation into a range of 2-D transition metal dichalcogenides with different band gaps. The researchers expect that each one will employ different interaction mechanisms.</p>\n<p/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/-kk7LFEZVww\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Tue, 10 May 2016 19:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/nanoclast/semiconductors/materials/2d-semiconductor-glows-20000-times-more-brightly</guid>\r\n      <dc:creator>Dexter Johnson</dc:creator>\r\n      <dc:date>2016-05-10T19:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MTU1NQ\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MTU1NQ\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/nanoclast/semiconductors/materials/2d-semiconductor-glows-20000-times-more-brightly</feedburner:origLink></item>\r\n    <item>\r\n      <title>Why You Should Be Glad That Quadrotors Have Learned to Dodge Swords</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/yVu_mqZy274/quadrotors-have-learned-to-dodge-swords</link>\r\n      <description>A Stanford roboticist (and fencer) discusses drones, swords, and why mixing them is such a great idea</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"quad_fence1-1462898028908.jpg\" src=\"http://spectrum.ieee.org/img/quad_fence1-1462898028908.jpg\"/>\n<figcaption class=\"hi-cap\">Image: Stanford ASL</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>When that quadrotor fencing video showed up everywhere last month, we asked Ross Allen, the Stanford PhD candidate (and fencer) responsible for the research, if he\u2019d be willing to talk to us about it. He said sure, except his thesis defense was that Friday, so would we mind waiting a bit? It\u2019s been a bit, and after a successful defense, Dr. Allen is somehow not sick and tired of robots and answered a bunch of our questions about quadrotors, swords, and why mixing them is such a great idea.</p>\n<p>Here\u2019s the video that you (and a couple hundred thousand other people) probably saw a few weeks ago:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/kdlhfMiWVV0\"/>\n</p>\n<p>The swordplay is cool, but even cooler is the fact that this is the first demonstration of truly \u201c<a shape=\"rect\" href=\"http://asl.stanford.edu/projects/real-time-kinodynamic-planning/\">real-time kinodynamic planning</a>\u201d on a quadrotor system navigating an obstructed environment. Or at least, that\u2019s what <a shape=\"rect\" href=\"http://asl.stanford.edu/wp-content/papercite-data/pdf/Allen.Pavone.AIAAGNC16.pdf\">this recent paper</a> from Allen (along with his colleague Marco Pavone) at Stanford\u2019s Autonomous Systems Laboratory\u00a0says. We asked him to explain this to us in a way that we might have a shot at understanding, and he did such a fantastic job that we\u2019re just going to quote him:</p>\n<blockquote>\n<p>\n<em>Think of a maze, like a pen-and-paper maze that you would solve as a kid. Solving this type of maze is solving a very basic path planning problem: navigating a point (the tip of your pen) through obstacles (the maze \"walls\") to an objective (the exit of the maze). You could make this problem more complicated by imagining a 3D maze. An example of such a problem would be moving a piano through a cluttered apartment without bumping it into other furniture or slamming it into walls. You could imagine coming up with a continuous path for the piano, involving translations and rotations, that could navigate the piano through the apartment.</em>\n</p>\n<p>\n<em>Now let\u2019s make the problem a bit harder. Now imagine your planning problem is to drive a car through an urban environment with buildings, cars, and pedestrians and reach an objective in as short of time as possible. This isn\u2019t really the same as moving a piano because we can\u2019t arbitrarily translate and rotate my car. The car has to follow its steering path, I can\u2019t \"slide\" my car sideways. If I want to stop my car, I have to hit the brakes and it takes time to slow down. If I take a corner too fast, I\u2019ll skid out and lose control. You can see this is still a planning problem\u2014 \u00a0navigating some object (or robot) to a goal while avoiding obstacles\u2014 \u00a0but now there are these additional constraints on HOW my agent (or robots) can move. We term these constraints \"kinodynamic\" constraints (some are constraints on kinematics, such as the steering, and some are constraints based on the dynamics, such as the braking to slow down). When solving these types of problems, we are solving \"kinodynamic planning problems.\"</em>\n</p>\n<p>\n<em>What our research has done is to develop a framework for solving such kinodynamic problems that drastically reduces the computations that have to be executed during operation of the robot. We\u2019ve demonstrated this framework on a quadrotor system navigating an indoor environment with dynamic, even adversarial obstacles. This is arguably the first demonstration of real-time planning for a quadrotor system which represents a kinodynamic system.</em>\n</p>\n<p>\n<em>The video and GIF that got so much attention really doesn\u2019t display the full capabilities of our research; they just give a flashy, easy-to-understand demonstration. The video below shows some more of the full capabilities when the quadrotor tries to navigate around a wall but I continue to block it\u2019s path. It keeps recomputing paths until the optimal path shifts to the other side of the wall and it navigates away from me:</em>\n</p>\n</blockquote>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/fv_6KB2eEFc\"/>\n</p>\n<p>The example here shows \u201ckinodynamic planning\u201d applied to a quadrotor, but the research is generalized and can be applied to any system you want. Allen suggests that shipping and distribution infrastructure are just a few of the applications that could benefit:</p>\n<blockquote>\n<p>\n<em>You could imagine an autonomous cargo ship navigates through a busy port to an autonomous crane that safely unloads containers onto awaiting autonomous trucks for delivery. Each step in this infrastructure is faced with it\u2019s own unique planning problem (the ship avoiding other marine craft, the crane moving large suspended loads around other containers, trucks driving on busy roadways) yet each planning problem can be approached with the same framework, just slightly tailored to the specific application (as it is for the quadrotor system).</em>\n</p>\n</blockquote>\n<aside class=\"inlay pullquote rt med-lrg\">\u201cFor one, this is not a precursor to a Terminator robot. . . . We\u2019re not using machine learning to train the quadrotor how to fight or anything like that. Our robot won\u2019t get any smarter without us programming it with better algorithms. In other words, it\u2019s autonomous but it is not learning how to be more autonomous.\u201d<span class=\"pq-attrib\">\u2014Ross Allen, Stanford ASL</span>\n</aside>\n<p>And of course, you can add delivery drones and camera drones to the list of applications that you can imagine, too: \u201cI\u2019m convinced that delivery drones are in the imminent future and that they will have an immense impact on consumer purchasing,\u201d Allen tells us, with the qualifier that \u201cthe planning and control of such delivery drones are only a portion of the hurdles that must be overcome; sensing and policy being the other hurdles.\u201d Those are some pretty Big Hurdles, I think, but it\u2019s refreshing to hear an optimistic perspective on consumer delivery drones from someone involved in such cutting edge research.</p>\n<p>From here, Allen says he\u2019ll be pursuing opportunities in \u201cautonomous cars, drones, and rockets, along with research positions.\u201d Our vote would be for rockets, because rockets. There\u2019s still plenty more interesting stuff to be done with this research, though, like throwing a bunch of quadrotors into the mix all at once and looking for emergent cooperative behaviors, and trying to get everything running onboard the drone itself, rather than relying on motion capture systems.</p>\n<p>Finally, we asked Allen if there was anything he wanted to clear up after approximately one bajillion people on the Internet watched him fence with a quadrotor without any context whatsoever:</p>\n<blockquote>\n<p>\n<em>For one, this is not a precursor to a Terminator robot. While we use machine learning in our research, it is for the rather mundane sounding task of \u201creachability analysis.\u201d We\u2019re not using machine learning to train the quadrotor how to fight or anything like that. Our robot won\u2019t get any smarter without us programming it with better algorithms. In other words, it\u2019s autonomous but it is not learning how to be more autonomous.</em>\n</p>\n</blockquote>\n<p>Always good to hear that the very latest in robotic research has not resulted in the Terminator.</p>\n<p>Yet.</p>\n<p>[ <a shape=\"rect\" href=\"http://asl.stanford.edu/wp-content/papercite-data/pdf/Allen.Pavone.AIAAGNC16.pdf\">Stanford ASL</a> ]</p>\n<p>\n<em>Many thanks to Dr. Ross Allen for speaking with us.</em>\n</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/yVu_mqZy274\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Tue, 10 May 2016 18:35:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/automaton/robotics/drones/quadrotors-have-learned-to-dodge-swords</guid>\r\n      <dc:creator>Evan Ackerman</dc:creator>\r\n      <dc:date>2016-05-10T18:35:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MTU5MQ\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MTU5MQ\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/automaton/robotics/drones/quadrotors-have-learned-to-dodge-swords</feedburner:origLink></item>\r\n    <item>\r\n      <title>How Should a Self-Driving Car Tell You to Take the Wheel?</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/4m3KXeUcUFs/how-should-a-selfdriving-car-tell-you-it-needs-you-to-take-over</link>\r\n      <description>Experts say self-driving cars should start with vibrations in your seat</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"WilliamHowardGettyImagesRM157370227-1462879930811.jpg\" src=\"http://spectrum.ieee.org/img/WilliamHowardGettyImagesRM157370227-1462879930811.jpg\"/>\n<figcaption class=\"hi-cap\">Photo: William Howard/Getty Images</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Someday, the word <em>driver </em>will connote something completely different than it does today. When cars are <a shape=\"rect\" href=\"http://spectrum.ieee.org/static/the-self-driving-car\">fully automated</a> and don\u2019t need us for anything more than letting them know\u00a0where to go, the job of a \u201cdriver\u201d will be like that of a patron in a bar selecting a song on a jukebox.\u00a0But that scenario, where the passenger cabin is a rolling lounge in which all occupants are free to talk, entertain themselves with electronic gadgets, or sleep, is still decades away.</p>\n<p>Until then, even the most sophisticated <a shape=\"rect\" href=\"http://spectrum.ieee.org/tag/self-driving+cars\">self-driving car</a> will occasionally encounter <a shape=\"rect\" href=\"http://www.vox.com/2016/4/21/11447838/self-driving-cars-challenges-obstacles\">a circumstance that overwhelms</a> its computerized smarts. At those moments, it will look to a human to take charge. But how will it grab my attention from my phone call or my Netflix movie\u2014or rouse me from sleep\u2014and get me dialed into the potential danger in time to avoid a crash?</p>\n<p>Because it\u2019s possible to miss an audible or visual alert if you\u2019re yakking away on a cellphone or engrossed in a car chase scene in a James Bond movie, researchers are looking to add another modality to take-over requests: vibrations in the car seat or seatbelt. A team of researchers at <a shape=\"rect\" href=\"https://www.tum.de/en/homepage/\">Technische Universit\u00e4t M\u00fcnchen</a>, in Munich, Germany, and the <a shape=\"rect\" href=\"http://www.tudelft.nl/en/\">Delft University of Technology</a>, in the Netherlands, reviewed the spectrum\u00a0of such alerts, known collectively as vibrotactile displays in the April\u00a0<span>2016</span>\n<span>edition of the\u00a0</span>\n<a shape=\"rect\" href=\"http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=7328316&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel7%2F6979%2F4358928%2F07328316.pdf%3Farnumber%3D7328316\">\n<em>IEEE Transactions on Intelligent Transportation Systems</em>\n</a>.</p>\n<p>How effective is, say, a rumbling seat?\u00a0\u201cVibrotactile displays have been shown to be effective as warnings and hard to ignore,\u201d says Sebastiaan M. Petermeijer, a PhD candidate in the ergonomics department at Technische Universit\u00e4t\u00a0M\u00fcnchen\u00a0and the review\u2019s lead author. \u201cIn my opinion [they are] a potentially valuable addition to visual and auditory displays in cars, especially when drivers will be engaged in other tasks during highly automated driving,\u201d he says.</p>\n<p>The transition from checking email to helming the vehicle doesn\u2019t happen instantly. On average, it takes about 0.8 seconds for a driver to shift attention so that his eyes are on the road. But even after having receiving a tactile warning and turning his or her focus toward the situation, it still takes a moment before the driver has assessed the situation enough to make a helpful response. But tactile warnings, researchers have found, resulted in faster brake reaction times than did, say, flashing lights, beeps, or a computerized voice. Vibrations also spurred human drivers to take the wheel and begin steering much more quickly than did visual or auditory cues.</p>\n<p>What\u2019s more, say\u00a0Petermeijer and colleagues in Germany and The Netherlands, they also found literature indicating that it\u2019s possible to shorten the transition time by employing tactile warnings to direct the driver\u2019s attention towards the danger. This, the researchers concluded, allowed the driver to make a visual assessment of the situation more quickly. In other words, the driver gets up to speed with, well, more speed.</p>\n<p>How can the car tell you where to look? Previous research showed that, \u201ca person wearing a tactile waist belt can sense directional feedback in increments as fine as 10 degrees in the horizontal plane.\u201d They note that it is possible to map stimulus zones on the driver\u2019s body to specific locations on the road. So, for example, a series of vibrations on the seat under the driver\u2019s right hip might suggest the danger is coming from behind on the passenger side of the vehicle, while a similar buzzing under the left knee could be directing attention out of the front windshield toward a broken tree branch hanging precariously over the left side of the driving lane.</p>\n<p>But just because vibrations can be used this way, does that mean they should? Though the researchers recommend that takeover requests be conveyed by auditory, visual, and vibrotactile displays, they say that each\u00a0should have its own role in making the driver situationally aware.\u00a0 \u201cIf the situation is truly urgent, you can't risk to rely on a single modality warning,\u201d says Joost de Winter, a mechanical engineering professor at Delft University of Technology and one of the paper\u2019s co-authors.</p>\n<p>Asked whether it\u2019s possible that multiple alarms, engaging several senses at once could be confusing, de Winter said:</p>\n<blockquote>\n<p>\u201cThat is certainly a valid point. The issue of 'alarm floods' in process control rooms is well known; there is also some literature on overwhelming/startling alarms (in early aircraft, for example). These issues also apply to non-automated driving where increase of technology in cars may be both an opportunity and pose a risk because of mental overload.\u201d</p>\n</blockquote>\n<p>He notes that, although cars are becoming ever more complex, vibrotactile displays can be used in a way that\u2019s unlikely to add to the complexity. \u201cI would argue that vibrations can be intuitive as a warning,\u201d de Winter says, pointing to the highly effective buzzing of mobile phones when they\u2019re in silent mode.</p>\n<p>To prevent cognitive overload, they suggest that the different warning modalities (tactile, auditory, and visual) be used at different stages of the take-over process: shift of attention, reposition (as in your hands moving to the steering wheel and your foot moving toward the brake pedal), cognitive processing, action selection, and action implementation. \u00a0They note that vibrotactile feedback, \u201ccould be used to shift the attention, after which a visual head-up display could show the danger areas that should be avoided by the driver. This way, one effectively exploits the benefits of the vibrotactile warnings (e.g., them being hard to ignore) and of visual feedback (e.g., them being suitable for conveying semantics).\u201d</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/4m3KXeUcUFs\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Tue, 10 May 2016 16:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/cars-that-think/transportation/self-driving/how-should-a-selfdriving-car-tell-you-it-needs-you-to-take-over</guid>\r\n      <dc:creator>Willie D. Jones</dc:creator>\r\n      <dc:date>2016-05-10T16:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MTQzNw\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MTQzNw\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/cars-that-think/transportation/self-driving/how-should-a-selfdriving-car-tell-you-it-needs-you-to-take-over</feedburner:origLink></item>\r\n    <item>\r\n      <title>Effortless Biking With the GeoOrbital E-Wheel</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/N1B6Q3jmwaU/effortless-biking-with-the-geoorbital-ewheel</link>\r\n      <description>The startup\u2019s snap-in wheel assembly contains all you need for electric propulsion</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<p>A bicycle retrofitted with a GeoOrbital, electric-powered front wheel demands a light thumb: Touch the throttle lever on the handlebar a little too hard, and you lurch ahead.</p>\n<p>\u201cYou catch on in half an hour,\u201d says Michael Burtov, the founder of GeoOrbital. I rode around for just 10 minutes, so I never quite caught on. But it sure beats pedaling.</p>\n<p>It took\u00a0Bartov less than a minute to fit his wheel to the bike and a few seconds more to slip the 3.4-kilogram (7.5 lb.) battery pack into its holder, which hangs where the spokes would be, if the wheel had spokes. Instead, its rim orbits around a circular frame, under the motive power of a 500-watt motor. That power is delivered through two rubber rollers, which grip the inward part of the rim.</p>\n<p>The wheel is covered in hard rubber, which means you\u2019ll be missing the cushioning effect of an inner tube. The wheel also adds some 9 kilograms\u00a0to the bike, and though most of the mass is slung close to the ground, it does seem to make turning a bit harder. GeoOrbital has launched a <a shape=\"rect\" href=\"https://www.kickstarter.com/projects/1266381423/geoorbital-wheel-make-your-bike-electric-in-60-sec\">KickStarter campaign</a> to raise money for mass production.\u00a0</p>\n<p>Read More: <a shape=\"rect\" href=\"http://spectrum.ieee.org/cars-that-think/transportation/alternative-transportation/riding-geoorbitals-allinthewheel-ebike\">The GeoOrbital Wheel Lets You Make Your Bike Electric in Minutes</a>\n</p>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/N1B6Q3jmwaU\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Tue, 10 May 2016 15:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/video/transportation/alternative-transportation/effortless-biking-with-the-geoorbital-ewheel</guid>\r\n      <dc:creator>Philip E. Ross</dc:creator>\r\n      <dc:date>2016-05-10T15:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MTMyMA\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MTMyMA\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/video/transportation/alternative-transportation/effortless-biking-with-the-geoorbital-ewheel</feedburner:origLink></item>\r\n    <item>\r\n      <title>Review: Neato BotVac Connected</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/KpoLaufaF7I/review-neato-botvac-connected</link>\r\n      <description>An in-depth look at Neato's newest and fanciest Wi-Fi connected, LIDAR-equipped robotic vacuum</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"neato_hero1-1462817999246.jpg\" src=\"http://spectrum.ieee.org/img/neato_hero1-1462817999246.jpg\"/>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Six years ago, I drove from my crummy little apartment in the part of Berkeley that\u2019s too close to Oakland to somewhere in the south bay that I don\u2019t really remember to pick up, in person, what I\u2019m pretty sure was a development prototype of the Neato XV-11 robotic vacuum. I was instructed to return it in 24 hours, or they\u2019d send a robotic hit squad after me. I wrote a <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/home-robots/review-neato-robotics-xv11\">blazing fast review of the XV-11</a>, <a shape=\"rect\" href=\"https://www.youtube.com/watch?v=mg9ZaXHZQBI\">taped a butter knife to it and let it duel</a> my <a shape=\"rect\" href=\"http://www.ackdigital.com/botjunkie/pages/iRobot_Roomba_560_vs_Neato_XV11.html\">iRobot Roomba 560</a>, and then brought it back to Neato, having inflicted a bare minimum of physical (and emotional) scarring.\u00a0</p>\n<p>Since then, Neato Robotics has established itself as a solid and capable competitor to <a shape=\"rect\" href=\"http://spectrum.ieee.org/tag/roomba\">iRobot\u2019s Roombas</a> in the autonomous vacuum space. The XV-11 series has been incrementally upgraded, with a much more significant redesign in 2014 in the form of the BotVac series. Late last year, Neato announced the BotVac Connected, which adds WiFi connectivity and an app that lets you control your robot from anywhere in the world. This is Neato\u2019s top of the line model and currently sells for US $700. We took a look at it at CES, and then Neato promised to send us one to check out at home.</p>\n<p>Neato\u2019s robots, starting with the XV-11 and continuing with the BotVac Connected, are notable because of their ability to rapidly generate accurate maps of the spaces that they\u2019re in, and then localize and navigate to efficiently clean those spaces in nice straight lines. For a long time, this was a capability that was almost entirely unique to Neato, but over the last few years, other robot vacuum manufacturers have added mapmaking to their robots, most recently <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/home-robots/review-irobot-roomba-980\">iRobot with its Roomba 980</a>, which uses a camera for helping with localization and mapping. So the question is, now that Neato isn\u2019t the only robot vacuum on the block with this technology, how does the latest version look? Does its six-year-old navigation system still hold up, and can it survive in an increasingly competitive market without the feature that made it unequivocally unique?</p>\n<p>We\u2019ve got a full review for you after the break, complete with some long-exposure cleaning pics and an interview with the Neato robotics team.</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIxNw\"/>\n</div>\n<h2>Hardware</h2>\n<p>You\u2019ll notice two things about the BotVac Connected right away. The first is that it\u2019s not round or square, but \u201cD\u201d-shaped, and the back of the robot is the rounded bit, which takes some getting used to. Neato says that the square front helps the robot fit itself into corners and be generally more effective throughout the square-ish nature of your home, which is generally true, although the sweeping brush is set back far enough from the front of the robot that even if the bot is nose-to-nose with a wall, the effective distance is still a few inches. More on the brush later.</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIxOA\"/>\n</div>\n<p>The other thing you\u2019ll notice is that it\u2019s got a bump at the back. This houses the laser turret, which is literally a laser turret, housing a spinning infrared laser that the robot does its navigation with. More on that later, too. Otherwise, the top of the robot is spartan but functional, with a color screen and four button interface, plus two other dedicated buttons: one for a spot clean, and one for a full house clean. Most of the top of the robot lifts off to allow access to the dustbin, which is one of my favorite features. Overall, it\u2019s a very polished, slick-looking little robot.</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIxOQ\"/>\n</div>\n<p>Flip it over, and you\u2019ll see some wheels and casters, the main brush, and a side brush. There are two drop sensors right behind the bumper at the front of the robot, but none at the sides or back. The side brush is pretty small, and looks like it exists primarily to sweep dirt from close to a wall out in front of the robot. In a thoughtful little touch, the side brush held onto its axle with a magnet, so it pops off and on again quite easily if you need to clean or replace it.</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIyMA\"/>\n</div>\n<p>It\u2019s not immediately obvious how to remove the main brush for cleaning, but Neato\u2019s website has some handy how-to videos to help you out. I didn\u2019t watch those videos because I prefer haphazard flailing, but I discovered that a.) you stick your fingers in behind the wheels and pop the whole bottom plate off, and b.) if you do this without removing the side brush first, it will fly off into your face. Once the bottom plate is removed, the brush lifts right out for you to clean, check the bearings (they require some extra force to remove), and all that stuff to keep your Neato in top shape.\u00a0</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIyMQ\"/>\n</div>\n<p>On the back of the robot (the round part that looks like it should be the front) are two contacts that line up with the charging dock. The Neato docks itself in reverse (using an adorable little butt-wiggle motion) and can safely spend most of its time on the dock, keeping itself topped up and at your beck and call.</p>\n<h2>Sensing and Navigation</h2>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTM4NA\"/>\n</div>\n<p>The thing that makes the Neato cooler than any other robot vacuum out there is the fact that it\u2019s got an actual laser navigation system inside of it. That turret on the top of the robot houses a spinning laser cannon and detection system called the Revo LDS (for revolving laser distance sensor). It fires up to 4,000 infrared laser pulses per second while rotating at up to 10 Hz, watches for the reflection of each pulse, and then does some math to figure out the distance and angle of whatever the pulse reflected off of. A range of 5-ish meters with centimeter accuracy is plenty for building up a map of the rooms in your home to navigate with.</p>\n<p>There are all kinds of reasons why laser distance sensors are the sensors of choice for robotic map-making applications. They\u2019re fast and accurate, and have proven to be very robust, which is why you see the same fundamental technology spinning away on the roofs of just about every single autonomous car. Those things (usually beefy sensors from Velodyne) cost a ton, but Neato managed to get the price of the Revo LDS down to under $30 by developing it from scratch, and for indoor applications, it\u2019s easily as effective as it needs to be. You can read all of the technical details on Neato\u2019s laser sensor (the first gen, anyway) in <a shape=\"rect\" href=\"http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4543666&amp;isnumber=4543169\">this paper from ICRA 2008</a> by Kurt Konolige, Joseph Augenbraun, Nick Donaldson, Charles Fiebig, and Pankaj Shah.</p>\n<p>In the context of a robotic vacuum, Neato\u2019s navigation system is, in my opinion, still the best out there. Other vacuums that do vision-based simultaneous localization and mapping (SLAM) can slowly build up maps of their environments with relative positioning and then navigate around them, but with the Neato, you turn the thing on and <em>BAM</em>, in just a few seconds the laser has located walls, furniture, your legs, its dock, and anything else close to floor level. As it moves, that map is continuously being refined and updated to make sure that the robot never misses a spot or gets lost. Just about every single mobile base that you see doing anything in a commercial or industrial environment has a LDS on it, and it\u2019s awesome that you can buy a robot vacuum with that technology built in, too.</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIyMg\"/>\n</div>\n<p>As impressive at the Neato\u2019s LDS is, it does have plenty of other sensors that it uses to refine its navigation and obstacle avoidance. Having a big front bumper is especially important since the LDS operates in one horizontal plane, meaning that it can\u2019t detect obstacles shorter than it is, or any hovering furniture you may own. It also has a wall-following sensor, along with two infrared drop sensors at the front, right behind the bumper.</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIyMw\"/>\n</div>\n<p>The biggest problem that I have with robot vacuums in general is with these drop sensors, used to keep the robots from hurling themselves down stairs and off of balconies. They (generally) work by shining infrared lights onto your floor and then watching for the reflection: if they can\u2019t see the reflected light, it means that the light is shining down into nothingness, and they\u2019re probably about to drive off of a ledge. However, these sensors also don\u2019t see reflected light if the light is absorbed by the surface that it\u2019s shining on, and robots can have problems distinguishing black carpets made of certain materials from a void.</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIyNA\"/>\n</div>\n<p>As a frequent reviewer of robot vacuums, I made it my business to acquire such a carpet (and my girlfriend liked the pattern, so win-win). Roombas will not function on this carpet at all. If you start them from off the carpet, they won\u2019t vacuum the black areas, and if you start them on one of the black areas directly, they will completely freak out and refuse to move while making panicked beeping noises. <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/home-robots/review-irobot-roomba-980\">It\u2019s possible to solve this problem by taping tinfoil over the Roomba\u2019s cliff sensors, fooling them into thinking that they can always see the floor.</a> Doing so renders them inoperable, and consequently, the Roomba will make a beeline for the nearest stairway and attempt to throw itself down it. This happened to me first with my 880 (the virtual wall I was using to protect the stairway ran out of batteries), and then more recently with <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/home-robots/review-irobot-roomba-980\">my new 980</a> (I forgot to turn the virtual wall on). Both Roombas bounced down the entire flight of stairs and landed on concrete and were somehow fine, but this is not something that you want to happen frequently. And to be frank, it shouldn\u2019t happen: I shouldn\u2019t have to modify my robot in a way that is dangerous to it in order for it to work on a color of carpet that is not uncommon for people to have in their homes. If the Neato BotVac Connected can handle my black carpet, that\u2019ll be a major victory for it over iRobot\u2019s Roombas. But before we get into that, let\u2019s look at the robot\u2019s control interface.</p>\n<h2>Interface and Connectivity</h2>\n<p>The big new feature with the BotVac Connected is the Connected part, since the BotVac part has been out for more than a year. Connected means that the robot has WiFi it in and can talk to your home network. Through that, it can talk to Neato\u2019s servers and then to an app on your phone, allowing you to monitor the status of the robot, get alerts, and do a limited amount of remote scheduling and control.</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIyNQ\"/>\n</div>\n<p>Getting the app to work was not a particularly smooth process. Initially, I couldn\u2019t get it to work at all on my Android phone (a not very old Nexus 5x): the app would connect to the robot and then spend 5 minutes failing to connect to my home network, which resulted in a frustratingly useless \u201csomething has gone wrong\u201d error message before giving up completely and then offering a manual connection mode that didn\u2019t work either. At that point, I just ignored the app and used the controls on the robot itself, which mostly worked fine, and forgot about the whole \"connected\" part of the Neato Botvac.</p>\n<p>A few weeks later, after the app was updated on the Google Play store, I gave it another try, and everything worked fine. It\u2019s possible that this was just a temporary Android issue, and that it works flawlessly on iOS, but the mixed reviews on Google Play suggest that at least on Android, there have been some systematic problems. Hopefully they\u2019ve been fixed now, but it kind of seems like Neato may have let the Android app out of the box before it was completely ready to go.</p>\n<p>If for whatever reason you can\u2019t get the app to work, or don\u2019t want to use it, it doesn\u2019t actually matter all that much. You can use the app tell your robot to do a full cleaning cycle or a spot cleaning cycle. You can switch from Turbo mode to Eco mode. You can set the cleaning schedule. You can check the battery level. That\u2019s just about about it. Yeah, it\u2019s kind of cool that you can tell your robot to start cleaning the house from anywhere in the world, but since it\u2019s got a scheduling function anyway, I\u2019m not sure how much utility it actually offers. Really, the only fundamentally exciting thing that the app does that you can\u2019t do through the robot\u2019s own control panel is a \u201cManual clean\u201d option where you can drive the robot around yourself. It\u2019s a neat-o little feature (I\u2019ll only do that once, I promise), but hardly worth getting excited about. At this point, the app seems like more of a proof-of-concept for having a connected robot vacuum than something designed to add a significant amount of value for most users, but we\u2019re expecting a lot more with future updates.</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIyNg\"/>\n</div>\n<p>Generally, I got the robot to do my bidding more directly, through the little color screen on it and the two big buttons on the front. Weirdly, the screen offers all kinds of options that the app doesn\u2019t, including options for lights and sounds, scheduling, usage info, and even log file downloads and software updates. The most option-y option is deciding whether you want to use Turbo mode or Eco mode; Turbo spins the brush faster and cranks the vacuum up, while Eco is quieter and more energy efficient but in some situations may not clean as well.</p>\n<h2>Cleaning Behavior</h2>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIyNw\"/>\n</div>\n<p>I stuck my Neato BotVac Connected in a corner of my living room, hemmed in by the end of my couch, the end of a bookcase, and a lamp. It\u2019s just the right size for a robot like this, but I never housed any of my Roombas there for fear that they\u2019d have trouble getting out and would never be able to find their way home again. The Neato\u2019s LIDAR gave me confidence that the robot would be able to detect the way out, so I figured I\u2019d give it a try.</p>\n<p>When you start the BotVac from its dock, it plays a happy little song, drives forward a bit, and then sits for a second as the vacuum powers up and the LIDAR gets up to speed. Generally, it follows a counterclockwise edge-to-center pattern, at least at the beginning, but this can vary immensely (and immediately) depending on what obstacles (furniture) the robot encounters and where.\u00a0</p>\n<p>Here\u2019s a long-exposure picture that I took of the Neato BotVac Connected covering my living room over the course of about 20 minutes. I put some red LEDs on it to leave a red light trail, but the robot\u2019s screen was on the whole time as well, leaving a parallel blue light trail at the same time:</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTA3Mg\"/>\n</div>\n<p>The dock is in the upper left corner. The BotVac left that corner and immediately went under the couch, probably in an effort to favor its left side (with the side brush and wall sensor). It finished under the couch, and then spent a big chunk of time carefully going around each leg of the coffee table before covering the rest of the room in more or less straight lines.</p>\n<p>You\u2019d expect that since the BotVac is using its LIDAR to map the room and plot the most efficient route, running it in the same room would result in a similar pattern, and that\u2019s exactly what you see. Note that for this second run, I manually stopped the robot before it finished cleaning and returned to its dock, which is why you don\u2019t see the return path in the image:</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTA3Mw\"/>\n</div>\n<p>And here\u2019s the navigation path of a <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/home-robots/review-irobot-roomba-980\">Roomba 980</a> in the same space to compare:</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTA3NA\"/>\n</div>\n<p>And one more, of a Roomba 880, which navigates by zigzagging randomly across a room:</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTA3NQ\"/>\n</div>\n<p>All of these pictures were between 20 and 22 minute exposures; the BotVac is perhaps best described as \u201cmethodical.\u201d The Roomba 980 covered the same area over multiple passes in almost exactly the same amount of time (within a minute or two). I\u2019d characterize it as \u201cbrisk.\u201d</p>\n<p>You\u2019ll also notice that the transitions onto the black carpet patches don\u2019t even cause a wiggle: I\u2019m delighted to be able to report that the Neato BotVac was not the least bit intimidated by the black areas on my rug that no Roomba has been able to handle without modification. If you have a carpet in your house that\u2019s a deep dark color and a little shaggy and don\u2019t want to have to mess with your robot vacuum to get it to behave itself, Neato is hands-down the way to go.\u00a0</p>\n<p>The Neato seems to be a bit louder than the Roomba, at least when the Neato is in Turbo mode. This is partly because of the vacuum and the laser, but also because of how the robot is designed underneath. On hardwood, it tends to make <em>thunk thunk thunk</em> noises with its casters, and the plastic scraper makes, well, scraping noises that aren\u2019t particularly pleasant. I might consider hanging out in the living room while the Roomba was running, but not while the BotVac is, if it\u2019s on hardwood. On carpet, it\u2019s not so bad, and in Eco mode, it\u2019s much better, although I think the Roomba still wins because of its ability to toggle its own (louder) carpet boost mode on and off when necessary. I do, however, much prefer the lower-pitched hum of the Neato\u2019s vacuum. It\u2019s actually kind of soothing, especially when the bot is moving slowly.</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIyOA\"/>\n</div>\n<p>During normal cleaning, the Neato tends to be a bit aggressive at times, and it\u2019s not always clear to me why. In some situations, it\u2019ll pass right by the leg of a chair with just a few millimeters of clearance, whereas it sometimes instead seems to deliberately use its bumper to feel it was around corners or along walls, with repeated gentle nudges to make sure that it gets as close as possible. I haven\u2019t noticed any damage to the robot or my furniture, but I can\u2019t help but feel like the poor thing must be giving itself a headache.</p>\n<p>After the Neato decides that it\u2019s done, it immediately shuts its vacuum off and drives back to its dock. I was expecting it to make a beeline every time, but often the robot would go back towards the center of the room and then rotate back and forth a bit as of trying to decide where to go. It never failed to find its dock, though I\u2019m not sure what the state of its internal map is such that it can\u2019t plot a direct path every time. When the robot does make it back, it\u2019ll notify you that it\u2019s done through the app, which is nice.</p>\n<h2>Cleaning Performance</h2>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIyOQ\"/>\n</div>\n<p>As we\u2019ve come to expect from Neato, the BotVac Connected cleans like a champ. Like most robot vacuums, it\u2019s especially good on hard surfaces, but it also manages to give carpets that \u201cI just vacuumed\u201d look. It won\u2019t replace your upright vacuum (no robot vacuums can realistically do this), but it will keep your floors much cleaner, while you yourself vacuum much less often. The long exposure pictures above give a good sense of how the robot performs in an average living room: coverage is complete, with special attention paid to walls, edges of furniture, and furniture legs.</p>\n<p>Over a few solid months of testing, I had the BotVac running once or twice a week. Running it while I was at home was only a minimal hassle, since I could be doing something else while the robot got to work. Using the scheduler was even better, since I could just come home and the robot would be on its dock charging (right where I\u2019d left it) and the floor would be obviously cleaner, like magic. A single charge should be sufficient for the BotVac to cover your floor, even in Turbo mode, but if you have lots of rooms you want it to do, it\u2019s smart enough to return home for a charge if necessary and then continue where it left off.</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTIzMA\"/>\n</div>\n<p>\n<a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/home-robots/review-irobot-roomba-980\">Using my Roombas as a baseline comparison (the 980 and the 880)</a>, I can unscientifically say that it\u2019s unlikely that you\u2019ll notice much of a difference between them in normal day to day cleaning. Both the Neato and the Roombas somehow manage to fill their dustbins on almost every cleaning pass, which for the life of me I don\u2019t understand, because that\u2019s a lot of dirt that somehow continually makes it into my house. The Neato is particularly good at dealing with hair (pet and otherwise), and seems to do a better job with carpets, especially carpets with deep pile. And dark carpets, of course.</p>\n<p>No robot vacuum is flawless, and the BotVac is no exception. For whatever reason it will occasionally miss spots, and it had intermittent trouble in my kitchen, where it would leave some dirt close to the base of the fridge and stove, possibly because the side brush was too short to properly reach. The Neato will also have trouble if you have crap strewn all over your floors, which bothers some people, but I view it as a feature: for the robot to keep your floors clean, you have to first keep your floors tidy.</p>\n<p>In general, the BotVac seemed to be very durable, but I pride myself on my robot care. The more simple maintenance you give it, the less complex maintenance it will need. Simple maintenance includes emptying out its dirt bin after every run or two, wiping the filter clean, and making sure that the brush and bearings are in good shape. It\u2019s not a big deal, and if you do take care of your robot, it\u2019ll take care of your floors for years.</p>\n<h2>Q&amp;A with Neato Robotics</h2>\n<p>For more details, we spoke with Nancy Nunziati, VP of Marketing, and Matt Tenuta, director of hardware engineering at Neato Robotics.</p>\n<p>\n<strong>IEEE Spectrum: What\u2019s new with the Neato since my first review of the XV-11?</strong>\n</p>\n<p>\n<strong>Matt Tenuta:</strong>\n<em> There have been a lot of changes to the platform. Going from the XV series to the BotVac is a pretty major change in terms of the system architecture. With Connected, obviously the connectivity is the big piece, but so far we haven\u2019t shown off too much about exactly what that\u2019s going to do, for strategic reasons.</em>\n</p>\n<p>\n<em>As far as functionality, there have been improvements in the navigation and behavior. Going from XV to BotVac, you should notice that the robot requires less help and is more autonomous: we do a better job of escaping from situations where would have given up in the XV days. We reacted to user feedback on the XV, and added quite a bit: new escape maneuvers, new detection on stuck conditions, and figuring out the right strategy to get out of those situations.\u00a0</em>\n</p>\n<p>\n<strong>Can you talk about some of those challenging situations, and how the robot handles them?</strong>\n</p>\n<p>\n<strong>Matt Tenuta:</strong>\n<em>The Neato algorithms have always prioritized coverage. The first goal of the robot is to mark every cell in its map as \u2019clean.\u2019 Historically, the robot has been very persistent in trying to find its way into the middle of things like chair legs, and in the XV days, there were situations where we could get into those chair legs, and we\u2019d be reacting to a number of bumper hits and tilt events and things where the robot is just bouncing around, and eventually, we\u2019d give up and ask the user for help. In the BotVac, we added some ability to detect those conditions, and we have some behavior in the robot that once it\u2019s able to detect them, it\u2019s able to identify a maneuver that\u2019s likely to get it out. It may take a few tries, but the data that we have from our testing indicate that the BotVac is much better in terms of autonomy than the XV was.\u00a0</em>\n</p>\n<p>\n<em>Drops are something as well that we very extensively test. We come up with these platforms that are designed to create very challenging environments for the robot to navigate around. We\u2019ve basically got a table with a number of different cutouts, and the robot has to be very careful in terms of how it\u2019s reacting and how far it\u2019s willing to back up. So, we don\u2019t just test against simple ledges, we test against these platforms that have holes and juts and all kinds of different geometries that challenge the robot.</em>\n</p>\n<p>\n<em>We have well over 2,000 square feet of testing environments, with different environments set up to represent kitchens, home offices, a number of different drop platforms with different surfaces and different features. For example, there\u2019s one particular type of carpet made of this particular material that can cause additional friction with the plastic that we use in the squeegee that funnels the dirt up into the intake path. If we didn\u2019t have this specific type of carpet in our lab, a few percent of users in Europe might have a problem that we would never have noticed. Neato\u2019s had products on the market for nearly six years now, and we\u2019ve learned a lot over that period.\u00a0</em>\n</p>\n<p>\n<strong>What was a challenge of operating a robot vacuum in a home environment that surprised you?</strong>\n</p>\n<p>\n<strong>Matt Tenuta:</strong>\n<em>Drapes are the one that surprised me. We\u2019ve had issues with sheer drapes. Because we have a number of different sensors that we use for wall tracking, we\u2019ve had drapes that allow our laser light to pass through and return back, but don\u2019t allow the IR light from the LED in the wall follower [on the right side of the robot]. So, you\u2019ve got an area where the LIDAR says, \"sure, you can get there,\" and then as soon as you get there, the wall follower says, \"you can\u2019t get there.\" That\u2019s one that been challenging and that we\u2019re working through for our next generation.</em>\n</p>\n<p>\n<strong>I\u2019ve been impressed by the robot\u2019s ability to deal with black carpet. How\u2019d you get that to work?</strong>\n</p>\n<p>\n<strong>Matt Tenuta:</strong>\n<em>We realized there was an issue [with robot vacuums and black carpeting]; we spent a lot of time reading reviews, both of our products and our competitor\u2019s products, and we wanted to make sure that we did better. It\u2019s one of our qualification items: we do quite a bit of testing on carpets that we know to be problematic, usually black long-pile carpet, and we\u2019ve tuned our drop sensors to be able to continue to range successfully on those surfaces. In the future, we\u2019ll be doing more with different types of sensor technologies to better distinguish surface reflectivity from distance.</em>\n</p>\n<p>\n<strong>Neato\u2019s LIDAR system was initially designed eight years ago. How\u2019s it been holding up?</strong>\n</p>\n<p>\n<strong>Matt Tenuta:</strong>\n<em>For what we\u2019re doing indoors, [Neato\u2019s LIDAR system] works very, very well. It reacts to the features that we care about; things like walls, doorways, chair legs, table legs, we get very high fidelity data back from those. We get a ton of returns that are very linear and very consistent, and that allows us to write very simple algorithms to detect features and also to reference our position relative to those objects.</em>\n</p>\n<p>\n<strong>So why isn\u2019t everyone using LIDAR for these applications?</strong>\n</p>\n<p>\n<strong>Matt Tenuta:</strong>\n<em>Neato\u2019s had the LDS technology for many years. We get people quite frequently asking about the technology and licensing and things, and I think that goes to show that it\u2019s very, very difficult to copy. The amount of understanding that Neato has in terms of how to align the optics, how to test and calibrate, how to align the firmware, there\u2019s just a ton of internal research that\u2019s been done. I think we do it better than anybody else, and I think everybody else who\u2019s tried to come up with something that works just as well has gotten frustrated.</em>\n</p>\n<p>\n<em>But for us, it\u2019s more about the product. It\u2019s more about what the LIDAR enables. It\u2019s a fantastic technology, but it can do much more than SLAM. While we can compare and contrast SLAM and VSLAM, we\u2019re thinking beyond SLAM. We\u2019re thinking about what LIDAR is going to be doing in the future. \u00a0</em>\n</p>\n<p>\n<strong>Cool, so what will LIDAR be doing in the future?</strong>\n</p>\n<p>\n<strong>Matt Tenuta: </strong>\n<em>You can imagine a number of things. If the robot is able to wake up, and within a few seconds figure out exactly what room it\u2019s in, maybe that would change its cleaning behavior. Rather than doing an entire SLAM-based cleaning run, it could do a more dedicated cleaning run. And it could also do more in terms of working with other devices: if the Neato knows exactly where it is in the environment, then maybe it could collaborate with other kinds of connected devices, share information, and obtain information to help it learn more about the environment to make a more useful product. There\u2019s quite a bit there, and I\u2019m sure you can let your imagination run a little.</em>\n</p>\n<p>\n<em>Something that we\u2019re working on for the future is trying to make it a little more flexible\u2014 giving the user the option to set behavior depending on their environment, their furniture, and the level of autonomy that they feel is right.</em>\n</p>\n<p>\n<strong>Nancy Nunziati:</strong>\n<em>From a purely speculative viewpoint, I think that you\u2019ll see us putting increasing importance on connectivity, and really being the leader in that area. From the beginning, Neato has had a fast development cycle, and you\u2019ll see that in frequent app releases and a more rapid product cycle for release.</em>\n</p>\n<p>\n<strong>How about opening up the Neato to people who want to use it as a mobile base?</strong>\n</p>\n<p>\n<strong>Matt Tenuta:</strong>\n<em>We have a ton of people on the team who are really passionate about the ROS community, and these robotic base platforms that are available for personal projects. We\u2019d love to support it, but obviously, we\u2019re a really small team, so in terms of developer outreach and SDKs and things, we can\u2019t do too much because we have far too many projects as it is. But we\u2019ll be doing more over-the-air type potential things, making the system accessible to hobbyists and developers and students. I don\u2019t think we\u2019ll have anything as robust as [the iRobot Create] anytime soon because it takes a big team to support those efforts, but maybe down the road when we have a little breathing room.</em>\n</p>\n<h2>Conclusions</h2>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MTM5Nw\"/>\n</div>\n<p>If you\u2019re in the market for a new robotic vacuum, we have no trouble recommending the Neato BotVac Connected: it\u2019s a solid robot that will do you proud. In our experience, the quality of its cleaning is easily comparable to the iRobot Roomba, which (like it or not) has over the years become the standard of comparison in this space. The Neato is noticeably better at getting dirt and hair out of carpet, and a bit less capable in some specific kitchen situations. Realistically, for most people, the biggest difference between the top of the line Neato and the top of the line Roomba is as simple as the $200 more that the Roomba 980 costs: the BotVac Connected is $700, while the Roomba 980 is $900.</p>\n<p>However, this is the reason why we usually don\u2019t recommend top of the line robots to most people: you\u2019re paying a premium for new features that you may not get hundreds of dollars worth of value out of. So, the real question is whether the premium you\u2019ll pay for the BotVac Connected\u2019s connectedness is really worth it: the non-connected BotVac D series starts at $450, while the Connected is $700.</p>\n<p>Fortunately, the WiFi connectivity and app isn\u2019t the only upgrade that the Neato BotVac Connected has over its siblings. You\u2019ll also get some additional cleaning settings (including Turbo and Eco mode), a more powerful internal vacuum, and a lithium battery (instead of a NiMH) that will add up to 2,000 square feet to the area that the robot can clean on one charge. Is all of this worth an extra $200? It depends: if you\u2019re likely to run the vacuum when you\u2019re home most of the time, then the app won\u2019t do a whole lot for you, and if if you\u2019re willing to commit to a fixed schedule, you can just use the scheduling function on the vacuum itself anyway. On the other hand, if you want to be able to initiate a cleaning cycle from halfway around the world, the app might be a good idea.</p>\n<p>The other reason to invest in the Connected is to (potentially) benefit from the future of a house full of connected devices. This is the future, mind you, not the present, and while it sounds like Neato is actively working on a bunch of stuff, we don\u2019t have a list or a timeline or anything tangible that you could use to talk yourself into buying a robot with a lot of expensive potential baked into it.</p>\n<p>As much as I\u2019ve tried to remain impartial by comparing the performance and features and whatnot of these robot vacuums, I have to mention one more thing: the technology that Neato\u2019s robots have inside them is just awesome. The LIDAR is a small miracle of engineering, and it works amazingly well. It seems so much more futuristic than using VSLAM like every other navigating robot vacuum does, and having a LIDAR in your household robot has the potential to enable much, much more.\u00a0</p>\n<p>The Neato BotVac Connected is available on <a shape=\"rect\" href=\"https://neatorobotics.com/where-to-buy/botvac-connected/\">Amazon, Best Buy, and other retailers</a>.</p>\n<p>[ <a shape=\"rect\" href=\"https://neatorobotics.com/robot-vacuum/botvac-d-series/\">Neato Robotics</a> ]</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/KpoLaufaF7I\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Mon, 09 May 2016 20:21:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/automaton/robotics/home-robots/review-neato-botvac-connected</guid>\r\n      <dc:creator>Evan Ackerman</dc:creator>\r\n      <dc:date>2016-05-09T20:21:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MTQxMQ\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MTQxMQ\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/automaton/robotics/home-robots/review-neato-botvac-connected</feedburner:origLink></item>\r\n    <item>\r\n      <title>Hyperloop Transportation Technologies Picks Passive Levitation for Pods</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/FHGfVBEo-wc/hyperloop-company-plans-to-levitatepassively</link>\r\n      <description>The forward motion of its pods will provide magnetic lift</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"050916hyperloop-1462813412786.jpg\" src=\"http://spectrum.ieee.org/img/050916hyperloop-1462813412786.jpg\"/>\n<figcaption class=\"hi-cap\">Image: Hyperloop</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>The forward motion of its subsonic, people-carrying pods\u00a0will provide the necessary\u00a0magnetic levitation\u00a0as a side\u00a0effect, says\u00a0Hyperloop Transportation Technologies, a Southern Calif.-based startup, in an announcement <a shape=\"rect\" href=\"http://www.prnewswire.com/news-releases/hyperloop-transportation-technologies-inc-reveals-hyperloop-levitation-system-300264946.html\">made\u00a0this morning</a>.\u00a0</p>\n<p>Dirk Ahlborn, the CEO and founder of HTT, told <em>IEEE Spectrum</em> that this passive leviation saves energy and trouble. \u201cOne of the biggest problems\u00a0is the need for\u00a0a high-powered track; this allows us to achieve leviation without having power stations all along the track,\u201d he said.</p>\n<p>Forward motion is\u00a0important\u00a0to HTT as well, which has been generating news stories with some regularity as it\u00a0courts partners and investors to help realize\u00a0Elon Musk\u2019s 2013 vision of\u00a0a\u00a0partially evacuated, superfast tubular train: the\u00a0Hyperloop. Back in March <a shape=\"rect\" href=\"http://spectrum.ieee.org/tech-talk/transportation/alternative-transportation/will-the-hyperloop-arisein-slovakia\">HTT said</a> it had come to an agreement with the government of\u00a0Slovakia to explore\u00a0building a track in that country. Later this week, the company is to stage a media event in Las Vegas.</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/4zAMgbYhPVE\"/>\n</p>\n<p>The passive levitation system, called Inductrack, was developed in the 1990s at Lawrence Livermore National Labs, in California. It works by lining the bottom of the pod with\u00a0permanent magnets, placed\u00a0in a so-called Halbach array so as\u00a0to induce a repelling field when they pass over shorted, that is, non-powered, electromagnetic coils on the railbed.\u00a0Because the maglev effect is passive, any power\u00a0cutoff\u00a0would both slow the\u00a0pod and cause it to\u00a0settle\u00a0down on the track.\u00a0</p>\n<p>\u201cThe levitating force becomes effective at very low vehicle speeds and remains constant at high speeds,\u201d <a shape=\"rect\" href=\"http://gcep.stanford.edu/pdfs/ChEHeXOTnf3dHH5qjYRXMA/09_Post_10_11_trans.pdf\">wrote the inventor, the late Richard F. Post, of Livermore,\u00a0</a>in a presentation\u00a0at Stanford University in 2005. An accompanying slide shows that\u2014at least in the non-vacuum-packed maglev train\u00a0envisaged at\u00a0Livermore\u2014half of the maximum lift force is achieved at a speed of just 1.2 kilometers per hour, or 0.75 mph. That\u2019s\u00a0about\u00a0the <a shape=\"rect\" href=\"http://www.reptilesmagazine.com/the-worlds-fastest-tortoise-he-sprints-at-028-meters-per-second-trending/\">speed of a racing\u00a0turtle</a>. \u00a0</p>\n<p>The high-speed HTT system\u00a0would be tuned differently, so its pods would levitate only at a\u00a0higher velocity, about 10 meters per second, Ahlborn says. At that point the pod would be just a centimeter or two above the track.</p>\n<p>\u201cWe licensed the technology\u00a0exclusively for\u00a0HTT,\u201d he said. \u201cDr. Post passed away a year ago,, but\u00a0we met in the early days, and he was very excited about the technology.\u201d</p>\n<p>At least one other startup, the similarly named Hyperloop Technologies,\u00a0is also\u00a0pursing the Hyperloop, although it seems to want to\u00a0buoy\u00a0its pods on a cushion of air. The <a shape=\"rect\" href=\"http://spectrum.ieee.org/transportation/mass-transit/elon-musks-hyperloop-proposal-gains-momentum\">same goes </a>for a number of the student teams that have participated in competitions for the best Hyperloop design.</p>\n<p>Ahlborn argues that air cushioning would be harder to achieve, given the low-pressure inner environment of the tube. He said it\u00a0would also be less comfortable fot the passengers, for instance by making it harder to bank on turns.</p>\n<p>There\u2019s no word yet on the preferred mode of levitation of the latest\u00a0commercial entry:\u00a0Transpod, of Toronto. Transpod says\u00a0that later this year\u00a0it will unveil a\u00a0plan to get a Hyperloop up and running\u00a0by 2020.\u00a0</p>\n<p/>\n<p/>\n<p/>\n<p/>\n<p/>\n<p/>\n<p/>\n<p/>\n<p/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/FHGfVBEo-wc\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Mon, 09 May 2016 17:03:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/tech-talk/transportation/alternative-transportation/hyperloop-company-plans-to-levitatepassively</guid>\r\n      <dc:creator>Philip E. Ross</dc:creator>\r\n      <dc:date>2016-05-09T17:03:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MTIwMg\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MTIwMg\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/tech-talk/transportation/alternative-transportation/hyperloop-company-plans-to-levitatepassively</feedburner:origLink></item>\r\n    <item>\r\n      <title>Two Andrews and an Algorithm Aim to Accelerate Drug Discovery</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/PxW3PJa2498/two-andrews-and-an-algorithm-aim-to-accelerate-drug-discovery</link>\r\n      <description>Pharmaceutical researchers hope to find promising paths to cancer cures and other remedies, but many are dead ends. TwoXAR wants computers to lead their way.</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"TwoXarhalfcolumn-1462469977704.jpg\" src=\"http://spectrum.ieee.org/img/TwoXarhalfcolumn-1462469977704.jpg\"/>\n<figcaption class=\"hi-cap\">Photo: Tekla Perry</figcaption>\n<figcaption>Andrew M. Radin (left) and Andrew A. Radin's matching names brought them together; now they are heading up drug discovery startup twoXAR.</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Pharmaceutical researchers have for years used computers to aid them in, say, modeling molecules\u00a0or mining clinical records. This allows them to\u00a0come up with candidates for potential drug therapies, often thousands of them at a time. The researchers then produce medicines\u00a0and test them\u2014first in petri dishes,\u00a0then in animals, and,\u00a0if the drugs continue to prove promising, in humans. This process can take a decade or longer and cost billions of dollars, and means that the road to a cure for cancer, diabetes, or Parkinson\u2019s\u00a0(or even better palliative treatments for these and other diseases), is a long one.</p>\n<p/>\n<p/>\n<p>Palo Alto-based startup <a shape=\"rect\" href=\"http://www.twoxar.com/\">twoXAR</a> aims to make that road a lot shorter, by turning the task of navigating\u00a0through the forest of potential drug candidates over to an algorithm. And the company\u2019s\u00a0founders, whose background is in\u00a0computer science instead of\u00a0pharmaceutical research, say they are uniquely positioned to make it happen.</p>\n<p/>\n<p>\u201cConsider the auto industry,\u201d says twoXAR cofounder and CEO\u00a0<a shape=\"rect\" href=\"https://www.linkedin.com/in/andrewradin?authType=NAME_SEARCH&amp;authToken=FF-7&amp;locale=en_US&amp;srchid=400244151462468732390&amp;srchindex=1&amp;srchtotal=11&amp;trk=vsrp_people_res_name&amp;trkInfo=VSRPsearchId%3A400244151462468732390%2CVSRPtargetId%3A368856%2CVSRPcmpt%3Aprimary%2CVSRPnm%3Atrue%2CauthType%3ANAME_SEARCH\">Andrew Radin</a>. \u201cWhen you ask an auto engineer to make a car safer, he comes up with seatbelts and airbags and antilock brakes. But ask a software engineer, and he says, \u2018Let\u2019s replace the driver.\u2019 Driverless cars did not originate at GM and Ford.\u201d</p>\n<p/>\n<p>\u201cComputers can do a lot of things,\u201d says twoXAR cofounder and chief business officer <a shape=\"rect\" href=\"https://www.linkedin.com/in/radinandrew\">Andrew Radin</a>.\u00a0\u201cBut they usually augment what researchers are already doing. People working in drug discovery are life scientists, chemists, molecular biologists; they build molecular models, and then decide which ones to test, instead of letting computers crunch the data to make the decision about which candidates to investigate further.\u201d</p>\n<p>You may have noticed something peculiar\u00a0about those last two paragraphs\u2014besides their introduction to the general premise of the company. And no, it wasn\u2019t a type.\u00a0TwoXAR\u2019s cofounders have the same name: Andrew Radin. In fact, twoXAR stands for \u201ctwo times Andrew Radin.\u201d\u00a0And the two Andrews\u2014CEO Andrew A. Radin and CBO Andrew M. Radin\u2014connected with each other in a way that\u2019s almost as serendipitous as some of the drug discovery processes they are trying to replace.</p>\n<p/>\n<p>It all started because Andrew A, a decade ago, purchased the Internet domain AndrewRadin.com. In 2008, Andrew A, recalls, he got an email from then-stranger Andrew M looking to buy the domain. \u201cI said no, go away, and he kept arguing that since I wasn\u2019t using it, I should sell it. I ended up being pretty rude to get rid of him.\u201d</p>\n<p/>\n<p>At the time, Andrew A, who had BS and MS degrees in computer science from the Rochester Institute of Technology, had been living in Silicon Valley for some time, working at startups and starting companies of his own. Andrew M, with a BS in biochemistry and cell biology and a BA in economics from the University of California, San Diego, was working in investment banking and evaluating MBA programs.</p>\n<p/>\n<p>Approximately one year later, a friend of Andrew M, as a prank, started an Andrew Radin fan club on Facebook; Andrew A was one of several other Andrew Radins from around the country who joined the group. So the two were now connected on social media, though Andrew A still thought of Andrew M as that creepy guy who tried to buy his URL.</p>\n<aside class=\"inlay pullquote rt med-lrg\">\u201cThat\u2019s when he evolved in my mind to \u2018not a creep\u2019\u201d</aside>\n<p>In 2010, Andrew A traveled for a month in China as a tourist, and posted the occasional update on Facebook. A few months after he got back, Andrew M, who was moving to China for six months to study Chinese, Facebook messaged him and asked if he could call him to ask him for China travel advice. They talked for a few hours on the phone. \u201cThat\u2019s when he evolved in my mind to \u2018not a creep,\u2019\u201d Andrew A recalls.</p>\n<p/>\n<p>They didn\u2019t talk again until 2013. Andrew A was studying bioinformatics at Stanford; Andrew M was at grad school at MIT.</p>\n<p/>\n<p>\u201cAn MIT class of mine was coming out to Silicon Valley on a study tour,\u201d says Andrew M. \u201cSo I contacted him [Andrew A] and had him sit down with my class to speak about his startup experiences; that was the first time we met in person.\u201d</p>\n<p/>\n<p>That\u2019s, essentially, how the two Andrews became friends. The story of how they came to start twoXAR was a different kind of convergence.</p>\n<p/>\n<p>Andrew A had worked in software development at Nortel Networks and then America Online, but then dove into Silicon Valley startup life, serving as CTO or engineering lead at startups, as he recalls, in \u201call sorts of different industries: making phone calls, videogames, advertising solutions, geolocation,\u201d from 2004 to 2013. None were unicorns\u2014or anything close, though the geolocation company, <a shape=\"rect\" href=\"http://techcrunch.com/2013/07/19/with-its-acquisition-of-locationary-apple-zeros-in-on-maps-big-data-and-competing-with-google/\">Locationary, was acquired by Apple</a> and the acquisition gave him enough cash to not work for a while.</p>\n<p>By 2013, he was tired of focusing on technically interesting but not exactly earth-shaking developments, and wanted to do something meaningful. Initially, he\u2019d turned to considering the problem of homelessness, but his wife encouraged him to do something, he said \u201cwhere your skills would make a difference.\u201d He eventually settled on biomedical informatics, that is, using computer science to solve medical problems. Realizing he needed to go back to school to learn more about that field, he enrolled in a graduate program at Stanford.</p>\n<p/>\n<p>Meanwhile, Andrew M, 13 years younger, had always had the startup bug. \u201cMy goal,\u201d he says, \u201cwas always to start a biotech company. I was doing genetic engineering when I was in high school and chose UC San Diego because there were a lot of biotech startups that came out of it; I majored in biochemistry and economics.\u201d Andrew M\u2019s first job out of college was working for a bank in tech investing; then he enrolled in MIT\u2019s graduate business school where he focused on entrepreneurship.</p>\n<p/>\n<p>So, in 2014, the two Andrews, had one more thing in common: they were both in graduate school.</p>\n<p/>\n<p>Over at Stanford, Andrew A, taking <a shape=\"rect\" href=\"https://sites.google.com/site/bmi2172016/\">BMI 217</a>, was assigned to come up with an idea for a medical problem that could be solved by computer science. He came up with a methodology to use data science to find new drugs. Essentially, the idea involved an algorithm for merging and mining a number of disparate biomedical databases to make predictions about what drugs were most likely to be successful in testing as particular disease therapies. The databases used included, but weren\u2019t limited to, clinical record searches, molecular similarity models, and <a shape=\"rect\" href=\"http://www.dana-farber.org/Research/Featured-Research/Identifying-drug-candidates-through-gene-expression-screening.aspx\">gene expression information</a>, among others. (Clinical record searches examine individual medical records of people taking a drug for one disease to see if they have unusually low rates of any other diseases or symptoms that indicate efficacy; molecular similarity models look at drugs already in use and try to find other substances with similar structures, gene expression looks at certain gene signatures and matches them with molecules known to regulate them).</p>\n<p/>\n<p>\u201cIt was essentially a tool for comparing relationships between huge and disparate data sets to distill signal from noise,\u201d he says. The technology can combine any number of public and proprietary data sets to rank the probability of a particular drug candidate being effective in treating a particular disease.</p>\n<p/>\n<aside class=\"inlay pullquote lt med-lrg\">\u201cPaul gave me the \u2018You\u2019re an idiot\u2019 look and said, \u2018If you write a paper nothing will happen; if you want something to happen you need to write a patent application and start a company.\u201d</aside>\n<p>Initially, he planned to simply write a paper about his methodology. Then he had lunch with V. Paul Lee, former president of Electronic Arts, who had invested in some of Andrew A\u2019s startups \u201cPaul gave me the \u2018You\u2019re an idiot\u2019 look and said, \u2018If you write a paper nothing will happen; if you want something to happen you need to write a patent application and start a company.\u2019\u201d</p>\n<p/>\n<p>So, Andrew A says, he continued for months grinding away at his dilemma\u2014paper or company? Chatting on the\u00a0phone with Andrew M, he reviewed his dilemma, and asked him if he thought a company could be built around the technology. As Andrew A recalls, \u201cHe told me to come out to Boston\u00a0and whiteboard it with some MIT professors.\u201d He did, in June of 2014.</p>\n<p/>\n<p>The professors helped the two Andrews see that there was indeed a business opportunity. They made the final decision to start twoXAR while sailing in one of MIT\u2019s <a shape=\"rect\" href=\"http://sailing.mit.edu/future/tech.php\">Tech Dinghy\u2019s</a> on the Charles River and incorporated the company in June of that year. In July, Andrew M came out to Silicon Valley, moving in\u2014temporarily\u2014with Andrew A and his wife.</p>\n<p/>\n<p>Today, twoXAR has ten employees, $4.3 million in venture funding, and a sweet deal on office space in Palo Alto. They are hiring, and by the end of the year expect to add a few key team members in data science and bioinformatics.</p>\n<p/>\n<aside class=\"inlay pullquote rt med-lrg\">\u201cDrug discovery to date has been the reverse of Moore\u2019s Law; it is becoming logarithmically more difficult to find new drugs and the cost keeps going up.\u201d</aside>\n<p>The company has been working with research institutions and biopharmaceutical companies to test their prediction algorithm on known drugs, that is, running sets of data though their algorithms and seeing if the algorithm includes existing drugs in its set of drugs that it determines are promising candidates. So far, they\u2019ve evaluated over 40 diseases in this way, including cancer, autoimmune, and neurologic disorders. Based on those tests, they came up with four areas for their initial work, which the company has yet to publicly identify. In those areas, they have made drug candidate predictions and have begun testing those compounds in animals. The two Andrews, at this point, do not plan to sell the software to pharmaceutical companies; they believe it would take too long to change an entrenched culture that remains skeptical about letting computers lead decision-making. Instead, they are working to build their own pipeline of drug candidates, typically working with partners at research organizations.</p>\n<p>They say several partnerships have been signed with academic and commercial organizations but only one is public. That one, announced last month, has twoXAR working with the Department of Dermatology at Stanford\u2019s School of Medicine to identify drug candidates targeting lymphatic malformation, epidermolysis bullosa simplex (EBS), and other rare disorders.</p>\n<p/>\n<p>Today, there is \u201ca tremendous opportunity for finding more precise medicines for diseases, common and rare, that have inadequate or non-existent treatments,\u201d says Andrew A. \u201cDrug discovery to date has been the reverse of Moore\u2019s Law; it is becoming logarithmically more difficult to find a new drugs and the costs keep going up.\u201d</p>\n<p/>\n<p>TwoXAR aims to reverse that trend.</p>\n<p/>\n<p>And that domain name? The two now have a joint website at <a shape=\"rect\" href=\"http://www.andrewradin.com/\">AndrewRadin.com</a>.</p>\n<p/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/PxW3PJa2498\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Fri, 06 May 2016 21:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/view-from-the-valley/at-work/start-ups/two-andrews-and-an-algorithm-aim-to-accelerate-drug-discovery</guid>\r\n      <dc:creator>Tekla S. Perry</dc:creator>\r\n      <dc:date>2016-05-06T21:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MDQwNg\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MDQwNg\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/view-from-the-valley/at-work/start-ups/two-andrews-and-an-algorithm-aim-to-accelerate-drug-discovery</feedburner:origLink></item>\r\n    <item>\r\n      <title>Silicon Nanoparticles Could Be a Boon for Fiber Optic Telecommunications</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/PUs3jSqM4xY/silicon-nanoparticles-could-be-a-boon-for-fiber-optic-telecommunications</link>\r\n      <description>Silicon nanoparticles significantly increase the intensity of the Raman effect for nanoscale light emitters</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"RamanScatteringblog-1462556169984.jpg\" src=\"http://spectrum.ieee.org/img/RamanScatteringblog-1462556169984.jpg\"/>\n<figcaption class=\"hi-cap\">Illustration: ITMO University</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>An international team of researchers from the Moscow Institute of Physics and Technology (MIPT), ITMO University (St Petersburg), and the Australian National University have demonstrated that\u00a0<a shape=\"rect\" href=\"http://www.photonicsonline.com/doc/silicon-nanoparticles-pave-the-way-towards-nanoscale-light-emitters-0001\">silicon nanoparticles can significantly increase the intensity of the Raman effect</a>. The results could be a boon to\u00a0nanoscale light emitters and nanoscale amplifiers used in fiber optic telecommunications.</p>\n<p/>\n<p>In the <a shape=\"rect\" href=\"http://spectrum.ieee.org/searchContent?q=Raman+effect&amp;type=&amp;sortby=relevance\">Raman scattering effect</a>, light interacts with certain materials to produce longer or shorter wavelengths, or different colors. This occurs because the light causes the molecule it is interacting with to increase its energy in an amount equivalent to the vibration of the molecule. In this newly energized state, the molecule re-emits a photon that has a smaller amount of energy than the incident photon. This smaller photon energy leads to a longer wavelength and a red color.</p>\n<p/>\n<p>This effect is often leveraged today in fiber optic telecommunication in order to boost signals traveling through long stretches of glass fiber.\u00a0Raman scattering\u00a0also makes it possible to transfer light from a strong pump beam into a weaker data beam. This Raman amplification enables most long-distance telephone calls today.</p>\n<p/>\n<p>Typically, metallic nanoparticles are used to induce this Raman scattering effect. However, in research described in the journal <a shape=\"rect\" href=\"http://pubs.rsc.org/en/Content/ArticleLanding/2016/NR/C5NR07965A#!divRelatedContent\">\n<em>Nanoscale</em>\n</a>, Russian and Australian\u00a0researchers looked at silicon nanospheres that support optical resonances, known as <a shape=\"rect\" href=\"http://spectrum.ieee.org/searchContent?q=Mie&amp;type=&amp;sortby=relevance\">Mie resonances</a>.</p>\n<p/>\n<p>The resonant wavelengths depend on the particle size. The largest of these, known as the magnetic dipole resonance, is typically\u00a0comparable to the diameter of the particle.</p>\n<p/>\n<p>However, this latest research revealed\u00a0that silicon\u2019s refractive index\u2014how light propagates through a medium\u2014is so large that its magnetic diplole resonance is observed in wavelengths longer than 300 nanometers even though the diameter of the particle is only 100 nanometers.</p>\n<p/>\n<p>The upshot is that much smaller silicon nanoparticles can be used to produce enhanced optical phenomena, such as spontaneous light emission, and enhanced light absorption.</p>\n<p/>\n<p>In the experiments, the researchers found that when light hit a resonant particle, it produced a Raman emission intensity 100 times greater than that of non-resonant particles.</p>\n<p>\u201cThe Raman effect is incredibly useful in practice, and will help not only in detecting microscopic amounts of chemical compounds,\u201d said Denis Baranov, a post-graduate student of MIPT and one of the authors of the paper, in a press release, \u201cbut [will also be useful for]\u00a0transmitting information over long distances.\u201d Baranov notes that because electronics and optical devices continue to shrink, the need for nanostructures that have outsize Raman effects is increasingly important. \u201cOur observations have revealed a potential candidate\u2013silicon nanoparticles,\u201d says Baranov.</p>\n<p/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/PUs3jSqM4xY\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Fri, 06 May 2016 19:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/nanoclast/semiconductors/optoelectronics/silicon-nanoparticles-could-be-a-boon-for-fiber-optic-telecommunications</guid>\r\n      <dc:creator>Dexter Johnson</dc:creator>\r\n      <dc:date>2016-05-06T19:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MTAwNw\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MTAwNw\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/nanoclast/semiconductors/optoelectronics/silicon-nanoparticles-could-be-a-boon-for-fiber-optic-telecommunications</feedburner:origLink></item>\r\n    <item>\r\n      <title>Jet-Lag Sleep App is a Viable Way to Collect Big Data</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/V4F8R0k9q9g/jet-lag-sleep-app-is-a-viable-way-to-collect-big-data</link>\r\n      <description>Researchers prove sleep data collected from a mobile app is reliable</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"PhonesleepGettyImages611475641-1462394184611.jpg\" src=\"http://spectrum.ieee.org/img/PhonesleepGettyImages611475641-1462394184611.jpg\"/>\n<figcaption class=\"hi-cap\">Getty Images</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Can scientists trust a mobile app\u00a0as a reliable\u00a0vehicle for collecting health data? A <a shape=\"rect\" href=\"http://advances.sciencemag.org/content/2/5/e1501705\">study</a> published today in <em>Science Advances</em> suggests the answer is yes, it\u2019s possible, at least for sleep studies.</p>\n<p>Researchers have for years been eyeing the trove of health data sets that could be collected via mobile apps. Cheap to build and easy to distribute, apps can make recruitment of massive, global study populations possible on a grad student\u2013like budget.</p>\n<p>But the reliability of that kind of data is still largely unproven, and presents a risk for scientists. Unlike laboratory-based research or phone surveys, there\u2019s <span>no study coordinator to keep people honest and on track, and no voice awaiting a response at the other end of the line.</span>\n</p>\n<p>But researchers at the University of Michigan in Ann Arbor say that global sleep data collected from their custom app yielded reliable results. \u201cIt validates mobile apps as a data\u00a0collection method,\u201d says\u00a0<span>\n<a shape=\"rect\" href=\"http://www-personal.umich.edu/~ojwalch/\">Olivia Walch</a>, a graduate student at the University of Michigan and\u00a0lead author of the report. </span>\n</p>\n<p>Walch and her team designed an <a shape=\"rect\" href=\"http://entrain.math.lsa.umich.edu\">app called Entrain</a> to help travelers adjust to new time zones. Users\u00a0input information about their sleep habits, home time zone, and where they would be traveling, and the app provided advice on minimizing jet lag.</p>\n<p>The travel advice was the carrot that\u00a0motivated people to use the app, but was actually beside the point. What the scientists actually cared about\u00a0was the chance to get information about\u00a0baseline sleep habits and home time zones. App users could opt to share that information with the research team; eight percent of them did. That gave the Michigan team sleep data from more than 8,000 respondents around the world \u201cat essentially no cost,\u201d the researchers said in their report.</p>\n<p>To validate the data, the Michigan team compared it with our knowledge of circadian rhythms and sleep data collected using more traditional methods<span>, such as sleep labs and\u00a0questionnaires. The researchers used mathematical models to simulate\u00a0bed times and wake times in various time zones\u2014what people\u2019s sleep habits would be if they were governed by sunsets and sunrises, rather than social influences. </span>\n</p>\n<p>\n<span>To their relief, their data lined up with known trends in sleep habits amassed from more traditional studies. \u201cThere were many sleepless nights\u201d waiting for those results, says Walch. \u201cTo see it line up was really gratifying.\u201d Prior studies have suggested that women sleep more than men, and that bed times get earlier\u00a0with age. The Entrain data yielded those same results.</span>\n</p>\n<p>\n<span>The Michigan study revealed new sleep trends too. Residents of Singapore and Japan go to bed later and get the least sleep, while residents of the Netherlands go to be earlier and get the most sleep. Later bed times didn\u2019t necessarily mean later wake times, the researchers found. They hypothesize, based on these and other trends they observed, that social influences are governing bed time, and circadian rhythms\u2014our internal clocks\u2014govern wake times. </span>\n</p>\n<p class=\"jwcode\">\n<script class=\"jwembed\" src=\"//content.jwplatform.com/players/1uXp5IyE-7pFgM9ap.js\"/>\n</p>\n<p>\n<span>The success of the Michigan study is due largely to the fact that the\u00a0Entrain app was widely adopted. And in a sense, they got lucky. When they launched the app in 2014, several global media outlets <a shape=\"rect\" href=\"http://www.npr.org/sections/health-shots/2014/04/11/301579620/this-jet-lag-app-does-the-math-so-youll-feel-better-faster\">picked up the story</a>. Without that, spreading the word would have been difficult. \u201cOur advertising budget was zero,\u201d says Walch.</span>\n</p>\n<p>The researchers plan to continue collecting data from Entrain. Walch is improving its interface, and adapting it to work with wearables.\u00a0The app has been downloaded over 150,000 times, Walch says.</p>\n<p>\n<span>Other groups have embarked on the task of scientific data collection with mobile apps as well. A collaboration of academic researchers studied mobile interventions that aid medication use. In the journal <em>PLOS ONE</em>, the researchers <a shape=\"rect\" href=\"http://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0116980\">reported</a> last year that \u201cmobile applications may be a promising approach to support the treatment of patients with chronic conditions.\u201d Seattle-based\u00a0<a shape=\"rect\" href=\"http://sagebase.org\">Sage Bionetworks</a> in March published <a shape=\"rect\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4776701/\">a study</a> on tracking symptoms of Parkinson disease through its app mPower. The group reported that such data \u201cmay help establish baseline variability of real-world activity measurement collected via mobile phones, and ultimately may lead to quantification of the ebbs-and-flows of Parkinson symptoms.\u201d</span>\n</p>\n<p>An academic group out of Germany has been collecting sleep habit data since 2003 through its <a shape=\"rect\" href=\"https://www.bioinfo.mpg.de/mctq/core_work_life/core/introduction.jsp?language=eng\">Munich ChronoType Questionnaire</a>. The survey is Internet based, rather than mobile, and reveals many of the same trends found in the Michigan study. Its developer, <a shape=\"rect\" href=\"http://www.mcn.uni-muenchen.de/members/regular/roenneberg/index.html\">Till Roenneberg</a>, <a shape=\"rect\" href=\"http://www.nature.com/nature/journal/v498/n7455/full/498427a.html\">advocated in 2013</a> for a global effort to use mobile phones and wearables to collect sleep data\u2014a project he dubbed \u201cthe human sleep project.\u201d\u00a0The call to arms inspired Walch\u2019s team, she says. \u201cWe need infrastructure in place so that when everybody has a wearable, we\u2019re ready to analyze the data.\u201d</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/V4F8R0k9q9g\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Fri, 06 May 2016 18:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/the-human-os/biomedical/devices/jet-lag-sleep-app-is-a-viable-way-to-collect-big-data</guid>\r\n      <dc:creator>Emily Waltz</dc:creator>\r\n      <dc:date>2016-05-06T18:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc0OTk3NA\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc0OTk3NA\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/the-human-os/biomedical/devices/jet-lag-sleep-app-is-a-viable-way-to-collect-big-data</feedburner:origLink></item>\r\n    <item>\r\n      <title>Israeli Startup\u2019s Vision Device Can Help the Nearly-Blind Read and Recognize Faces</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/Km5jpIHM4ZM/israeli-startups-vision-device-can-help-nearlyblind-read-and-recognize-faces-study-shows</link>\r\n      <description>Independent study by ophthalmologists shows that the clip-on device dramatically improves ability to carry out daily life tasks</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"OrCam device-1462541176567.jpg\" src=\"http://spectrum.ieee.org/img/OrCam device-1462541176567.jpg\"/>\n<figcaption class=\"hi-cap\">Photo: OrCam</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Over 21 million adults in the United States\u00a0suffer from impaired vision. Several companies are have been racing to develop wearables and implants that can improve or restore eyesight in these patients.</p>\n<p/>\n<p>One such wearable vision device, made by Israeli startup\u00a0<a shape=\"rect\" href=\"http://www.orcam.com\">OrCam</a>, just got an enthusiastic <a shape=\"rect\" href=\"http://www.ucdmc.ucdavis.edu/publish/news/newsroom/11071\">thumbs up from eye doctors</a> who tested the gadget on 12 legally blind patients. OrCam\u2019s little camera device, which clips on to eyeglasses, reads aloud words on labels and signs, and names objects and faces for the wearer. Opthalmologists at the University of California Davis Eye Center published results of their study in the journal <em>\n<a shape=\"rect\" href=\"http://archopht.jamanetwork.com/article.aspx?articleid=2520689\">JAMA Opthalmology</a>. </em>They found\u00a0that the device dramatically improved the reading ability of the patients, many of them elderly.</p>\n<p/>\n<p>\u201cWe haven\u2019t seen anything else comparable to OrCam\u2019s product,\u201d says <a shape=\"rect\" href=\"http://www.ucdmc.ucdavis.edu/publish/facultybio/search/faculty/8\">Mark J. Mannis</a>, director of the UC Davis Eye Center.\u00a0\u201cWhile the technology is sophisticated, it is easy to operate even for elderly patients for whom technology is daunting,\u201d says Mannis, who specializes in corneal transplants. \u201cAnd secondly, it\u2019s very portable, not obtrusive, and it works very efficiently.\u201d</p>\n<p/>\n<p>That second part is a direct\u00a0comparison to the existing crop of visual aids like magnifiers and smartphone apps for reading help. \u201cMagnifiers don\u2019t work for patients with macular degeneration where there\u2019s essentially a big blotch on the center of vision so if they look at a face, it\u2019s blacked out,\u201d Mannis says. \u201cThey need to read or recognize faces and objects without needing a working retina.\u201d This is where smartphone reading apps come in, but they have limited capabilities and aren\u2019t easy to use.</p>\n<p/>\n<p>OrCam\u2019s camera device clips on magnetically to a user\u2019s eyeglasses. It is connected via a thin wire to a small computer that fits in a pocket. The interface is minimal: The wearer can tap the device or simply point to an object or text that the device recognizes. Like the Google Glass, the audio goes through a bone conduction speaker so only the wearer can hear the machine\u2019s interpretation. The device reads text and recognizes faces and objects such as buses, street lights and signs, and paper money. It comes with a library of objects that the user can add to by waving the object in front of the camera so it can see it from all angles.</p>\n<p/>\n<p>This requires sophisticated artificial vision and image recognition wizardry. But unlike the tremendous computing power needed by deep learning techniques used for vision and recognition, the OrCam device uses a computer vision algorithm called <a shape=\"rect\" href=\"http://arxiv.org/abs/1109.0820\">ShareBoost</a>. The program\u00a0was\u00a0developed by the company\u2019s founder,\u00a0<a shape=\"rect\" href=\"http://www.cs.huji.ac.il/~shashua/\">Amnon Shashua</a>, a professor of computer science at the Hebrew University in Jerusalem. The algorithm bases its decision on the most informative parts of an image, with a reasonable tradeoff between recognition accuracy and speed.</p>\n<p/>\n<p>OrCam is <a shape=\"rect\" href=\"http://spectrum.ieee.org/the-human-os/biomedical/bionics/a-new-bionic-eye-infrared-lightpowered-retina-implant-coming\">reportedly selling the device</a> in the U.S., UK, Israel, and Canada for $2,500, the price of a good hearing aid.</p>\n<p/>\n<p>For the UC Davis study, Mannis and\u00a0a colleague\u00a0trained the patients to use the device for 10 tasks such as reading from a computer screen, a newspaper and a book, which they couldn\u2019t do on their own. After using the device at home for a week, all of them could perform the tasks. A subset of 7 patients who had used\u00a0visual aids such as magnifiers and smartphones could do the tasks better with the OrCam device.</p>\n<p/>\n<p>One caveat is that the device does not work well in low light conditions. And contrary to the company\u2019s claims, the device might not help the blind or those with profound vision loss. According to\u00a0Mannis, it\u2019s, \u201cmostly because you do have to direct it to what you\u2019re looking at.\u201d</p>\n<p/>\n<p>\n<a shape=\"rect\" href=\"http://spectrum.ieee.org/tech-talk/biomedical/bionics/how-would-you-like-your-bionic-vision\">Retinal implants</a> might be more relevant for the blind. The first such bionic eye, made by Second Sight,\u00a0<a shape=\"rect\" href=\"http://spectrum.ieee.org/tech-talk/biomedical/bionics/bionic-eye-implants-will-hit-the-us-market-this-year\">is already on the market</a>, and a few others are on the way or being <a shape=\"rect\" href=\"http://spectrum.ieee.org/the-human-os/biomedical/bionics/a-new-bionic-eye-infrared-lightpowered-retina-implant-coming\">developed for human trials</a>.</p>\n<p/>\n<p>But for those suffering from low-vision due to diseases like macular degeneration and glaucoma, wearables like OrCam\u2019s offer a bright\u00a0glimmer of hope. \u201cExternal wearable devices are much further along in development and testing,\u201d Mannis says. \u201cThey\u2019re much easier than performing implant surgery and are probably more useful for a broader swath of people.\u201d</p>\n<p/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/Km5jpIHM4ZM\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Fri, 06 May 2016 17:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/the-human-os/biomedical/devices/israeli-startups-vision-device-can-help-nearlyblind-read-and-recognize-faces-study-shows</guid>\r\n      <dc:creator>Prachi Patel</dc:creator>\r\n      <dc:date>2016-05-06T17:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MDczOQ\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MDczOQ\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/the-human-os/biomedical/devices/israeli-startups-vision-device-can-help-nearlyblind-read-and-recognize-faces-study-shows</feedburner:origLink></item>\r\n    <item>\r\n      <title>Video Friday: Snake Monster, Crash-Proof Drone, and Usain Bot</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/i-sPLx2jRnM/video-friday-cmu-snake-monster-flyability-drone-puma-racing-robot</link>\r\n      <description>Your weekly selection of awesome robot videos</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"cmu-snake-monster-1462546368785.jpg\" src=\"http://spectrum.ieee.org/img/cmu-snake-monster-1462546368785.jpg\"/>\n<figcaption class=\"hi-cap\">Image: CMU Biorobotics Lab via YouTube</figcaption>\n<figcaption>What do you call a five-legged hexapod?</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Video Friday is your weekly selection of awesome robotics videos, collected by your slow-running Automaton bloggers. We\u2019ll also be posting a weekly calendar of upcoming robotics events for the next two months; here\u2019s what we have so far (<a shape=\"rect\" href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p>\n<h5>\n<a shape=\"rect\" href=\"http://u.cs.biu.ac.il/~agmon/arms2016/\">ARMS 2016</a> \u2013\u00a0May 9-13, 2016 \u2013\u00a0Singapore</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.nationalmanufacturingweek.com.au/\">National Manufacturing Week</a> \u2013\u00a0May 11-13, 2016 \u2013\u00a0Sydney, Australia</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.icra2016.org/\">ICRA 2016</a> \u2013\u00a0May 16-21, 2016 \u2013\u00a0Stockholm, Sweden</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.nasa.gov/offices/education/centers/kennedy/technology/nasarmc.html\">NASA Robotic Mining Competition</a> \u2013\u00a0May 18-20, 2016 \u2013\u00a0NASA KSC, Fla., USA</h5>\n<h5>\n<a shape=\"rect\" href=\"http://sk.ru/foundation/events/may2016/skrobotics2016/\">Skolkovo Robotics Conference</a> \u2013\u00a0May 20, 2016 \u2013\u00a0Skolkovo, Russia</h5>\n<h5>\n<a shape=\"rect\" href=\"http://innorobo.com/en/home/\">Innorobo 2016</a> \u2013\u00a0May 24-26, 2016 \u2013\u00a0Paris, France</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.robocity2030.org/events/event/evento-esp-2-2/\">RoboCity16</a> \u2013\u00a0May 26-27, 2016 \u2013\u00a0Madrid, Spain</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.robobusiness.eu/rb/\">RoboBusiness Europe</a> \u2013\u00a0June 1-3, 2016 \u2013\u00a0Odense, Denmark</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.dynamicwalking.org/index.php/dw/2016\">Dynamic Walking 2016</a> \u2013\u00a0June 4-7, 2016 \u2013\u00a0Holland, Mich., USA</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.comp.nus.edu.sg/~lowkh/mrsss.html\">IEEE RAS MRSSS 2016</a> \u2013\u00a0June 6-10, 2016 \u2013\u00a0Singapore</h5>\n<h5>\n<a shape=\"rect\" href=\"http://cts2016.cisedu.info/2-conference/workshops---cts-2016/workshop07-cr-hri2016\">CR-HRI</a> \u2013\u00a0June 6-10, 2016 \u2013\u00a0Orlando, Fla., USA</h5>\n<h5>\n<a shape=\"rect\" href=\"http://wp.wpi.edu/challenge/\">NASA SRRC Level 1</a> \u2013\u00a0June 6-11, 2016 \u2013\u00a0Worcester, Mass., USA</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.fieldrobot.com/event/\">Field Robot Event</a> \u2013\u00a0June 14-18, 2016 \u2013\u00a0Ha\u00dffurt, Germany</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.roboticsconference.org/\">RSS 2016</a> \u2013\u00a0June 18-22, 2016 \u2013\u00a0Ann Arbor, Mich., USA</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.elrob.org/\">European Land Robot Trial</a> \u2013\u00a0June 20-24, 2016 \u2013\u00a0Eggendorf, Austria</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.automatica-muenchen.com/en/Home\">Automatica 2016</a> \u2013\u00a0June 21-25, 2016 \u2013\u00a0Munich, Germany</h5>\n<h5>\n<a shape=\"rect\" href=\"http://conference.vde.com/isr2016/Pages/Start.aspx\">ISR 2016</a> \u2013\u00a0June 21-22, 2016 \u2013\u00a0Munich, Germany</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.icrom.org/\">ICROM 2016</a> \u2013\u00a0June 23-25, 2016 \u2013\u00a0Singapore</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.ukras.org/\">UK Robotics Week</a> \u2013\u00a0June 25-1, 2016 \u2013\u00a0United Kingdom</h5>\n<h5>\n<a shape=\"rect\" href=\"http://hamlyn.doc.ic.ac.uk/hsmr/\">Hamlyn Symposium on Medical Robotics</a> \u2013\u00a0June 25-28, 2016 \u2013\u00a0London, England</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.sheffieldrobotics.ac.uk/conferences/taros-2016/\">TAROS 2016</a> \u2013\u00a0June 28-30, 2016 \u2013\u00a0Sheffield, United Kingdom</h5>\n<h5>\n<a shape=\"rect\" href=\"http://www.robocup2016.org/en/\">RoboCup 2016</a> \u2013\u00a0June 30-4, 2016 \u2013\u00a0Leipzig, Germany</h5>\n<h5>\n<a shape=\"rect\" href=\"http://amazonpickingchallenge.org/\">Amazon Picking Challenge</a> \u2013\u00a0June 30-4, 2016 \u2013\u00a0Leipzig, Germany</h5>\n<p>\n<br clear=\"none\"/>\n<a shape=\"rect\" href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today\u2019s videos.</p>\n<hr/>\n<p>Snake Monster (a hexapod built with <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/military-robots/cmu-snake-robots-can-now-strangle-things-on-contact\">snake robot parts</a>) has gotten even more monstrous. And awesome.</p>\n<blockquote>\n<p>\n<em>A demonstration of mobile manipulation with \u201cSnake Monster,\u201d a hexapod robot built with modular actuators in the Biorobotics Lab, at the Robotics Institute of Carnegie Mellon University. Due to this robot\u2019s modular architecture, it is simple to change hardware from a passive rubber foot to a grasping end effector. The resulting leg can then be used for both locomotion and manipulation.</em>\n</p>\n</blockquote>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/7Mh3kqxle1c\"/>\n</p>\n<p>[ <a shape=\"rect\" href=\"http://www.cmu.edu/news/stories/archives/2015/january/reconfigurable-modular-robots.html\">CMU</a> ]</p>\n<p/>\n<hr/>\n<p>Unless you are the best drone pilot on the planet, and maybe even if you <em>are</em> the best drone pilot on the planet, the only way to safely inspect complex infrastructure is with <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/drones/gimball-drones-for-good-competition\">Flyability</a>\u2019s <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/drones/flyabilitys-gimball-drone-exploring-ice-caves\">contact-safe</a>\n<a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/drones/flyability-flashy-drones-dance\">drones</a>. They\u2019ve just announced a commercial product called Elios that can go anywhere and crash into anything and be totally fine:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/s96Q2GXgoeE\"/>\n</p>\n<p>You\u2019ll have to ask Flyability if you want to know how much one of these costs, but we\u2019re still hoping that sooner or later, they\u2019ll come out with a consumer version that we can afford to play around with.</p>\n<p>[ <a shape=\"rect\" href=\"http://www.flyability.com/elios/\">Flyability</a> ]</p>\n<p>\n<em>Thanks Adrien!</em>\n</p>\n<p/>\n<hr/>\n<p/>\n<p>The University of Edinburgh <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/humanoids/new-r5-valkyrie-robots\">has only had their Valkyrie robot for a few months</a>, but they\u2019re already making a lot of progress with it:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/AjSP8iZyhTE\"/>\n</p>\n<p>[ <a shape=\"rect\" href=\"http://valkyrie.inf.ed.ac.uk\">University of Edinburgh</a> ]</p>\n<p/>\n<p/>\n<hr/>\n<p/>\n<p>Robots don\u2019t get much more dexterous than this, because humans don\u2019t get much more dexterous than this, either:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/T4AnOwP3yZw\"/>\n</p>\n<p>RE2 just delivered one of these to the U.S. Army\u2019s Armament Research, Development and Engineering Center (ARDEC) for testing.</p>\n<p>[ <a shape=\"rect\" href=\"http://www.resquared.com/\">RE2</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<blockquote>\n<p>\n<em>Leka is an interactive and multi-sensory smart toy, offering children with special needs the ability to play fun and educational games that motivates social interactions, that increase motor, cognitive, and emotional skills, as well as stimulates autonomy.</em>\n</p>\n<p>\n<em>Based on papers published in peer reviewed sources, Leka has been developed hand-in-hand with parents, therapists and caregivers, to aid in a variety of settings. From the office, to schools, to home, Leka is making therapy easier, more efficient, and more accessible.</em>\n</p>\n</blockquote>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/ermEmpfWtmY\"/>\n</p>\n<p>[ <a shape=\"rect\" href=\"https://www.indiegogo.com/projects/leka-an-exceptional-toy-for-exceptional-children--2#/\">Indiegogo</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<p>There are <a shape=\"rect\" href=\"http://spectrum.ieee.org/tag/camera+drones\">drones that can act as jogging partners</a>, but that doesn\u2019t seem fair at all, does it? Wheels are still a bit of a cheat, but not as bad as flying:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/Q-g1DtiXfu8\"/>\n</p>\n<p>I like this robot too! It seems like it wouldn\u2019t be that hard to build something like this yourself, although it would be cool if it could also match acceleration profiles as opposed to just ending up at the finish line at the right time.</p>\n<p>[ <a shape=\"rect\" href=\"http://news.puma.com/\">PUMA</a> ] via [ <a shape=\"rect\" href=\"http://www.fastcocreate.com/3059417/puma-created-a-robot-as-fast-as-usain-bolt-to-make-athletes-better\">Fast Company</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<blockquote>\n<p>\n<em>IHMC Robotics will be competing in the Powered Exoskeleton Race of the Cybathlon, in October 2016. We have just gotten our exoskeleton, Mina v2, working and ready for testing. Our pilot, Mark Daniel, has joined the IHMC team full time to help with the design and testing, as well as train for the Cybathlon</em>\n</p>\n</blockquote>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/G-fJn43zlBk\"/>\n</p>\n<p>[ <a shape=\"rect\" href=\"http://robots.ihmc.us/cybathlon/\">IHMC</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<p>Very lifelike gaze control of a seriously pissed off iCub:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/I4ZKfAvs1y0\"/>\n</p>\n<p>Paper:\u00a0\u201cA Cartesian 6-DoF Gaze Controller for Humanoid Robots,\u201d by A. Roncone, U. Pattacini, G. Metta, and L. Natale. <em>Proceedings of Robotics: Science and Systems (RSS) 2016</em>.</p>\n<p>[ <a shape=\"rect\" href=\"http://www.icub.org/\">iCub</a> ]</p>\n<hr/>\n<p/>\n<p>\n<a shape=\"rect\" href=\"http://spectrum.ieee.org/robotics/home-robots/jibo-is-as-good-as-social-robots-get-but-is-that-good-enough\">Jibo</a> should start shipping to Indiegogo backers this fall, and here\u2019s a demo-filled update from Cynthia Breazeal and Jibo CEO Steve Chambers. If you just want to see Jibo doing stuff, skip to 5:33 for the most entertaining part:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/XuH_iaANSq0\"/>\n</p>\n<p>[ <a shape=\"rect\" href=\"https://www.jibo.com/\">Jibo</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<p>The six finalists in the Kuka Innovation Awards 2016 demonstrated their projects at the Hannover Messe industrial technology trade show last month, and we were there to help with the judging. It was not easy. All the teams worked hard and presented excellent demos. After tallying the votes, Kuka announced the winner: Team CoSTAR from Johns Hopkins University.</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/WBwLLSVXGOQ\"/>\n</p>\n<p>See the <a shape=\"rect\" href=\"https://www.youtube.com/playlist?list=PLcmh-lxe_PW5TVRqX__eY1PQkACCOPj6w\">other finalists and their demos here</a>.</p>\n<p>[ <a shape=\"rect\" href=\"https://www.kuka.com/en-DE/Press/Event%20calendar/Hannover%20Fair%202016/kuka-innovation-award\">KUKA Innovation Award</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<p>WowWee is for some reason crowdfunding this \u201cgaming and entertainment drone.\u201d It\u2019s notable for its low price (you can pledge $60 for one), as well as its localization technology, which uses a beacon that you place on the floor:</p>\n<blockquote>\n<p>\n<em>Lumi\u2019s advanced flight controls are enabled through BeaconSense\u2122 technology. Every Lumi comes with a separate beacon that acts as a core point of reference in space and is essentially a virtual leash for Lumi that tethers it to the beacon\u2019s location. The beacon enables Lumi to understand where it is in space at all times, which works in tandem with programmed algorithms to always keep Lumi stable, so if you move the beacon \u2013 Lumi will follow.</em>\n</p>\n</blockquote>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/O0Azd8XohSI\"/>\n</p>\n<p>HOLY COW LOOK AT THE SIZE OF THAT ROBOSAPIEN!</p>\n<p>[ <a shape=\"rect\" href=\"https://www.indiegogo.com/projects/lumi-world-s-best-gaming-entertainment-drone#/\">Indiegogo</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<p>As Sphero reminds us, having an inspiring teacher can be one of the most influential things to happen in your entire life:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/ACzaMffTRU4\"/>\n</p>\n<p>Thanks <a shape=\"rect\" href=\"http://www.bates.edu/geology/facultystaff/gene-clough/\">Gene</a>.</p>\n<p>Thanks <a shape=\"rect\" href=\"http://www.bv.fapesp.br/en/pesquisador/4738/jose-jaime-da-cruz/\">Jaime</a>.</p>\n<p>[ <a shape=\"rect\" href=\"http://www.sphero.com/sphero-sprk\">Sphero</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<blockquote>\n<p>\n<em>For those not familiar with macadamia yield trial work, macadamias fall to the ground when they are mature and from there they get picked up. Commercially this is usually done with \u201cfingerwheel\u201d type harvesters that have a gang of fingerwheels that entrap the nuts as they get run over and then extracts them into a transport system using combs. However while these machines are effective they are incapable of accurately stop-starting in order that tree-by-tree yields can be retrieved. Thus all research plots are harvested by hand which is expensive and hard work. There are hand push versions of fingerwheel type harvesters but they are only moderately effective and it is difficult to get them under the trees when the canopy skirt is low.</em>\n</p>\n</blockquote>\n<p>Solution: robots!</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/6wPZFHUxS38\"/>\n</p>\n<p>[ <a shape=\"rect\" href=\"http://www.hvp-macadamias.com/Automation?DroneHarvester.html\">Hidden Valley Plantations</a> ]</p>\n<p>\n<em>Thanks David!</em>\n</p>\n<p/>\n<hr/>\n<p/>\n<p>Technically, this may not quite be a robot, but it\u2019s certainly close enough for Video Friday:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/Lsiq_3-cSHs\"/>\n</p>\n<p>As much as I hate to admit it, human-in-the-loop is still the best way to go with things like this.</p>\n<p>[ <a shape=\"rect\" href=\"http://www.ccfe.ac.uk/\">CCFE</a> ] via [ <a shape=\"rect\" href=\"http://gizmodo.com/these-cute-robots-fix-problems-in-nuclear-reactors-1774385340\">Gizmodo</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<p>Look out GummiArm!</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/945XSTuKtAI\"/>\n</p>\n<p>Compliant, 3D printable, open source, and resistant to abuse from evil humans.</p>\n<p>[ <a shape=\"rect\" href=\"http://mstoelen.github.io/GummiArm/\">GummiArm</a> ] via [ <a shape=\"rect\" href=\"http://blog.trossenrobotics.com/2016/04/25/gummiarm-keeps-getting-better/\">Trossen Robotics</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<p>I\u2019m not sure if there is a \u201cwho can do the most epic aerial drone show\u201d competition, but there should be, and this one is up there:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" webkitallowfullscreen=\"\" width=\"620\" src=\"https://player.vimeo.com/video/163266757?title=0&amp;byline=0&amp;portrait=0\" mozallowfullscreen=\"\"/>\n</p>\n<p>[ <a shape=\"rect\" href=\"https://magic.microad.co.jp/skymagic/\">Sky Magic</a> ] via [ <a shape=\"rect\" href=\"http://sploid.gizmodo.com/these-drones-dancing-in-the-air-against-a-mountain-look-1774340050\">Gizmodo</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<p>Turning recycling into a collaborative game with robots sounds like fun, sort of, if your idea of a game involves recyclables:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/caMUE7lw_J0\"/>\n</p>\n<p>This might work best running through Mechanical Turk, and since you\u2019re basically getting humans to classify images for you, if they keep this up long enough, the robots will probably get pretty good at doing it all on their own.</p>\n<p>[ <a shape=\"rect\" href=\"http://www.jodone.com/\">Jodone</a> ] via [ <a shape=\"rect\" href=\"https://www.technologyreview.com/s/601373/recycling-workers-vie-for-bonuses-by-getting-robots-to-do-the-dirty-work/\">Tech Review</a> ]</p>\n<p/>\n<hr/>\n<p/>\n<p>From UCSD:</p>\n<blockquote>\n<p>\n<em>The sea urchin\u2019s intricate mouth and teeth are the model for a claw-like device developed by a team of engineers and marine biologists at the University of California, San Diego to sample sediments on other planets, such as Mars. The researchers detail their work in a recent issue of the Journal of Visualized Experiments. Researchers scanned the mouth with various methods (microCT, etc.). They then created design files based on the scans and build their prototype with a 3D printer. They attached the claw to a remote-controlled rover and successfully tested it on beach sand and on sand resembling Martian soil. They envision a fleet of mini rovers that would be deployed to sample soil sediments and bring them back to a main rover.</em>\n</p>\n</blockquote>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/4CG5peD6J3M\"/>\n</p>\n<p>[ <a shape=\"rect\" href=\"http://jacobsschool.ucsd.edu/news/news_releases/release.sfe?id=1928\">UCSD</a> ]</p>\n<p>\n<em>Thanks Ioana!</em>\n</p>\n<p/>\n<hr/>\n<p/>\n<p>How\u2019d the <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/space-robots/hawaiian-robot-landing-pad-construction\">PISCES landing pad construction</a> project do in its first test?</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" webkitallowfullscreen=\"\" width=\"620\" src=\"https://player.vimeo.com/video/163496839?title=0&amp;byline=0&amp;portrait=0\" mozallowfullscreen=\"\"/>\n</p>\n<p>Welp, the pad should be good for ::one:: takeoff, at least.</p>\n<p>[ <a shape=\"rect\" href=\"http://www.pacificspacecenter.com/new-documentary-on-piscesnasa-lunar-landing-pad/\">PISCES</a> ]</p>\n<p>\n<em>Thanks Megan!</em>\n</p>\n<p/>\n<hr/>\n<p/>\n<p>Kirstie Shepherd let us know about this beautifully illustrated tale of a scientist studying a band of feral robots on a distant planet:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/ATDrQUVIBYw\"/>\n</p>\n<p>A $45 pledge on Kickstarter will put you in line for a physical copy.</p>\n<p>[ <a shape=\"rect\" href=\"https://www.kickstarter.com/projects/curioandco/azr-0-robots-in-the-wild\">Kickstarter</a> ]</p>\n<p>\n<em>Thanks Kirstie!</em>\n</p>\n<p/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/i-sPLx2jRnM\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Fri, 06 May 2016 15:28:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/automaton/robotics/industrial-robots/video-friday-cmu-snake-monster-flyability-drone-puma-racing-robot</guid>\r\n      <dc:creator>Evan Ackerman and Erico Guizzo</dc:creator>\r\n      <dc:date>2016-05-06T15:28:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MDgzMA\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MDgzMA\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/automaton/robotics/industrial-robots/video-friday-cmu-snake-monster-flyability-drone-puma-racing-robot</feedburner:origLink></item>\r\n    <item>\r\n      <title>GM and Lyft Will Test Driverless Taxi Service</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/5qnrRCVzAdk/gm-and-lyft-will-test-driverless-taxi-service</link>\r\n      <description>Lyft customers will be able to opt in or out of the robotaxi service</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"LyftGettyImages508246192 (1)-1462546157748.jpg\" src=\"http://spectrum.ieee.org/img/LyftGettyImages508246192 (1)-1462546157748.jpg\"/>\n<figcaption>Photo: Mike Coppola/Getty Images</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>General Motors and Lyft will test a self-driving taxi service in an undisclosed city within a year, <a shape=\"rect\" href=\"http://www.wsj.com/articles/gm-lyft-to-test-self-driving-electric-taxis-1462460094\">according to a report</a> in the <em>Wall Street Journal</em> yesterday.</p>\n<p>It\u2019s not clear what steps the mystery city will take to align its rules of the road with the robotaxi service. Customers who get cold feet at the sight of an empty space where the driver normally sits\u00a0will be able to opt out of the robotaxi service and get a human-driven Lyft car\u00a0instead.</p>\n<p>It\u2019s one of\u00a0the first fruits in a dealmaking frenzy\u00a0that\u2019s roiling\u00a0the\u00a0auto industry.\u00a0GM recently\u00a0poured US $500 million into Lyft, a ride-hailing app that\u2019s like Uber but\u00a0smaller;\u00a0GM is also plunking down\u00a0$1 billion to acquire\u00a0Cruise Automation, whose technology it plans to use in the robotaxis.\u00a0And, to complete the high-tech showcase, the company\u00a0also plans to provide Lyft with its\u00a0new all-electric Chevy\u00a0Bolt, which <a shape=\"rect\" href=\"http://spectrum.ieee.org/cars-that-think/transportation/advanced-cars/the-chevy-bolt-wont-make-a-dime-for-gm\">has\u00a0more battery power </a>and thus more battery range than its existing offering, the Chevy Volt.</p>\n<p>In other recent dealmaking news, Google\u2014after failing to sign a deal with Ford\u2014is\u00a0<a shape=\"rect\" href=\"http://www.bloomberg.com/news/articles/2016-05-03/fiat-google-said-to-plan-partnership-on-self-driving-minivans\">reportedly</a> teaming up with\u00a0Fiat Chrysler.\u00a0Uber\u00a0is building up its own self-driving laboratory, a process it began by\u00a0<a shape=\"rect\" href=\"http://spectrum.ieee.org/cars-that-think/transportation/self-driving/uber-turns-from-google-teams-up-with-carnegie-mellon-on-selfdriving-cars\">strip-mining\u00a0the engineering\u00a0faculty</a> at\u00a0Carnegie Mellon University.</p>\n<p>One of the vexing points in any marriage between a car company and a tech company\u00a0is who gets to control the data.\u00a0No car company wants to be treated as a hewer of wood and drawer of water that gets a pittance while some\u00a0lordly tech partner grabs all the profits. <a shape=\"rect\" href=\"http://www.thestreet.com/story/13559868/1/next-driverless-car-battle-who-will-control-the-data.html\">According to a report</a> this week in\u00a0<em>TheStreet</em>, differences on this\u00a0question\u00a0were what probably scotched\u00a0the proposed\u00a0Google-Ford alliance. Ford is instead\u00a0buying its way into the relevant computer\u00a0expertise by investing $182\u00a0million in Pivotal Software, a cloud-computing company.</p>\n<p/>\n<p/>\n<p/>\n<p/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/5qnrRCVzAdk\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Fri, 06 May 2016 13:12:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/cars-that-think/transportation/self-driving/gm-and-lyft-will-test-driverless-taxi-service</guid>\r\n      <dc:creator>Philip E. Ross</dc:creator>\r\n      <dc:date>2016-05-06T13:12:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MDgyMg\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MDgyMg\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/cars-that-think/transportation/self-driving/gm-and-lyft-will-test-driverless-taxi-service</feedburner:origLink></item>\r\n    <item>\r\n      <title>HoloFlex: A Flexible Smartphone with a Holographic Display</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/el6J-BHNFOQ/holoflex-a-flexible-smartphone-with-a-holographic-display</link>\r\n      <description>This prototype smartphone has a  holographic lightfield displays glasses-free 3-D images, and a structure that bends for depth interaction</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"HoloFlexTeapot-1462475961901.jpg\" src=\"http://spectrum.ieee.org/img/HoloFlexTeapot-1462475961901.jpg\"/>\n<figcaption class=\"hi-cap\">Photo: The Human Media Lab/Queen\u2019s University</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>Based on how things have been going for the last few years, it seems like we're getting awfully close to hitting <a shape=\"rect\" href=\"http://spectrum.ieee.org/consumer-electronics/portable-devices/the-end-of-the-smartphone\">peak smartphone</a>, where new phones have run out of ways to differentiate themselves besides being slightly faster or slimmer or brighter or whatever. We\u2019ve almost forgotten what it\u2019s like to see brand new phones with a fundamentally new and exciting technologies inside of them. Oh, <a shape=\"rect\" href=\"http://www.koreatimes.co.kr/www/news/tech/2016/05/133_203761.html\">you've got a fingerprint sensor that's under the screen now instead of under the bezel</a>? Hooray.</p>\n<p>Fortunately for tech lovers everywhere, the <a shape=\"rect\" href=\"http://www.hml.queensu.ca/\">Queen\u2019s University\u00a0Human Media Lab</a> in Canada is keeping busy reminding us that smartphone hardware really does have a future that's worth getting excited about. Their latest prototype is the <a shape=\"rect\" href=\"http://www.hml.queensu.ca/holoflex\">HoloFlex</a>, which runs Android Lollipop, includes a full HD screen, and is powered by a 1.5 GHz Qualcomm Snapdragon 810 processor with a dedicated GPU and 2GB of RAM.</p>\n<p>And did we mention that you can bend it, and that it has a holographic lightfield display that can project glasses-free 3-D images to multiple users simultaneously? Because it\u2019s got that stuff going for it, too.</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/UDOkwJTPgCc?rel=0\"/>\n</p>\n<p>Let's take a look at the display first. It's based on a\u00a0FOLED (flexible OLED) screen with a resolution of 1920 x 1080 pixels (403 dpi) and a touch layer. On top of that is a 3-D printed flexible lens array made up of 16,640 half-dome shaped droplets in a 160 x 104 hexagonal matrix. Each lens projects the 12 pixel-wide circular area directly underneath it out into space, and each of those approximately\u00a080 pixel image blocks contains information about the entire scene from a virtual camera position that's unique to the position of the lens. Essentially, each of those 16,640 lenses is projecting a slightly different view of the same scene, and all of those image blocks together represent 3-D information about the full scene over a field of view of about 35 degrees.</p>\n<figure class=\"rt med\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MDU2Nw\"/>\n<figcaption class=\"hi-cap\">Illustration: The Human Media Lab/Queen\u2019s University</figcaption>\n</figure>\n<p>We keep using the word \u201cinformation\u201d\u00a0here because this is a <a shape=\"rect\" href=\"http://spectrum.ieee.org/tech-talk/consumer-electronics/gaming/4d-light-field-displays-are-exactly-what-virtual-reality-needs\">lightfield display</a>, which <a shape=\"rect\" href=\"http://spectrum.ieee.org/consumer-electronics/audiovideo/holographic-displays-coming-to-smartphones\">renders images holographically</a>. A hologram looks three dimensional because the lightfield that it\u2019s based on doesn't just include information about where a pixel is on the x-y plane as a traditional display does, but also information about the angle of light rays.\u00a0In other words, it\u2019s reconstruction of all of the information about a scene, not just a snapshot of a scene from one single perspective. You can replicate this effect in software, calculating the lightfield that\u00a0an arbitrary scene would create. Then your array of image blocks and\u00a0lens droplets can distribute the light rays across multiple perspectives simultaneously to generate a hologram of whatever you want.\u00a0</p>\n<p>The upshot of this is that HoloFlex can transform software models into lightfield display-based holograms, resulting in images that have depth and exhibit motion parallax and can be viewed from multiple perspectives by multiple users. The resolution isn't great (just 160 x 104), but no weird glasses or tracking is necessary. I'm sure the (2-D) video doesn't do the effect justice.</p>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MDU2NQ\"/>\n<figcaption class=\"hi-cap\">Photo: The Human Media Lab/Queen\u2019s University</figcaption>\n</figure>\n<figure class=\"rt med\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MDU2Ng\"/>\n<figcaption class=\"hi-cap\">Illustration: The Human Media Lab/Queen\u2019s University</figcaption>\n</figure>\n<p>Now that you\u2019ve got this awesome holographic smartphone display with a useful z-axis, you need some way of interacting with it. Why use something clunky like gestures when, thanks to flexible electronics,\u00a0you could just bend the entire phone itself? The left side of the phone is rigid, while the rest of it acts like a spring, providing passive haptic feedback for intuitive control over the z-dimension. And as a side benefit, bending the phone makes it the holograms seem even more three dimensional, as they can \u201cjump\u201d\u00a0across the display.</p>\n<p>This is by far one of the smallest and most mobile lightfield displays we've ever seen, and both of these demos (editing of a <a shape=\"rect\" href=\"http://www.computerhistory.org/revolution/computer-graphics-music-and-art/15/206\">model</a> for 3-D printing, and gaming) actually seem like useful applications for the technology rather than just novelties. The Queens inventors\u00a0don't make any mention of commercialization, but it\u2019s worth noting that the only bit of hardware that isn't commercially available (or close to it) is the microlens array, and that\u2019s 3-D printed. Does this mean that we're likely to see smartphones with holographic displays on the market sometime soon? Probably not. But at some point, maybe. At the very least, it\u2019s things like this that could mean that peak smartphone hasn\u2019t quite hit yet.</p>\n<p>HoloFlex will be presented next week at <a shape=\"rect\" href=\"https://chi2016.acm.org/wp/\">ACM CHI 2016</a> in San Jose, Calif.</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/el6J-BHNFOQ\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Thu, 05 May 2016 20:01:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/tech-talk/consumer-electronics/gadgets/holoflex-a-flexible-smartphone-with-a-holographic-display</guid>\r\n      <dc:creator>Evan Ackerman</dc:creator>\r\n      <dc:date>2016-05-05T20:01:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MDU5Ng\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MDU5Ng\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/tech-talk/consumer-electronics/gadgets/holoflex-a-flexible-smartphone-with-a-holographic-display</feedburner:origLink></item>\r\n    <item>\r\n      <title>Single-Atom Sensor Offers New View of the Nanoscale</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/d-naCjOY38c/singleatom-sensor-offers-new-view-of-the-nanoscale</link>\r\n      <description>A so-called nitrogen-vacancy defect sensor provides superb image resolution of minute phenomena</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"fig2a-1462466387494.jpg\" src=\"http://spectrum.ieee.org/img/fig2a-1462466387494.jpg\"/>\n<figcaption class=\"hi-cap\">Image: Quantum Sensing and Imaging Group/UC Santa Barbara</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>It was a eureka moment when IBM researchers <a shape=\"rect\" href=\"http://spectrum.ieee.org/nanoclast/semiconductors/nanotechnology/intuition-leads-to-the-tool-that-opened-up-the-nanoscale-universe-and-a-new-nanotechnology-lab\">first realized that they were imaging the surface of an atom</a> with what came to be known as the scanning tunneling microscope (STM). Many believe that invention triggered the field of nanotechnology. Now researchers at the University of California Santa Barbara (UCSB) have created a next-gen microscope that can image phenomena like magnetism\u00a0on the atomic scale across a huge range of temperatures. The heart of the microscope is <a shape=\"rect\" href=\"http://www.news.ucsb.edu/2016/016747/making-invisible-physics-visible\">a single atom</a> or, perhaps more accurately, the <em>absence</em> of a single atom.</p>\n<p/>\n<p>In research described in the journal <a shape=\"rect\" href=\"http://www.nature.com/nnano/journal/vaop/ncurrent/full/nnano.2016.68.html\">\n<em>Nature Nanotechnology</em>\n</a>, the researchers fabricated a new kind of microscope\u00a0sensor based on something called a nitrogen\u2013vacancy (NV) defect in diamond. In these NV defects, a nitrogen atom replaces a carbon atom at one point in the diamond\u2019s molecular\u00a0lattice.\u00a0This disrupts the structure of the lattice and leaves empty a\u00a0normally-occupied location\u00a0adjacent to the nitrogen atom.</p>\n<p/>\n<p>This defect in the diamond lattice makes possible the sensing of certain physical phenomenon, most notably magnetism. In operation, a NV-based magnetic sensor detects this magnetism by measuring the defect\u2019s\u00a0spin-dependent photoluminescence. To actually build a microscope, the UCSB researchers created\u00a0a sensor\u00a0that resembles a toothbrush. Each bristle in this toothbrush-like structure has one of these NV defects at the tip.</p>\n<p/>\n<p/>\n<p>\u201cThis is the first tool of its kind,\u201d said Ania Jayich, a professor and leader of the research, in a press release. \u201cIt operates from room temperature down to low temperatures where a lot of interesting physics happens. When thermal energy is low enough, the effects of electron interactions, for instance, become observable, leading to new phases of matter. And we can now probe these with unprecedented spatial resolution.\u201d</p>\n<p/>\n<p>To test their NV-based sensor, the researchers imaged a superconductor that contained magnetic structures known as vortices, which are areas of changing\u00a0magnetic flux. The new sensor was able to discern individual vortices.</p>\n<p/>\n<p/>\n<p>In continuing research, the UCSB team is exploiting this excellent resolution to look at the world of <a shape=\"rect\" href=\"http://spectrum.ieee.org/nanoclast/semiconductors/materials/developments-in-magnetic-skyrmions-come-in-bunches\">skyrmions</a>, which are swirling magnetic spin patterns in thin films. Skyrmions may sound obscure but they more form the basis a new kind of data storage.\u00a0 With this new NV-based sensor, the researchers believe that its high resolution will make it possible to image all the interactions that occur in a material that lead to skyrmions.</p>\n<p>Jayich added: \u201cThere are a lot of different interactions between atoms and you need to understand all of them before you can predict how the material will behave.\u201d</p>\n<p/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/d-naCjOY38c\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Thu, 05 May 2016 18:28:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/nanoclast/semiconductors/nanotechnology/singleatom-sensor-offers-new-view-of-the-nanoscale</guid>\r\n      <dc:creator>Dexter Johnson</dc:creator>\r\n      <dc:date>2016-05-05T18:28:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc1MDMwMA\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc1MDMwMA\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/nanoclast/semiconductors/nanotechnology/singleatom-sensor-offers-new-view-of-the-nanoscale</feedburner:origLink></item>\r\n    <item>\r\n      <title>Robot Roaches With Tiny Magnetic Winch Cooperate to Scale Steps</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/rqYUgrThutw/robot-roaches-with-tiny-magnetic-winch-cooperate-to-scale-steps</link>\r\n      <description>UC Berkeley's VelociRoACHes get magnetic grappling hooks to help them cooperatively climb over obstacles</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"VelociRoACHCooperation_b-1462335990000.jpg\" src=\"http://spectrum.ieee.org/img/VelociRoACHCooperation_b-1462335990000.jpg\"/>\n<figcaption class=\"hi-cap\">Image: UC Berkeley</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>In a world full of things that are much, much bigger than they are, insects manage to do pretty well with getting around. Some of the most successful insects are the social and cooperative ones, like ants, which can do incredible things such as <a shape=\"rect\" href=\"https://www.youtube.com/watch?v=IBTjQMtbViU\">using their bodies to create structures to get themselves across rivers</a>.</p>\n<p>In Ron Fearing\u2019s lab at UC Berkeley, <a shape=\"rect\" href=\"https://www.linkedin.com/in/carlos-casarez-7b9ab046\">Carlos Casarez</a> was inspired by behaviors like these to modify some VelociRoACHes to help each other climb up and over obstacles with the aid of an adorable little magnetic tether system.</p>\n<p>Robots like UC Berkeley\u2019s VelociRoACH (Velocity Robotic Autonomous Crawling Hexapod) are, relative to most other robots, very simple and inexpensive to manufacture, with high speed and good maneuverability. They\u2019d be just the thing you might want to send a swarm of into disaster areas to search for survivors, except that they\u2019re absolutely terrible at climbing over obstacles. The problem is that the underactuated nature that makes them so fast and cheap to produce also means that you can\u2019t control the motion of each leg independently, <a shape=\"rect\" href=\"http://spectrum.ieee.org/automaton/robotics/drones/rhex-does-parkour-all-over-upenn\">which is what allows other legged hexapods (like RHex) to do parkour</a>.</p>\n<p>VelociRoACH isn\u2019t going to be doing parkour anytime soon, but by getting a couple of them them to cooperate with each other like ants do, you can make a combined robot that\u2019s much more versatile than either robot would be by itself, able to leap tall steps in a single bound! Or, nearly:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/X78FWHc2pLg?rel=0\"/>\n</p>\n<p>Casarez, lead author of an upcoming paper on this approach, told us how this all came together:</p>\n<blockquote>\n<p>\n<em>This is a novel design because depending on how a pair of VelociRoACH robots coordinate their actions, they can form a sort of modular robot with an extended jointed body and twice the number of actuated legs, or one robot can be an anchor while the other one uses the tether to provide pulling assistance. It\u2019s also a minimal design because a single additional motor drives both the connection forming and the tether pulling assistance. My design solution combines previous approaches to robot cooperation to climb step or slope obstacles that I cite in the background section of the paper, which produces a more versatile platform than other systems. </em>\n</p>\n<p>\n<em>The cooperating robots benefit from distributed actuation of modular robots to climb steps, while being able to break the modular robot up when it is too cumbersome to drag the connected robots up the step. The tether-assisted climbing mode fills in the last piece of the step climbing strategy\u2014 instead of just sending one member of the team up the step after climbing while connected, the winch can be used to pull the other team member up the step as well.</em>\n</p>\n</blockquote>\n<p>The tethering system itself consists of a rapidly-prototyped winch module with a polyethylene monofilament tether and magnetic connector that can be latched on to a compliant pin on the back of a second VelociRoACH.</p>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc1MDIwMg\"/>\n<figcaption class=\"hi-cap\">Image: UC Berkeley</figcaption>\n<figcaption>Two VelociRoACH robots with a compliant connection. Left: Forming a connection begins with a magnetic connector tethered to the back robot. Right: Magnets engage to form a compliant pin connection between robots.</figcaption>\n</figure>\n<p>Once the tether is secure, the robots execute a series of \u201cmotion primitives\u201d that together, if everything goes well, result in both robots on top of a 6.5 cm step that neither one would be able to climb by itself. Each primitive has a failure rate of about 50 percent for a variety of reasons, including the robots flipping over, twisting, getting stuck, or slipping off the step completely. This results in an overall success rate of 10 percent, which is not all that great, but Casarez explains how just some basic sensor feedback could significantly improve this:</p>\n<blockquote>\n<p>\n<em>While an overall success rate of 10 percent is too low to practically implement the cooperative step climbing strategy as is, it is still impressive considering that the robots used no sensory feedback to control each segment of the behavior. In the experiments performed in this paper, we simply prescribed a fixed gait and winch load control strategy for each phase of the behavior. In the future, we plan to improve the reliability of the cooperative step climbing behavior by adding closed-loop feedback control involving robot-to-robot localization, connection contact sensors, and IMU/motor torque information from each robot.</em>\n</p>\n<p>\n<em>Once a single VelociRoACH robot encounters an obstacle it cannot climb (leading into primitive II), implementing robot-to-robot localization using vision or IR emitter/receiver pairs (emitters on one robot, receivers on the other) could assist the second robot with the winch module in finding the first robot and forming a connection with it. In addition, contact sensors could be used to sense whether the compliant connection is formed between robots (right before primitive III), or whether the tether is attached to the leading robot (right before primitive V) \u00a0before continuing on with the next phase of the behavior.</em>\n</p>\n</blockquote>\n<aside class=\"inlay pullquote rt med\">\u201cI am also very interested in . . . cooperation between heterogeneous robots. Let\u2019s forget about step climbing altogether and attach the tethered magnetic connector of a VelociRoACH with winch module to a quadrotor, which can fly over whatever obstacles are in the way\u201d</aside>\n<p>Cooperative insects like ants are able to cooperatively form more complex structures as their numbers increase, so we also asked Carlos about whether his future plans included adding more VelociRoACH robots into the mix:</p>\n<blockquote>\n<p>\n<em>If we think about scaling the step-climbing behavior by adding more robots with winch modules and connection components to form a chain of 3-10 robots, we can infer that the increasingly long chain of robots with more joints and actuators will be able to climb over taller and taller steps. In addition, a long chain of robots could potentially cross gaps in terrain that are smaller than the length of the robot chain. The chain of robots could wedge themselves in the gap and boost the front robot in the chain past the opposite side of the gap. However, as we add more and more robots to these cooperative behaviors, it becomes harder to reliably perform the behavior\u2014if the connection between one pair of robots in the chain fails, then the entire operation fails.\u00a0</em>\n</p>\n<p>\n<em>I think there is some more interesting work in exploring how far two-robot cooperation can go. For example, instead of climbing a step, the robot with the winch could be used as an anchor for a tethered VelociRoACH that explores down an unknown chasm, which can then be retrieved after exploring. Also, if you want to get 10 VelociRoACH robots over a two-robot climbable obstacle, you can simply follow the cooperation primitives in the paper, then keep attaching more winch robots to the back of the leading winch robot. The first robot that gets over the obstacle is the anchor, and tether-assisted climbing can be used to pull as many robots as you want up the step.\u00a0</em>\n</p>\n<p>\n<em>I am also very interested in using the components presented in this paper to enable cooperation between heterogeneous robots. Let\u2019s forget about step climbing altogether and attach the tethered magnetic connector of a VelociRoACH with winch module to a quadrotor, which can fly over whatever obstacles are in the way and then anchor itself to the environment. The VelociRoACH with winch can then pull itself over whatever terrain is in between it and the quadrotor.</em>\n</p>\n</blockquote>\n<p>We\u2019ve been fans of heterogeneous robot swarms for a long time, and it\u2019s very cool to think about how teams of VelociRoACH robots might be able to work with teams of quadrotors like this. The other thing to reiterate is how simple and cheap these roachbots are, and if you can use a bunch of them to mimic (or even improve upon) the functionality of larger and more expensive robots, you could save a lot of time and resources by relying on them instead. Imagine sending a swarm of these little guys into, say, <a shape=\"rect\" href=\"http://spectrum.ieee.org/energy/nuclear/24-hours-at-fukushima\">Fukushima</a> to gather data about the environment inside: you know that half of them are going to get fried by radiation, and none of them are going to come back, but it just isn\u2019t that big of a deal, because they\u2019re essentially expendable. And as long as they can cooperate with each other to get up stairs and over rubble that\u2019s bigger than they are, there will be no stopping them. Even if you want to.</p>\n<h4>\u201cStep Climbing Cooperation Primitives for Legged Robots With a Reversible Connection,\u201d by\u00a0Carlos S. Casarez and Ronald S. Fearing from UC Berkeley, will be presented later this month at ICRA 2016 in Stockholm, Sweden.</h4>\n<p>[ <a shape=\"rect\" href=\"https://robotics.eecs.berkeley.edu/~ronf/Biomimetics.html\">UC Berkeley</a> ]</p>\n<p/>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/rqYUgrThutw\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Thu, 05 May 2016 14:22:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-roaches-with-tiny-magnetic-winch-cooperate-to-scale-steps</guid>\r\n      <dc:creator>Evan Ackerman</dc:creator>\r\n      <dc:date>2016-05-05T14:22:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc0OTI0NA\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc0OTI0NA\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-roaches-with-tiny-magnetic-winch-cooperate-to-scale-steps</feedburner:origLink></item>\r\n    <item>\r\n      <title>Bell Labs Looks at Claude Shannon\u2019s Legacy and the Future of Information Age</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/IwcjOZtsoNk/bell-labs-looks-at-claude-shannon-legacy-future-of-information-age</link>\r\n      <description>Tech luminaries celebrate one of the greatest electrical engineering heroes of all time</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"claude-bell-labs-milestone-plaque-1462385781350.jpg\" src=\"http://spectrum.ieee.org/img/claude-bell-labs-milestone-plaque-1462385781350.jpg\"/>\n<figcaption class=\"hi-cap\">Photo: Erico Guizzo/IEEE Spectrum</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>As the doors of Hamming Hall swung open, a parade of tech-world notables headed inside. Internet pioneers and Silicon Valley executives crammed into the front rows, along with 16 IEEE Fellows, of which four were also IEEE Medal of Honor recipients. A Nobel laureate walked in and squeezed himself into a seat between them.</p>\n<p/>\n<p>Those brilliant minds gathered at Bell Labs in Murray Hill, N.J., last Thursday to celebrate a man who also stood out for his extraordinary genius: <a shape=\"rect\" href=\"http://spectrum.ieee.org/computing/software/claude-shannon-tinkerer-prankster-and-father-of-information-theory\">Claude Shannon, the mathematician and electrical engineer who founded the field of information theory</a> and whose groundbreaking ideas on computing, cryptography, and communications have had a lasting impact on the world of technology. Shannon, who worked at Bell Labs from 1941 until 1956, when he joined the MIT faculty, <a shape=\"rect\" href=\"http://news.mit.edu/2001/shannon\">died in 2001</a> at age 84. This year marks the 100th anniversary of his birth.</p>\n<p/>\n<figure class=\"rt med\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc0OTU2OQ\"/>\n<figcaption>Bell Labs, IEEE, and the IEEE Information Theory Society unveiled the monument above at the Shannon Centennial Conference in Murray Hill, N.J. The plaque reads: \u201cThe mathematical principles of Information Theory, laid down by Claude Elwood Shannon during the period 1939-1967, set in motion a revolution in communication systems engineering. They quantified the concept of information, established fundamental limits for the representation and reliable transmission of information, and revealed the architecture of systems for approaching them. Today, Information Theory, continues to provide the foundation for advances in information collection, storage, distribution, and processing.\u201d</figcaption>\n</figure>\n<p>Bell Labs, now part of Nokia, organized the <a shape=\"rect\" href=\"https://www.bell-labs.com/programs/shannon-conference/\">Claude Shannon Centennial Conference</a> to reflect on the immense legacy of one of its most legendary alumni and look at the future of the information age he helped launch. Invited speakers discussed how Shannon\u2019s work transformed their research fields and their own careers, and what lessons his life offers to today\u2019s innovators.</p>\n<p/>\n<p>\u201cHe was the engineer-mathematician-theoretician-intuitive-genius,\u201d said <a shape=\"rect\" href=\"http://www.lk.cs.ucla.edu/index.html\">Leonard Kleinrock</a>, a professor of computer science at UCLA who was a PhD student at MIT under Shannon. \u201cHe opened up vast new domains of science and engineering,\u201d Kleinrock, an <a shape=\"rect\" href=\"http://theinstitute.ieee.org/people/achievements/the-fathers-of-the-internet\">Internet pioneer</a> who helped develop <a shape=\"rect\" href=\"https://en.wikipedia.org/wiki/Packet_switching\">packet switching</a>, said. \u201cHe set the standards that others have yet to achieve.\u201d</p>\n<p/>\n<p>Qualcomm co-founder <a shape=\"rect\" href=\"http://spectrum.ieee.org/geek-life/profiles/irwin-jacobs-captain-of-cdma\">Irwin Jacobs</a>, who took Shannon\u2019s graduate seminar at MIT, said he could only imagine what his professor, an <a shape=\"rect\" href=\"http://spectrum.ieee.org/computing/software/claude-shannon-tinkerer-prankster-and-father-of-information-theory\">inveterate tinkerer and prankster</a>, would have done with today\u2019s processors and sensors. Shannon would have built \u201csome really amazing toys, robots, drones,\u201d he told the audience. \u201cHe\u2019d just have a great time with all of these things.\u201d</p>\n<p/>\n<p>Bell Labs showed a video of Shannon demonstrating one of his many wondrous contraptions, Theseus, a device in which a mouse was able to find its way out of a maze all on its own:</p>\n<p>\n<iframe frameborder=\"0\" height=\"349\" scrolling=\"auto\" allowfullscreen=\"\" width=\"620\" src=\"//www.youtube.com/embed/nS0luYZd4fs\"/>\n</p>\n<p>\n<a shape=\"rect\" href=\"https://www.princeton.edu/~verdu/\">Sergio Verd\u00fa</a>, a professor of electrical engineering at Princeton, gave a colorful talk on Shannon\u2019s life, presenting the steps that seem to have led him to information theory. \u201cThis is really one of the few cases in science where a whole discipline comes out of the blue,\u201d he said, referring to the seminal paper Shannon published in 1948, titled \u201cA Mathematical Theory of Communication.\u201d <em>[Read it on IEEE Xplore: <a shape=\"rect\" href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6773024\">Parts I and II</a>; <a shape=\"rect\" href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6773067\">Part III</a>.]</em>\n</p>\n<p/>\n<figure class=\"rt med\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc0OTU3MQ\"/>\n<figcaption>Sergio Verd\u00fa of Princeton shows a Bell Labs telephone directory that listed some of Shannon\u2019s coworkers in 1947. It includes some major figures in electrical engineering and mathematics of the 20th century: <a shape=\"rect\" href=\"https://en.wikipedia.org/wiki/Hendrik_Wade_Bode\">Hendrik Bode</a>, <a shape=\"rect\" href=\"https://en.wikipedia.org/wiki/Ralph_Beebe_Blackman\">Ralph B. Blackman</a>, <a shape=\"rect\" href=\"https://en.wikipedia.org/wiki/Richard_Hamming\">Richard Hamming</a>, <a shape=\"rect\" href=\"https://en.wikipedia.org/wiki/Brockway_McMillan\">Brockway McMillan</a>, and <a shape=\"rect\" href=\"https://en.wikipedia.org/wiki/John_Tukey\">John Tukey</a>. Also on the list was Mary Elizabeth (Betty) Moore, whom Shannon would marry in 1949.</figcaption>\n</figure>\n<p>Verd\u00fa described the paper as having \u201celegant mathematics\u201d that proved to be \u201cextremely practical,\u201d even though it took more than two decades until digital systems evolved to the point where <a shape=\"rect\" href=\"http://spectrum.ieee.org/computing/hardware/david-forney-the-man-who-launched-a-million-modems\">engineers could apply Shannon\u2019s ideas to build reliable, error-free communication systems</a>.</p>\n<p/>\n<p/>\n<p/>\n<p>Also in attendance were MIT\u2019s <a shape=\"rect\" href=\"http://spectrum.ieee.org/computing/hardware/david-forney-the-man-who-launched-a-million-modems\">David Forney, this year\u2019s recipient of the IEEE Medal of Honor</a>; <a shape=\"rect\" href=\"https://math.berkeley.edu/~berlek/\">Elwyn Berlekamp</a>, a professor emeritus at UC Berkeley who worked with Shannon at MIT; <a shape=\"rect\" href=\"http://theinstitute.ieee.org/technology-focus/technology-history/ethernet-turns-40\">Bob Metcalfe</a>, the inventor of Ethernet and now a professor of innovation at the University of Texas at Austin; and <a shape=\"rect\" href=\"https://en.wikipedia.org/wiki/Robert_Woodrow_Wilson\">Robert W. Wilson</a>, who shared the 1978 Nobel Prize in physics.</p>\n<p/>\n<h4>STUMBLING AROUND IN THE DARK</h4>\n<p/>\n<p>The conference also aimed to look at the future of innovation in the digital age. <a shape=\"rect\" href=\"https://www.bell-labs.com/about/presidents/\">Marcus Weldon</a>, president of Bell Labs and chief technology officer of Nokia, predicted \u201ca revolution in networking\u201d as today\u2019s centralized communication systems reach their limits and give way to a more distributed infrastructure. \u201cWe\u2019re going to build a brand new network over the next 10 years.\u201d</p>\n<p/>\n<p>He added that if it weren\u2019t for Shannon\u2019s insights on the capacity of communication channels, \u201cwe would be stumbling around in the dark.\u201d (At one point he did make the auditorium go dark by <a shape=\"rect\" href=\"https://twitter.com/ericoguizzo/status/725677442627735552\">\u201cturning off\u201d Bell Labs\u2019 innovations</a>. Don\u2019t worry, he turned them back on!)</p>\n<figure class=\"xlrg\" role=\"img\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc0OTU3Mg\"/>\n<figcaption>Marcus Weldon, president of Bell Labs and Nokia CTO, said that without Shannon\u2019s insights on communications, \u201cwe would be stumbling around in the dark.\u201d</figcaption>\n</figure>\n<p>Another speaker, <a shape=\"rect\" href=\"https://en.wikipedia.org/wiki/Eric_Schmidt\">Eric Schmidt</a>, the former CEO of Google and now executive chairman of Alphabet, Google\u2019s parent company, offered a wide-ranging view of innovation. As a summer student at Bell Labs in the 1970s, Schmidt was part of the famed group that developed Unix and C. He said that supporting \u201ccuriosity-driven research,\u201d with \u201cscientists largely left alone to pursue hunches,\u201d is a vital part of a successful innovation ecosystem.</p>\n<p>Other components include \u201cbrutal academic competition\u201d and \u201cencouragement of entrepreneurship with light or no regulation.\u201d He argued that the combination of these elements allowed technologies like the transistor, search engines, and autonomous vehicles to evolve into \u201csystemic platforms\u201d that can have a big societal impact.</p>\n<p/>\n<aside class=\"inlay pullquote rt med\">\u201cShannon was interested in [solving] intellectual puzzles and he did this brightly by having multiple things he was interested in the back of his brain all the time. And then when he got up in the morning he\u2019d have some idea to kind of start to pursue one of those problems. And whichever one came up first, he would work on that.\u201d<span class=\"pq-attrib\">\u2014Robert G. Gallager, MIT</span>\n</aside>\n<p>Today\u2019s Bell Labs, however, is very different than the Bell Labs where\u00a0 <a shape=\"rect\" href=\"http://ect.bell-labs.com/mnc/csp/history.html\">information theory was invented</a>. In 2007, it became part of Alcatel-Lucent and went through a major reorganization. Early this year, after Nokia acquired Alcatel-Lucent, Bell Labs found itself, yet again, adjusting to a new owner.</p>\n<p/>\n<p/>\n<p>\n<a shape=\"rect\" href=\"http://www.rle.mit.edu/rgallager/\">Robert G. Gallager</a>, a professor emeritus at MIT who worked with Shannon in the 1960s, called on Nokia to support the \u201c<a shape=\"rect\" href=\"http://www.ifp.illinois.edu/~tieliu/Shannon.html\">Shannon-style research</a>\u201d that is now increasingly hard to find in industrial labs and universities. \u201cManagers need to meet deadlines, they need to do all these other things,\u201d he said. \u201cBut if they want to have really good people, to start to do things a little like what Shannon did, they need to give people time to think.\u201d</p>\n<p/>\n<p/>\n<p>Gallager added that Shannon \u201cwas interested in intellectual puzzles,\u201d and not in particular businesses or companies. \u201cShannon did this brightly by having multiple things he was interested in the back of his brain all the time. And then when he got up in the morning he\u2019d have some idea to kind of start to pursue one of those problems. And whichever one came up first, he would work on that.\u201d</p>\n<p>Or as Shannon himself once <a shape=\"rect\" href=\"https://www.youtube.com/watch?v=HSoog0OqgV0\">put it</a>, \u201cI think, indeed, that most scientists are oriented that way, that they are working because they like the game.\u201d</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/IwcjOZtsoNk\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Wed, 04 May 2016 21:05:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/tech-talk/telecom/internet/bell-labs-looks-at-claude-shannon-legacy-future-of-information-age</guid>\r\n      <dc:creator>Erico Guizzo</dc:creator>\r\n      <dc:date>2016-05-04T21:05:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc0OTYyNQ\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc0OTYyNQ\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/tech-talk/telecom/internet/bell-labs-looks-at-claude-shannon-legacy-future-of-information-age</feedburner:origLink></item>\r\n    <item>\r\n      <title>High-Tech Eavesdropping on the Ganges River Dolphin</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/Sc-STfF37XQ/hightech-eavesdropping-on-the-ganges-river-dolphin</link>\r\n      <description>Sonar signals hold clues that could save an endangered species</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<p>The <a shape=\"rect\" href=\"http://wwf.panda.org/what_we_do/endangered_species/cetaceans/about/river_dolphins/ganges_river_dolphin/\">Ganges river dolphin</a> is one of only two remaining freshwater dolphin species on earth. But pollution, fishing, and dams threaten to wipe it out entirely.</p>\n<p>So acoustical engineer Harumi Sugimatsu and her team have deployed an experimental sonar monitoring system<span>\u00a0just under the surface of the murky water.</span>\u00a0The\u00a0hope\u00a0is to\u00a0track\u00a0the dolphins by the high-frequency clicks they use to navigate and hunt. By eavesdropping on their underwater lives, Sugimatsu believes she can gather data about their behavior and geographical range\u2014data that conservationists can use in their struggle to keep the species from going extinct.</p>\n<p>Read More: <a shape=\"rect\" href=\"http://spectrum.ieee.org/green-tech/conservation/engineers-to-the-rescue-the-ultrasonic-mission-to-save-the-ganges-river-dolphin\">Engineers to the Rescue! The Ultrasonic Mission to Save the Ganges River Dolphin</a>\n</p>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/Sc-STfF37XQ\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Wed, 04 May 2016 21:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/video/green-tech/conservation/hightech-eavesdropping-on-the-ganges-river-dolphin</guid>\r\n      <dc:date>2016-05-04T21:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc0OTYzOA\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc0OTYzOA\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/video/green-tech/conservation/hightech-eavesdropping-on-the-ganges-river-dolphin</feedburner:origLink></item>\r\n    <item>\r\n      <title>Do Police Body Cameras Really Work?</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/siB3BnFbydo/do-police-body-cameras-really-work</link>\r\n      <description>Sometimes police body cameras accomplish their intended purpose, but other times they backfire. And nobody knows why</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<div class=\"articleBody\">\n                      &lt;style type=\"text/css\"&gt;.article-detail figure figcaption.moreVertSpace {\n    margin-bottom: 24px;\n}\n&lt;/style&gt;\n&lt;figure class=\"xlrg\" role=\"img\"&gt;&lt;img alt=\"img\" src=\"/image/Mjc0NzI1OA\" /&gt;\n&lt;figcaption class=\"hi-cap\"&gt;Image: Rialto Police Department&lt;/figcaption&gt;\n\n&lt;figcaption&gt;&lt;strong&gt;Calm Down: &lt;/strong&gt;Ideally, the very act of using a body cam will keep everyone on his or her best behavior. But if tempers nevertheless flare, the video collected from the camera can help others judge whether a police officer&amp;rsquo;s use of force was appropriate. This image comes from a vast amount of footage captured by officers in Rialto, Calif., where the first large-scale controlled trial of police body cameras was performed.&lt;/figcaption&gt;\n&lt;/figure&gt;\n\n&lt;p&gt;Police body cameras are popping up everywhere, often to good effect because both police and suspects normally behave better in their presence. No wonder these small devices, enthusiastically endorsed by police, politicians, and civil-rights advocates, have generated a burgeoning industry. Yet people know very little about how and why they work, so the intended and unintended consequences of using them remain nebulous.&lt;/p&gt;\n\n&lt;p&gt;That&amp;rsquo;s not for lack of effort. There have been nearly 40 studies on the use of body cameras, including a dozen randomized controlled trials on the magnitude of their effect on policing. Despite all this work, it&amp;rsquo;s still not entirely apparent why these cameras are helpful, under what conditions, or for whom.&lt;/p&gt;\n\n&lt;p&gt;Here I&amp;rsquo;d like to offer my interpretation of all that research and to delve into what sets police body cameras apart from other video-recording equipment, such as closed-circuit television, dashcams, and everyday smartphone cameras.&lt;/p&gt;\n\n&lt;p&gt;Ultimately, body cameras are just video cameras, albeit specialized ones for capturing evidence in a reliable way. Although body cams are relatively straightforward technology, their effectiveness has proven highly variable, for reasons that remain puzzling. Why, for example, does their use reduce by more than half the number of times officers apply force during their encounters with the public in some places, while in other places it nearly doubles the reported use of force?&lt;/p&gt;\n\n&lt;p&gt;To address these questions, it&amp;rsquo;s helpful first to consider other surveillance devices used in policing. These have a longer track record and could offer some insight into the new phenomenon of body cameras.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Over the past 25 or so years,&lt;/strong&gt; surveillance cameras have increasingly become an integral part of law enforcement as technological advancements made video cameras better, more reliable, and substantially cheaper. Most, if not all, police agencies in developed nations use closed-circuit television (CCTV) with the aim of deterring criminal activity, investigating crimes that have taken place, and prosecuting those responsible for them.&lt;/p&gt;\n\n&lt;p&gt;London, for instance, is under such &lt;a href=\"http://www.csmonitor.com/World/Europe/2012/0222/Report-London-no-safer-for-all-its-CCTV-cameras\"&gt;heavy police surveillance&lt;/a&gt; that it&amp;rsquo;s hard to find streets &lt;em&gt;not &lt;/em&gt;covered by CCTV. And many of the blind spots that remain are targeted by private video cameras. So when you&amp;rsquo;re in London, as well as many other cities, nearly every move you make in public is videotaped, tagged, and filed away somewhere.&lt;/p&gt;\n\n&lt;p&gt;But do these cameras really prevent crime and disorder? Common sense says, of course they do. If potential offenders are rational actors, aware that their misdeeds will be caught on video, they will surely be deterred from wrongdoing. This is the rationale for concluding that bad guys will not victimize us when there is CCTV, an ever-more common part of life for which we all pay the price of infringed privacy.&lt;/p&gt;\n\n&lt;figure class=\"xlrg\" role=\"vid\"&gt;\n&lt;p&gt;&lt;iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"https://www.youtube.com/embed/edGetW1VRbc\" width=\"620\"&gt;&lt;/iframe&gt;&lt;/p&gt;\n\n&lt;figcaption class=\"hi-cap\"&gt;Video: SPDwatcher&lt;/figcaption&gt;\n\n&lt;figcaption&gt;&lt;strong&gt;Not-So-Candid Camera: &lt;/strong&gt;A typical body-cam video demonstrates the usefulness of this technology for documenting people&amp;rsquo;s behavior during the interaction, even if the camera is not quite pointed where you&amp;rsquo;d want it.&lt;/figcaption&gt;\n&lt;/figure&gt;\n\n&lt;p&gt;Despite such intuition, and despite the huge investments made, overwhelming evidence indicates that CCTV equipment has, in fact, little deterrent effect. At least 44 studies illustrate that CCTV reduces the overall level of crime by only about 16 percent, with half of that reduction concentrated in parking lots. There is no effect at all on assaults, robberies, and similar against-person violent crimes. &lt;a href=\"https://rucore.libraries.rutgers.edu/rutgers-lib/37338/\"&gt;The evidence&lt;/a&gt; also tends to show that what little criminal behavior CCTV prevents is just displaced to other areas.&lt;/p&gt;\n\n&lt;p&gt;In short, CCTV cameras are pretty much a failure. Perhaps that&amp;rsquo;s because offenders know where the blind spots are. Or maybe it&amp;rsquo;s because CCTV does not work well in the dark or when the perpetrator wears a hoodie. Another possibility is simply that police lack the resources to assign somebody to sit through endless hours of recorded video hoping to find clues in not-so-serious crimes. Or it could also be that CCTV has become such an integral part of everyday life that its presence escapes people&amp;rsquo;s conscious attention. Whatever the reason, it&amp;rsquo;s clear that although CCTV may make you &lt;em&gt;feel&lt;/em&gt; more secure, it does not really make you any safer, although it makes it modestly less likely you&amp;rsquo;ll get your car broken into or stolen.&lt;/p&gt;\n\n&lt;p&gt;Another common type of video surveillance, the one that has accompanied the proliferation of smartphones, might actually be more important. Video cameras are ubiquitous, and the video recording of engagements between police and public is incredibly influential, especially when misconduct is caught, be it the infamous beating of Rodney King in Los Angeles in 1991 or the killing of Eric Garner in New York City in 2014. These recordings certainly demonstrate the effect of cameras on public reactions to the police, having sparked the Los Angeles riots of 1992 and the Black Lives Matter movements of 2015.&lt;/p&gt;\n\n&lt;p&gt;Given the notoriety of such videos, a camera at the scene of a police-public encounter ought, logically, to send out an accountability cue, eliminating feelings of anonymity on either side. There is no strong evidence, however, to support the conclusion that mobile-phone cameras deter officers from misconduct. The Garner incident is particularly telling in this regard: As clearly shown in the recording released to the media, officers were well aware of the cameras filming them (some of them looked directly at the cameraman), yet they still used a &lt;a href=\"http://www.nyc.gov/html/ccrb/downloads/pdf/Chokehold%20Study_20141007.pdf\"&gt;prohibited chokehold&lt;/a&gt;. Why?&lt;/p&gt;\n\n&lt;p&gt;There are probably two reasons. First, the officers might have been aware of a camera, but it still didn&amp;rsquo;t really register. Here, as in many other highly charged encounters, people just don&amp;rsquo;t think much about all the cameras around them before making the decision to throw a punch or otherwise misbehave. Second, even if they are aware of being recorded, the parties involved might not perceive a strong possibility that the footage captured can and will be used to hold them accountable for any type of misconduct. Because it is not official evidence, the video from a civilian&amp;rsquo;s smartphone camera doesn&amp;rsquo;t seem to inspire concern that it will surface later.&lt;/p&gt;\n\n&lt;p&gt;I should point out that one particular type of camera &lt;em&gt;is &lt;/em&gt;effective in preventing unwanted behavior: road or speed cameras. A systematic review of 35 rigorous tests of road cameras has shown that they reduce serious and fatal accidents by &lt;a href=\"http://www.cochrane.org/CD004607/INJ_do-speed-cameras-reduce-road-traffic-crashes-injuries-and-deaths\"&gt;as much as 44 percent&lt;/a&gt;. The evidence is unequivocal. They do exactly what they are intended to do. Even a speed-camera &lt;em&gt;sign&lt;/em&gt;, with no actual camera anywhere, prompts drivers to slow down. Unlike CCTVs or smartphones, speed cameras work because punishment for illegal actions is virtually guaranteed.&lt;/p&gt;\n\n&lt;div class=\"slideshowembed slideshow_2747252\" contenteditable=\"false\"&gt;&lt;/div&gt;\n\n&lt;p&gt;&lt;strong&gt;Where do body cameras sit&lt;/strong&gt; on this spectrum of effectiveness? There have been 12 randomized control trials, with another 30 ongoing research projects seeking the answer. My students in the police executive program at the University of Cambridge, in England, and I have conducted most of the randomized controlled trials that have been published so far, including &lt;a href=\"http://link.springer.com/article/10.1007/s10940-014-9236-3\"&gt;one&lt;/a&gt; that is now commonly referred to as the &amp;ldquo;Rialto experiment.&amp;rdquo;&lt;/p&gt;\n\n&lt;p&gt;This study, which really gave rise to today&amp;rsquo;s heated debate about police body cameras, took place in Rialto, Calif., a town of about 100,000 located some 100 kilometers east of Los Angeles. I worked on this experiment with &lt;a href=\"http://www.policefoundation.org/team_detail/chief-tony-farrar/\"&gt;Tony Farrar&lt;/a&gt;, then Rialto&amp;rsquo;s police chief, while he was completing a master&amp;rsquo;s thesis, for which I served as his academic advisor.&lt;/p&gt;\n\n&lt;p&gt;My Cambridge colleague &lt;a href=\"http://www.crim.cam.ac.uk/people/academic_research/alex_sutherland/\"&gt;Alex Sutherland&lt;/a&gt; helped Tony and me give this evaluation of body cameras a high degree of statistical rigor. Indeed, we devised this test just as if we were investigating the effectiveness of some new drug therapy. The study involved all of the town&amp;rsquo;s 54 frontline police officers, who for an entire year, starting in February 2012, would be assigned either to treatment (camera-wearing) or control (not camera-wearing) conditions when they went out on patrol.&lt;/p&gt;\n\n&lt;p&gt;During treatment shifts, officers were asked to take video of all their interactions with the public, to announce that the encounter was being recorded, and subsequently to store the footage on a secure cloud-based server. In control shifts, the officers were told not to use body cameras at all. Outcomes were then measured in terms of officially recorded use-of-force incidents and complaints lodged against Rialto police officers. At the end of a year, we were able to compare nearly 500 patrol shifts during which all police-public encounters were assigned to treatment conditions (wearing cameras) with a roughly equal number of shifts assigned to control conditions (not wearing them).&lt;/p&gt;\n\n&lt;aside class=\"inlay xlrg\"&gt;\n&lt;h3 class=\"sb-hed\"&gt;Results of the Rialto Experiment (incidents)&lt;/h3&gt;\n\n&lt;figure role=\"img\"&gt;&lt;img alt=\"img\" src=\"/image/Mjc0ODI0OA\" /&gt;\n&lt;figcaption&gt;&lt;strong&gt;Take the Plunge: &lt;/strong&gt;The author and a student working with him&amp;mdash;who was then chief of police in Rialto, Calif.&amp;mdash;undertook a large-scale controlled trial of police body cameras starting in February of 2012. The number of times Rialto officers resorted to using force and the number of complaints received both diminished markedly after just half the patrols began using body cameras.&lt;/figcaption&gt;\n&lt;/figure&gt;\n&lt;/aside&gt;\n\n&lt;p&gt;The results were stunning. There were roughly 50 percent fewer incidents of force being used while the officers were wearing body cameras compared with control conditions (8 incidents as compared with 17). And after reviewing the footage, we discovered that all eight times the camera-using officers resorted to force, they did so in response to violent behavior on the part of the people they were engaged with. The evidence suggests that in 4 of the 17 instances in which officers not wearing cameras resorted to force, it was the officer who initiated physical contact. This seems a key finding, because it really points to cameras making police officers less likely to use force without ample justification.&lt;/p&gt;\n\n&lt;p&gt;What&amp;rsquo;s more, there was a 90 percent reduction in citizens&amp;rsquo; complaints against police officers compared with the 12 months prior to the experiment. That&amp;rsquo;s particularly remarkable, because it was a 90 percent reduction in the &lt;em&gt;total&lt;/em&gt; number of complaints filed, not just those filed against officers wearing cameras.&lt;/p&gt;\n\n&lt;p&gt;Perhaps because all officers wore cameras some of the time, their behavior even when not wearing cameras changed. The statistics suggest as much, because the officers not wearing cameras resorted to force only half as often as they did in the year before the experiment. The moderating effect of the cameras seems to have been contagious.&lt;/p&gt;\n\n&lt;p&gt;The results of the Rialto experiment would lead you to think that police body cameras are an unequivocal success. In Rialto they were, but studies my colleagues and I have since conducted elsewhere require that I add a rather large note of caution. You see, if you consider the 10 other places where we have now completed tests of such cameras, you would conclude that their overall effect on police use of force is a wash: In some instances they help, in some they don&amp;rsquo;t appear to change police behavior, and in other situations they actually backfire, seemingly increasing the use of force.&lt;/p&gt;\n\n&lt;aside class=\"inlay xlrg\"&gt;\n&lt;h3 class=\"sb-hed\"&gt;Use of Force in Multisite Trial (incidents)&lt;/h3&gt;\n\n&lt;figure role=\"img\"&gt;&lt;img alt=\"graph showing treatment control\" src=\"/image/Mjc0NzUxMw\" /&gt;\n&lt;figcaption&gt;&lt;strong&gt;Uneven Results: &lt;/strong&gt;After the Rialto experiment, the author and his colleagues expanded their trials of body cameras to 10 sites spread over three continents. The results proved perplexing, with the use of force sometimes diminishing when cameras were worn, sometimes staying the same, and sometimes even increasing. (The locations of the various test sites cannot be disclosed because of agreements with the police forces involved.)&lt;/figcaption&gt;\n&lt;/figure&gt;\n&lt;/aside&gt;\n\n&lt;p&gt;That wearing a camera would ever cause an officer to use force more than he or she would otherwise do is puzzling, to say the least. But some hints of what is happening come from looking at how well officers complied with the experimental protocol.&lt;/p&gt;\n\n&lt;p&gt;In places where they closely followed the instructions (use the camera during each encounter if you&amp;rsquo;re in a treatment group; don&amp;rsquo;t use it if you&amp;rsquo;re in a control group), the results were positive&amp;mdash;a 37 percent reduction in use of force on average. But if you allow the treatment group discretion to choose when to turn it on, the result is 71 percent greater use of force. Thus the problem seems to arise mainly when officers are allowed to turn cameras on at times of their own choosing.&lt;/p&gt;\n\n&lt;aside class=\"inlay lt med\"&gt;\n&lt;h3 class=\"sb-hed\"&gt;Three Types of Body Cameras&lt;/h3&gt;\n\n&lt;figure class=\"lt med\" role=\"img\"&gt;&lt;a class=\"zoom\" href=\"/image/Mjc0MTg3OA\" rel=\"lightbox\"&gt;&lt;img alt=\"img\" src=\" /image/Mjc0MTg3Nw\" /&gt;&lt;span class=\"magnifier\"&gt;&amp;nbsp;&lt;/span&gt;&lt;/a&gt;\n\n&lt;figcaption class=\"moreVertSpace\"&gt;From Edinburgh-based&amp;nbsp;&lt;a href=\"http://www.edesix.com/\"&gt;Edesix&lt;/a&gt;.&lt;/figcaption&gt;\n&lt;/figure&gt;\n\n&lt;figure class=\"lt med\" role=\"img\"&gt;&lt;a class=\"zoom\" href=\"/image/Mjc0MTg4MA\" rel=\"lightbox\"&gt;&lt;img alt=\"img\" src=\"/image/Mjc0MTg3OQ \" /&gt;&lt;span class=\"magnifier\"&gt;&amp;nbsp;&lt;/span&gt;&lt;/a&gt;\n\n&lt;figcaption class=\"moreVertSpace\"&gt;From &lt;a href=\"https://www.taser.com/\"&gt;Taser International&lt;/a&gt; of&amp;nbsp;Scottsdale,&amp;nbsp;Ariz.&lt;/figcaption&gt;\n&lt;/figure&gt;\n\n&lt;figure class=\"lt med\" role=\"img\"&gt;&lt;a class=\"zoom\" href=\"/image/Mjc0MTg4Mg\" rel=\"lightbox\"&gt;&lt;img alt=\"img\" src=\"/image/Mjc0ODQ4OQ\" /&gt;&lt;span class=\"magnifier\"&gt;&amp;nbsp;&lt;/span&gt;&lt;/a&gt;\n\n&lt;figcaption&gt;From &lt;a href=\"http://www.revealmedia.com/\"&gt;Reveal Media&lt;/a&gt; of Surrey, England.&lt;/figcaption&gt;\n&lt;/figure&gt;\n\n&lt;figure&gt;\n&lt;figcaption class=\"hi-cap\"&gt;Photos, from top: Edesix; Taser; Reveal Media&lt;/figcaption&gt;\n&lt;/figure&gt;\n&lt;/aside&gt;\n\n&lt;p&gt;&lt;strong&gt;Although there are exceptions,&lt;/strong&gt; especially where officers are allowed too much discretion, it nevertheless seems clear to me that police body cameras properly used can be very helpful because officers and suspects alike become more certain that they&amp;rsquo;ll be punished for bad behavior when a camera is rolling.&lt;/p&gt;\n\n&lt;p&gt;For this reason, body cameras are sometimes seen as a panacea: They record everything and therefore tell the unmediated story of what took place. They increase transparency, heighten accountability, and keep the actions of all parties in check. Because no rational person wants to get into trouble (or into more trouble than he or she is already in), police-public interactions become less heated.&lt;/p&gt;\n\n&lt;p&gt;Very often that&amp;rsquo;s the case, but it doesn&amp;rsquo;t always play out that way. It&amp;rsquo;s common for officers to interact with people who are mentally ill, drunk, high, raging with anger, or otherwise emotionally disturbed&amp;mdash;people who are not likely to be aware of a camera even if told about it. Similarly, officers in emotionally heightened situations&amp;mdash;such as during a high-speed pursuit or while subduing a resisting offender&amp;mdash;might easily fail to turn the camera on or just ignore it even if it&amp;rsquo;s running.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;rsquo;s where better technology could help. Body cameras could be automatically activated immediately when certain cues are triggered, such as when the officer enters a crime hotspot, leaves a police vehicle, takes out handcuffs or a weapon, turns on the siren, or makes a call for assistance on the radio. This way, the officer doesn&amp;rsquo;t have to think about activating the body camera in a tense situation.&lt;/p&gt;\n\n&lt;p&gt;One manufacturer, Taser International of Scottsdale, Ariz., is already offering body cameras that &lt;a href=\"http://www.axon.io/products/signal\"&gt;activate automatically&lt;/a&gt; when an officer turns on a vehicle&amp;rsquo;s flashing lights or draws a &amp;ldquo;smart&amp;rdquo; weapon. And others are investigating the use of cameras that &lt;a href=\"http://firevideo.net/index.php/oncall\"&gt;stream live video&lt;/a&gt; back to headquarters. Right now, power and bandwidth limitations prevent that general practice, but one day in the not-too-distant future it will likely be the norm.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;While the variability in the effectiveness &lt;/strong&gt;of these cameras is disappointing, perhaps it shouldn&amp;rsquo;t be so surprising. After all, what could work for a sheriff&amp;rsquo;s department in Iowa may not necessarily apply to a national police force in the Middle East. The results are going to hinge on the way official records are stored, the prevalence of police misconduct, and how legitimate the force is in the eyes of the people they serve.&lt;/p&gt;\n\n&lt;p&gt;So even though I&amp;rsquo;m a great supporter of body cameras, I&amp;rsquo;d advise police departments to move slowly and thoughtfully in their adoption. Test them in ways that allow these cameras to fail as much as to succeed. Try out different procedures and pick the ones that work best. After all, who&amp;rsquo;s to say that the policies practiced in the United States or the United Kingdom are appropriate elsewhere? Police administrators need to consider, in particular, whether rank-and-file officers will embrace the cameras or lash back at &amp;ldquo;big brother&amp;rdquo; and the threat to their autonomy.&lt;/p&gt;\n\n&lt;p&gt;There are certainly a lot of issues to address: Should superior officers be allowed to view all of the patrol officers&amp;rsquo; footage, or ought those rights be limited to specific cases? Should officers be able to review their own recordings before filing their written reports? And who should be in charge of curating all this video, police departments or some independent agency subject to public-records laws? The different people involved will no doubt answer those questions very differently based on their hunches and past experiences, so to find what really works best will require much experimentation.&lt;/p&gt;\n\n&lt;p&gt;I would also add that there are still plenty of technological hurdles. The most pressing is the need to integrate the evidence captured by body cameras with other information-technology systems used in law enforcement. These are already troublingly fragmented. Typically, police forces have one system for taking emergency calls, another for handling investigations, and another yet for recording their reactions to crimes. Offender databases do not talk to victim databases. And there is often no link whatsoever with databases held by prosecutors, so court outcomes are never then fed back to the police. The list goes on.&lt;/p&gt;\n\n&lt;p&gt;These kinds of problems will grow only more acute as police departments look to incorporate additional layers of information with their body-cam video, including location data and the results from facial-recognition technologies. To fully explore the possibilities and rewards, many more tests are needed.&lt;/p&gt;\n\n&lt;p&gt;There&amp;rsquo;s another reason to test these cameras and their associated technologies carefully before handing them out. Imagine a police force is looking to use body cameras as a way to boost public trust. If police administrators simply announced, &amp;ldquo;We&amp;rsquo;re using cameras now, and we find that police misconduct is extremely rare,&amp;rdquo; most people would take such statements with a grain of salt. If, however, the police force had first partnered with independent researchers to test the effectiveness of the cameras, positive results would more likely be believed. That, in my view, is the best prescription for any police agency that wants to move forward with body cameras and the technology that is swiftly evolving around them.&lt;/p&gt;\n\n&lt;h2&gt;About the Author&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"http://www.crim.cam.ac.uk/people/academic_research/barak_ariel/\"&gt;Barak Ariel&lt;/a&gt; is a lecturer at the University of Cambridge&amp;rsquo;s Institute of Criminology. He is also the chief analyst for this institute&amp;rsquo;s Jerry Lee Center, where he uses various experimental methods to investigate the causes of, and responses to, crime.&lt;/p&gt;\n                    </div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/siB3BnFbydo\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Wed, 04 May 2016 19:00:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/consumer-electronics/portable-devices/do-police-body-cameras-really-work</guid>\r\n      <dc:creator>Barak Ariel</dc:creator>\r\n      <dc:date>2016-05-04T19:00:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc0ODgyMA\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc0ODgyMA\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/consumer-electronics/portable-devices/do-police-body-cameras-really-work</feedburner:origLink></item>\r\n    <item>\r\n      <title>Autonomous Robot Surgeon Bests Humans in World First</title>\r\n      <link>http://feedproxy.google.com/~r/IeeeSpectrumFullText/~3/_uFk-VaAjNQ/autonomous-robot-surgeon-bests-human-surgeons-in-world-first</link>\r\n      <description>In a tricky surgical procedure on pigs, independent robotic surgery produced better outcomes</description>\r\n      <content:encoded><![CDATA[<?xml version=\"1.0\" encoding=\"UTF-8\"?><html>\n<body>\n<div id=\"artBody\">\n<figure class=\"xlrg\">\n<img image=\"robosurgblog1-1462378348880.gif\" src=\"http://spectrum.ieee.org/img/robosurgblog1-1462378348880.gif\"/>\n<figcaption class=\"hi-cap\">Gif: Sheikh Zayed Institute for Pediatric Surgical Innovation</figcaption>\n<figcaption>The smart surgical bot planned and performed the operation, although supervising humans occasionally reached in to help.</figcaption>\n</figure>\n<div class=\"articleBody\">\n<p class=\"articleBodyPln\"/>\n<p>In a robotic surgery breakthrough, a bot stitched up a pig\u2019s small intestines using its own vision, <span>tools, and\u00a0</span>intelligence\u00a0to carry out the procedure. What\u2019s more, the Smart Tissue Autonomous Robot (STAR) did a better job on the operation than human surgeons who were given the same task.</p>\n<p>STAR\u2019s inventors don\u2019t claim that robots can replace humans in the operating room anytime soon. Instead they see the accomplishment as a proof of concept\u2014both for the specific technologies used\u00a0and for the general concept of \u201csupervised autonomy\u201d in the OR.</p>\n<p>Pediatric surgeon <a shape=\"rect\" href=\"http://childrensnational.org/research-and-education/about-cri/faculty-and-leadership-directory/peter-kim\">Peter Kim</a>, one of the researchers, didn\u2019t sound threatened when he spoke to reporters in a press call yesterday. \u201cEven though we surgeons take pride in our craft at doing procedures, to have a machine that works with us to improve outcomes and safety would be a tremendous benefit,\u201d he said.</p>\n<p>For <a shape=\"rect\" href=\"http://stm.sciencemag.org/lookup/doi/10.1126/scitranslmed.aad9398\">this study</a>, published today in the journal <em>Science Translational Medicine</em>, researchers programmed their robot to carry out a procedure called <a shape=\"rect\" href=\"http://www.nlm.nih.gov/medlineplus/ency/article/002231.htm\">intestinal </a>\n<a shape=\"rect\" href=\"https://www.nlm.nih.gov/medlineplus/ency/article/002231.htm\">anastomosis</a>, in which a piece of intestine that\u2019s been cut through is stitched back together. It\u2019s like repairing a garden hose, said <a shape=\"rect\" href=\"https://www.linkedin.com/in/ryan-decker-66026325\">Ryan Decker</a>, the senior engineer on the team, in that the sutures must be tight and regularly spaced to prevent leaks. STAR performed this task both on <em>ex vivo</em> tissue in the lab and on <em>in vivo</em> tissue in an anesthetized pig, and experienced human surgeons were given the same tasks. When the resulting sutures were compared, STAR\u2019s stitches were more consistent and more resistant to leaks. \u00a0</p>\n<p>The robot did have a little help.\u00a0In about 40 percent of its trials, the researchers intervened to offer guidance of some sort\u2014as in the GIF above, where a human hand is seen holding the thread. In the other 60 percent of trials, STAR did the job completely on its own.\u00a0</p>\n<p>The researchers don\u2019t think these assists invalidate their claim of autonomy; instead they see the setup as representative of shared control setups that would be appropriate for real ORs.\u00a0Human surgeons could supervise procedures or even trade off tasks with the robot, letting the machine do more routine or tedious parts of an operation.\u00a0\u201cYou can imagine that if something critical is happening, that would be a point where the surgeon is going to be closely monitoring the robot,\u201d Decker said.\u00a0\u201cI\u2019m sure they wouldn\u2019t feel comfortable just letting it run and going to take a coffee break.\u201d\u00a0</p>\n<div class=\"imgWrapper xlrg\">\n<img alt=\"img\" src=\"http://spectrum.ieee.org/image/Mjc0OTQ2OA\"/>\n</div>\n<p>Today, some surgical procedures already incorporate smart machines. Robots routinely carry out the crucial steps in some procedures\u00a0including orthopedic <a shape=\"rect\" href=\"http://www.hindawi.com/journals/aorth/2013/970703/\">knee replacements</a>, <a shape=\"rect\" href=\"http://en.wikipedia.org/wiki/LASIK\">Lasik eye surgery</a>, and <a shape=\"rect\" href=\"http://restorationrobotics.com/artas-harvesting/\">hair transplants</a>. What these types of surgery have in common, though, is the fixed nature of their targets, as leg bones, eyes, and heads can be held in place during the procedure. Soft tissue surgeries are\u00a0much messier and more difficult to automate, because all the slippery pink parts of the body shift around\u00a0and are hard to track.\u00a0</p>\n<p>The current state-of-the-art robot for soft tissue surgery is the\u00a0<a shape=\"rect\" href=\"http://www.intuitivesurgical.com/products/davinci_surgical_system/\">da Vinci system</a> from Intuitive Surgical, but it\u2019s not automated at all. The da Vinci is a teleoperated\u00a0system, in which the surgeon sits at a console and manipulates controls in dexterous maneuvers that are mimicked by\u00a0tiny tools inside the patient\u2019s body.\u00a0</p>\n<p>STAR solved the soft tissue challenge by integrating a few different technologies. Its vision system relied on near-infrared fluorescent (NIRF) tags placed in the intestinal\u00a0tissue; a specialized NIRF camera tracked those markers while a 3D camera recorded images of the entire surgical field. Combining all this data allowed STAR to keep its focus on its target. The robot made its own plan for the suturing job,\u00a0and it adjusted that plan as tissues moved during the operation.\u00a0</p>\n<p>The researchers trained STAR only on how to perform this particular intestinal suturing procedure.\u00a0\u201cWe\u00a0programmed\u00a0the best surgeon\u2019s techniques, based on consensus\u00a0and physics, into the machine,\u201d Kim said.\u00a0</p>\n<p>An outside\u00a0expert\u00a0in the field of surgical robotics called this study a breakthrough, but also said its limitations show\u00a0that autonomous robots \u201cwill not come to the OR soon.\u201d\u00a0<a shape=\"rect\" href=\"http://www.ee.washington.edu/faculty/hannaford/\">Blake Hannaford</a>, a pioneer of autonomous surgical robotics at the University of Washington, noted that the\u00a0NIRF tags that the robot relied on were placed by humans.</p>\n<p>Hannaford\u00a0also questioned the clinical significance of the task that STAR performed.\u00a0\u201cWhile in a technical sense, semi-autonomous suturing is a \u2018grand challenge\u2019\u00a0problem of surgical robotics, clinically much suturing and bowel anastomosis is done by staplers which can do the whole thing in seconds,\u201d he wrote in an email.\u00a0\u201cClearly the task they chose does not justify the elaborate equipment they used.\u201d \u00a0<strong> \u00a0</strong>\n</p>\n<p>The STAR team said this task was simply\u00a0intended as proof that autonomous robots could meet the challenge\u00a0of\u00a0soft tissue surgery. While the robot may not be ready to take over the OR,\u00a0Kim said he hopes his technology will be integrated into commercial devices in the next few years. If robotic systems are shown to\u00a0improve safety and\u00a0patient outcomes, he said, medicine may go the way of the auto industry.\u00a0</p>\n<p>\u201cNow driverless cars are\u00a0coming into our lives,\u201d Kim said.\u00a0\u201cIt\u00a0started with self-parking,\u00a0then a technology that tells you not to go\u00a0into the wrong lane. Soon you have a car that can drive by itself.\u201d Similarly, he said, surgical robots could start by giving\u00a0human surgeons a helping hand. And maybe one day they\u2019ll\u00a0take over.\u00a0</p>\n</div>\n</div>\n</body>\n</html><img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumFullText/~4/_uFk-VaAjNQ\" height=\"1\" width=\"1\" alt=\"\"/>]]></content:encoded>\r\n      <pubDate>Wed, 04 May 2016 18:17:00 GMT</pubDate>\r\n      <guid isPermaLink=\"false\">http://spectrum.ieee.org/the-human-os/robotics/medical-robots/autonomous-robot-surgeon-bests-human-surgeons-in-world-first</guid>\r\n      <dc:creator>Eliza Strickland</dc:creator>\r\n      <dc:date>2016-05-04T18:17:00Z</dc:date>\r\n      <media:content url=\"http://spectrum.ieee.org/image/Mjc0OTkxNA\">\r\n        <media:thumbnail url=\"http://spectrum.ieee.org/image/Mjc0OTkxNA\" />\r\n      </media:content>\r\n    <feedburner:origLink>http://spectrum.ieee.org/the-human-os/robotics/medical-robots/autonomous-robot-surgeon-bests-human-surgeons-in-world-first</feedburner:origLink></item>\r\n  </channel>\r\n</rss>\r\n",
  "headers": {
    "Expires": "Fri, 13 May 2016 07:43:09 GMT",
    "X-XSS-Protection": "1; mode=block",
    "Transfer-Encoding": "chunked",
    "Cache-Control": "private, max-age=0",
    "ETag": "CA7Yy71FpPrLsPCbb5GpOB7yMF8",
    "Content-Encoding": "gzip",
    "Last-Modified": "Fri, 13 May 2016 07:37:03 GMT",
    "Date": "Fri, 13 May 2016 07:43:09 GMT",
    "X-Content-Type-Options": "nosniff",
    "Content-Type": "text/xml; charset=UTF-8",
    "Server": "GSE"
  },
  "response_url": "http://feeds.feedburner.com/IeeeSpectrumFullText"
}